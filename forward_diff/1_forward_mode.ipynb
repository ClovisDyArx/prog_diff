{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-clznqoAwj9"
      },
      "source": [
        "### Implémentation de l'Autodifférentiation en Mode Forward par surcharge d'opérateurs\n",
        "\n",
        "### Objectif\n",
        "\n",
        "L'objectif de ce TP est d'implémenter l'autodifférentiation en mode forward en utilisant des nombres duals. Vous allez créer une classe Python qui représente un tenseur contenant une partie réelle et une partie duale, et implémenter les opérations arithmétiques de base (addition, soustraction, multiplication, division, puissance) ainsi que la différentiation automatique en mode forward.\n",
        "\n",
        "\n",
        "### Principe de la méthode d'autodifférentiation en mode forward\n",
        "\n",
        "L'autodifférentiation (ou différentiation automatique) est une technique utilisée pour calculer les dérivées de fonctions définies par des programmes informatiques. Contrairement aux méthodes de dérivation symbolique ou numérique, l'autodifférentiation est précise et efficace. Il existe deux modes principaux : le mode forward et le mode reverse.\n",
        "En mode forward, l'idée est de propager des paires de valeurs (la valeur de la fonction et la valeur de sa dérivée) à travers chaque opération élémentaire de la fonction. Voici comment cela fonctionne :\n",
        "\n",
        "1. **Initialisation** : On commence par associer à chaque variable d'entrée une paire de valeurs $(x_i, \\dot{x}_i)$, où $x_i$ est la valeur de la variable et $\\dot{x}_i$ est la dérivée de la variable par rapport à l'entrée d'intérêt (souvent initialisée à 1 pour la variable d'intérêt et à 0 pour les autres).\n",
        "\n",
        "2. **Propagation** : À chaque étape de la fonction, pour chaque opération élémentaire (comme l'addition, la multiplication, etc.), on calcule à la fois la nouvelle valeur de la fonction et la nouvelle valeur de la dérivée. Par exemple, pour une opération élémentaire $z = x + y$, on aurait :\n",
        "   - Valeur : $z = x + y$\n",
        "   - Dérivée : $\\dot{z} = \\dot{x} + \\dot{y}$\n",
        "   \n",
        "   Pour une multiplication $z = x \\cdot y$, on aurait :\n",
        "   - Valeur : $z = x \\cdot y$\n",
        "   - Dérivée : $\\dot{z} = \\dot{x} \\cdot y + x \\cdot \\dot{y}$\n",
        "\n",
        "3. **Résultat** : À la fin de la propagation, on obtient la valeur finale de la fonction ainsi que la valeur de sa dérivée par rapport à l'entrée d'intérêt.\n",
        "\n",
        "### Exemple simple\n",
        "\n",
        "Supposons que nous voulons dériver la fonction $f(x) = x^2 + 3x$ par rapport à $x$.\n",
        "\n",
        "1. **Initialisation** :\n",
        "   - $x = x$\n",
        "   - $\\dot{x} = 1$ (puisque nous dérivons par rapport à $x$)\n",
        "\n",
        "2. **Propagation** :\n",
        "   - Première opération : $u = x^2$\n",
        "     - $u = x \\cdot x$\n",
        "     - $\\dot{u} = \\dot{x} \\cdot x + x \\cdot \\dot{x} = 1 \\cdot x + x \\cdot 1 = 2x$\n",
        "   - Deuxième opération : $v = 3x$\n",
        "     - $v = 3 \\cdot x$\n",
        "     - $\\dot{v} = 3 \\cdot \\dot{x} = 3 \\cdot 1 = 3$\n",
        "   - Troisième opération : $f = u + v$\n",
        "     - $f = u + v$\n",
        "     - $\\dot{f} = \\dot{u} + \\dot{v} = 2x + 3$\n",
        "\n",
        "3. **Résultat** :\n",
        "   - La valeur de la fonction est $f(x) = x^2 + 3x$\n",
        "   - La dérivée de la fonction est $\\dot{f} = 2x + 3$\n",
        "\n",
        "Bon travail et bonne programmation !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ3xVCM1Awj-"
      },
      "source": [
        "### Préliminaire, surcharge des opérateurs et plot du graphe de calcul\n",
        "\n",
        "1. Observer et décrire la classe et le first_test\n",
        "2. Remarquer comment la surcharge d'opérateur permet de propager le graphe de calcul\n",
        "2. Adapter la classe pour rendre le second_test fonctionnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "CvdPt3dOB_QU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GwZUo-DiAwj-",
        "outputId": "4910ca17-cbaf-499b-b1c4-6377fb21fc59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'float' object has no attribute 'data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-325820f25617>\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m### Second test - FIXME: fullfill the requirements in the class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0msecond_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-325820f25617>\u001b[0m in \u001b[0;36msecond_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Perform some operations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-325820f25617>\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorSimple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"({self.name}*{other.name})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'data'"
          ]
        }
      ],
      "source": [
        "class TensorSimple:\n",
        "    graph = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "    def __init__(self, data, name=None):\n",
        "        self.data = np.array(data)\n",
        "        self.name = name\n",
        "        TensorSimple.graph.add_node(self)  # Add the current tensor as a node in the graph\n",
        "        self.operation = None\n",
        "        self.result = None\n",
        "\n",
        "    def __add__(self, other):\n",
        "        result = TensorSimple(self.data + other.data, name=f\"({self.name}+{other.name})\")\n",
        "        self._add_edge(other, result, '+')\n",
        "        return result\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        result = TensorSimple(self.data - other.data, name=f\"({self.name}-{other.name})\")\n",
        "        self._add_edge(other, result, '-')\n",
        "        return result\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        result = TensorSimple(self.data * other.data, name=f\"({self.name}*{other.name})\")\n",
        "        self._add_edge(other, result, '*')\n",
        "        return result\n",
        "\n",
        "    def __truediv__(self, other):\n",
        "        result = TensorSimple(self.data / other.data, name=f\"({self.name}/{other.name})\")\n",
        "        self._add_edge(other, result, '/')\n",
        "        return result\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        result = TensorSimple(self.data ** other.data, name=f\"({self.name}^{other.name})\")\n",
        "        self._add_edge(other, result, '^')\n",
        "        return result\n",
        "\n",
        "    def _add_edge(self, other, result, operation):\n",
        "        TensorSimple.graph.add_edge(self, result, operation=operation)\n",
        "        TensorSimple.graph.add_edge(other, result, operation=operation)\n",
        "        result.operation = operation\n",
        "        result.result = result\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.data)\n",
        "\n",
        "\n",
        "def plot_graph(graph):\n",
        "    pos = nx.spring_layout(graph)\n",
        "    labels = {node: f\"{node.name}\\n{node.data}\" for node in graph.nodes}\n",
        "    edge_labels = {(u, v): f\"{d['operation']}\" for u, v, d in graph.edges(data=True)}\n",
        "    nx.draw(graph, pos, with_labels=True, labels=labels, node_size=2000, node_color='lightblue')\n",
        "    nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels)\n",
        "    plt.show()\n",
        "\n",
        "def second_test():\n",
        "\n",
        "    TensorSimple.graph = nx.DiGraph()\n",
        "\n",
        "    # Create some tensors\n",
        "    a = TensorSimple(2, name='a')\n",
        "    b = TensorSimple(3, name='b')\n",
        "    c = TensorSimple(4, name='c')\n",
        "\n",
        "    # Perform some operations\n",
        "    d = (a + b) * 3.0\n",
        "    e = (d * c) + 1.0\n",
        "    f = e / a\n",
        "\n",
        "    # Draw the computational graph\n",
        "    plot_graph(TensorSimple.graph)\n",
        "\n",
        "def first_test():\n",
        "\n",
        "    TensorSimple.graph = nx.DiGraph()\n",
        "\n",
        "    # Create some tensors\n",
        "    a = TensorSimple(2, name='a')\n",
        "    b = TensorSimple(3, name='b')\n",
        "    c = TensorSimple(4, name='c')\n",
        "\n",
        "    # Perform some operations\n",
        "    d = a + b\n",
        "    e = d * c\n",
        "    f = e / a\n",
        "\n",
        "    # Draw the computational graph\n",
        "    plot_graph(TensorSimple.graph)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    ### Second test - FIXME: fullfill the requirements in the class\n",
        "    second_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90kp4WJ-Awj_"
      },
      "source": [
        "### Autodifférentiation en mode forward\n",
        "\n",
        "### Description du Projet\n",
        "\n",
        "Vous allez implémenter une classe `Tensor` qui représente un nombre dual. Chaque `Tensor` contient deux attributs :\n",
        "- `real` : la partie réelle du nombre.\n",
        "- `dual` : un dictionnaire représentant la partie duale, où les clés sont des indices nommés et les valeurs sont les valeurs duales correspondantes.\n",
        "\n",
        "### Méthodes à Implémenter\n",
        "\n",
        "1. **Constructeur (`__init__`)** :\n",
        "    - Initialise un `Tensor` avec une partie réelle et une partie duale.\n",
        "\n",
        "2. **Addition (`__add__` et `__radd__`)** :\n",
        "    - Permet l'addition de deux tenseurs ou l'addition d'un scalaire à un tenseur.\n",
        "\n",
        "3. **Soustraction (`__sub__` et `__rsub__`)** :\n",
        "    - Permet la soustraction de deux tenseurs ou la soustraction d'un scalaire d'un tenseur.\n",
        "\n",
        "4. **Multiplication (`__mul__` et `__rmul__`)** :\n",
        "    - Permet la multiplication de deux tenseurs ou la multiplication d'un scalaire par un tenseur.\n",
        "\n",
        "5. **Division (`__truediv__` et `__rtruediv__`)** :\n",
        "    - Permet la division de deux tenseurs ou la division d'un scalaire par un tenseur.\n",
        "\n",
        "6. **Puissance (`__pow__`)** :\n",
        "    - Permet d'élever un tenseur à une puissance donnée.\n",
        "\n",
        "7. **Négation (`__neg__`)** :\n",
        "    - Permet de négativer un tenseur.\n",
        "\n",
        "8. **Méthode auxiliaire `div_neg`** :\n",
        "    - Permet de négativer la partie duale d'un tenseur.\n",
        "\n",
        "9. **Représentation en chaîne (`__str__`)** :\n",
        "    - Retourne une représentation en chaîne de caractères du tenseur.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Implémentez la classe `Tensor` avec les méthodes décrites ci-dessus.\n",
        "2. Testez chaque méthode avec des exemples concrets pour vérifier leur bon fonctionnement.\n",
        "3. Documentez votre code et commentez chaque méthode pour expliquer son fonctionnement.\n",
        "\n",
        "### Évaluation\n",
        "\n",
        "Votre projet sera évalué sur les critères suivants :\n",
        "- Fonctionnalité : Toutes les méthodes doivent fonctionner correctement et produire les résultats attendus.\n",
        "- Qualité du code : Votre code doit être bien structuré, lisible et commenté.\n",
        "- Tests : Vous devez ajouter des exemples supplémentaires de tests démontrant le bon fonctionnement de chaque méthode.\n",
        "- Documentation : Votre documentation doit être clair, bien organisé et expliquer votre démarche et vos résultats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6lZzrPDoAwj_",
        "outputId": "a5b1b005-6f12-4ae8-979e-46a818c1c7af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f = 20\n",
            "fa = 6\n",
            "fb = 0\n",
            "fc = 0\n",
            "fd = 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Tensor:\n",
        "\n",
        "    def __init__(self, real, dual):\n",
        "        '''\n",
        "        real: real number\n",
        "        dual: dict (key=name_index and value=value)\n",
        "        '''\n",
        "        self.real = real  # The real part of the dual number\n",
        "        self.dual = dual  # The dual part of the dual number\n",
        "\n",
        "    def __add__(self, argument):\n",
        "\n",
        "        if isinstance(argument, Tensor):\n",
        "\n",
        "            real = self.real + argument.real\n",
        "\n",
        "            dual = {}\n",
        "            for key in self.dual:\n",
        "                dual[key] = self.dual[key]\n",
        "\n",
        "            for key in argument.dual:\n",
        "                if key in dual:\n",
        "                    dual[key] += argument.dual[key]\n",
        "                else:\n",
        "                    dual[key] = argument.dual[key]\n",
        "\n",
        "            return Tensor(real, dual)\n",
        "\n",
        "        else:\n",
        "            return Tensor(self.real + argument, self.dual)\n",
        "\n",
        "    __radd__ = __add__\n",
        "\n",
        "    def __mul__(self, argument):\n",
        "\n",
        "       if isinstance(argument, Tensor):\n",
        "          # Calcul partie réelle\n",
        "          real = self.real * argument.real\n",
        "\n",
        "          # Calcul partie dual\n",
        "          dual = {}\n",
        "          for key in self.dual: # u' * v\n",
        "              dual[key] = self.dual[key] * argument.real\n",
        "\n",
        "          for key in argument.dual:\n",
        "              if key in dual: # + u * v'\n",
        "                  dual[key] += self.real * argument.dual[key]\n",
        "              else: # = u * v'\n",
        "                  dual[key] = self.real * argument.dual[key]\n",
        "\n",
        "          return Tensor(real, dual)\n",
        "\n",
        "       else:\n",
        "          dual = {}\n",
        "          for key in self.dual:\n",
        "              dual[key] = self.dual[key] * argument\n",
        "          return Tensor(self.real * argument, dual)\n",
        "\n",
        "    __rmul__ = __mul__\n",
        "\n",
        "\n",
        "    def __sub__(self, argument):\n",
        "\n",
        "        if isinstance(argument, Tensor):\n",
        "\n",
        "            real = self.real - argument.real\n",
        "\n",
        "            dual = {}\n",
        "            for key in self.dual:\n",
        "                dual[key] = self.dual[key]\n",
        "\n",
        "            for key in argument.dual:\n",
        "                if key in dual:\n",
        "                    dual[key] -= argument.dual[key]\n",
        "                else:\n",
        "                    dual[key] = -argument.dual[key]\n",
        "\n",
        "            return Tensor(real, dual)\n",
        "\n",
        "        else:\n",
        "            return Tensor(self.real - argument, self.dual)\n",
        "\n",
        "    __rsub__ = __sub__\n",
        "\n",
        "\n",
        "    def div_neg(self, argument):\n",
        "        # FIXME: implement the division of a tensor by a negative number\n",
        "        dual = {}\n",
        "        for key in argument.dual:\n",
        "            dual[key] = argument.dual[key] * (-1)\n",
        "        return Tensor(argument.real, dual)\n",
        "\n",
        "\n",
        "    def __truediv__(self, argument):\n",
        "        if isinstance(argument, Tensor):\n",
        "\n",
        "            real = self.real / argument.real\n",
        "\n",
        "            dual = {}\n",
        "            for key in self.dual:\n",
        "                dual[key] = self.dual[key] * argument.real\n",
        "\n",
        "            for key in argument.dual:\n",
        "                if key in dual:\n",
        "                    dual[key] -= self.real * argument.dual[key]\n",
        "                    dual[key] /= argument.real ** 2\n",
        "                else:\n",
        "                    dual[key] = - self.real * argument.dual[key]\n",
        "                    dual[key] /= argument.real ** 2\n",
        "\n",
        "            return Tensor(real, dual)\n",
        "\n",
        "        else:\n",
        "            dual = {}\n",
        "            for key in self.dual:\n",
        "                dual[key] = self.dual[key] / argument\n",
        "            return Tensor(self.real / argument, dual)\n",
        "\n",
        "    __rtruediv__ = __truediv__\n",
        "\n",
        "    def __pow__(self, power):\n",
        "        if isinstance(power, Tensor):\n",
        "          # u^v <=> (e^ln(u))^v <=> e^(ln(u)*v)\n",
        "          # (u^v)' = [e^(ln(u)*v)]' = (ln(u)*v)' * e^(ln(u)*v)\n",
        "          #  ...   = [ln(u)' * v + ln(u) * v'] * e^(ln(u)*v)\n",
        "          #  ...   = [u'/u * v + ln(u) * v'] * e^(ln(u)*v)\n",
        "          # Calcul partie réelle\n",
        "          real = self.real ** power.real\n",
        "\n",
        "          # Calcul partie dual\n",
        "          dual = {}\n",
        "          for key in self.dual: # = e^(ln(u)*v)\n",
        "              dual[key] = np.exp(np.log(self.real) * power.real)\n",
        "\n",
        "          for key in power.dual:\n",
        "              if key in dual: # *= [u'/u * v + ln(u) * v']\n",
        "                  dual[key] *= (self.dual[key] / self.real * power.real + np.log(self.real) * power.dual[key])\n",
        "              else: # *= [ln(u) * v']\n",
        "                  dual[key] *= (np.log(self.real) * power.dual[key])\n",
        "\n",
        "          return Tensor(real, dual)\n",
        "\n",
        "        else:\n",
        "          # n*u'*u^(n-1)\n",
        "          real = self.real ** power\n",
        "\n",
        "          dual = {}\n",
        "          for key in self.dual:\n",
        "              dual[key] = power * self.dual[key] * self.real ** (power - 1)\n",
        "\n",
        "          return Tensor(real, dual)\n",
        "\n",
        "\n",
        "    def __neg__(self):\n",
        "        real = -self.real\n",
        "        dual = {}\n",
        "        for key in self.dual:\n",
        "            dual[key] = -self.dual[key]\n",
        "\n",
        "        return Tensor(real, dual)\n",
        "\n",
        "    def __str__(self):\n",
        "        s = 'f = ' + str(round(self.real,6)) + '\\n'\n",
        "        for key in self.dual:\n",
        "            s += 'f' + key + ' = ' + str(round(self.dual[key],6)) + '\\n'\n",
        "        return s\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  a = Tensor(2, {\"a\": 1})\n",
        "  b = Tensor(3, {\"b\": 0})\n",
        "  c = Tensor(4, {\"c\": 0})\n",
        "  d = Tensor(2, {\"d\": 0})\n",
        "\n",
        "  res = (a * b + c) * d\n",
        "  print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Hy9v9DgIAwkA",
        "outputId": "c4b5ee5b-59b1-4f02-eacd-2d4f3231a6a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f = 4\n",
            "fW = 2\n",
            "fb = 1\n",
            "\n",
            "f = 72\n",
            "fW = 42\n",
            "fb = 9\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Test d'autodifférentiation pour l'addition et la multiplication\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    ## Test the class\n",
        "    # f(x) = x*W + b\n",
        "    # c(x,y) = f(x) - y\n",
        "    # c(x,y) = x*W + b - y\n",
        "    # c'(x,y) = x'*W + x*W' + b' - y'\n",
        "\n",
        "    ## Parameters\n",
        "    W = Tensor(3, {'W': 1})\n",
        "    b = Tensor(2, {'b': 1})\n",
        "\n",
        "    ## Constantes\n",
        "    x = 2\n",
        "    y = 4\n",
        "\n",
        "    f = x*W + b\n",
        "    cost = f - y\n",
        "    print(cost)\n",
        "\n",
        "    ############################\n",
        "\n",
        "    ## Constantes\n",
        "    x_anchor = 2\n",
        "    x_pos = 4\n",
        "    x_neg = 1\n",
        "\n",
        "    # f(x) = aW + b\n",
        "    f_anchor = x_anchor*W + b\n",
        "    f_pos = x_pos*W + b\n",
        "    f_neg = x_neg*W + b\n",
        "\n",
        "    # c(x,y) = f(x) - y\n",
        "    cost =  (f_anchor * f_pos) - (f_anchor * f_neg)\n",
        "\n",
        "    print(cost)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Test d'autodifférentiation pour l'addition et la multiplication\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    ## Test the class\n",
        "    # f(x) = W**x / b\n",
        "    # c(x,y) = f(x) - y\n",
        "    # c(x,y) = W**x / b - y\n",
        "    #\n",
        "    # résultats espérés : (cost)\n",
        "    # f = 3**2 / 2 - 4 = 9/2 - 4 = 4.5 - 4 = 0.5\n",
        "    # fW =\n",
        "    # fb =\n",
        "    #\n",
        "    # (W**x)' = x * W' * W^(x - 1)\n",
        "    #\n",
        "    # (W**x / b)' = [(W**x)' * b - W**x * b'] / b**2\n",
        "    # (W**x / b)' = [x * W' * W^(x - 1) * b - W**x * b'] / b**2\n",
        "    #\n",
        "    # c(x,y)' = [x * W' * W^(x - 1) * b - W**x * b'] / b**2 - y'\n",
        "\n",
        "    ## Parameters\n",
        "    W = Tensor(3, {'W': 1})\n",
        "    b = Tensor(2, {'b': 1})\n",
        "\n",
        "    ## Constantes\n",
        "    x = 2\n",
        "    y = 4\n",
        "\n",
        "    f = W**x / b\n",
        "    cost = f - y\n",
        "    print(cost)\n"
      ],
      "metadata": {
        "id": "b0vrg5g9TGca",
        "outputId": "3120d048-2398-4614-f8a3-c14cd3ef86ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f = 0.5\n",
            "fW = 12\n",
            "fb = -2.25\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlT2VSu5AwkA"
      },
      "source": [
        "### Fonctions Mathématiques avec des Nombres Duals pour l'Autodifférentiation\n",
        "\n",
        "### Objectif\n",
        "\n",
        "L'objectif de cette partie du TP est d'implémenter des fonctions mathématiques couramment utilisées dans le contexte de l'autodifférentiation en mode forward en utilisant des nombres duals. Vous allez créer des fonctions qui calculent à la fois la valeur d'une fonction mathématique et sa dérivée, représentée par la partie duale.\n",
        "Vous allez implémenter plusieurs fonctions mathématiques en utilisant des nombres duals représentés par la classe `Tensor`. Chaque fonction doit calculer la valeur de la fonction et la dérivée, et renvoyer un objet `Tensor` contenant ces informations.\n",
        "\n",
        "### Fonctions à Implémenter\n",
        "\n",
        "1. **Logarithme naturel (`log_d`)** :\n",
        "    - Calcule la valeur du logarithme naturel et sa dérivée.\n",
        "\n",
        "2. **Exponentielle (`exp_d`)** :\n",
        "    - Calcule la valeur de l'exponentielle et sa dérivée.\n",
        "\n",
        "3. **Sinus (`sin_d`)** :\n",
        "    - Calcule la valeur du sinus et sa dérivée.\n",
        "\n",
        "4. **Cosinus (`cos_d`)** :\n",
        "    - Calcule la valeur du cosinus et sa dérivée.\n",
        "\n",
        "5. **Sigmoïde (`sigmoid_d`)** :\n",
        "    - Calcule la valeur de la fonction sigmoïde et sa dérivée.\n",
        "\n",
        "6. **Tangente hyperbolique (`tanh_d`)** :\n",
        "    - Calcule la valeur de la tangente hyperbolique et sa dérivée.\n",
        "\n",
        "7. **Tangente (`tan_d`)** :\n",
        "    - Calcule la valeur de la tangente et sa dérivée.\n",
        "\n",
        "8. **Racine carrée (`sqrt_d`)** :\n",
        "    - Calcule la valeur de la racine carrée et sa dérivée.\n",
        "\n",
        "9. **Puissance (`pow_d`)** :\n",
        "    - Calcule la valeur de la fonction puissance et sa dérivée.\n",
        "\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Implémentez les fonctions décrites ci-dessus.\n",
        "2. Testez chaque fonction avec des exemples concrets pour vérifier leur bon fonctionnement.\n",
        "3. Documentez votre code et commentez chaque fonction pour expliquer son fonctionnement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tQ_xQlAgAwkA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def log_d(dual_number):\n",
        "    dual = {}\n",
        "    a = dual_number.real\n",
        "    sa = np.log(a)\n",
        "    for key in dual_number.dual:\n",
        "        dual[key] = dual_number.dual[key]/a\n",
        "    return Tensor(sa, dual)\n",
        "\n",
        "def exp_d(dual_number):\n",
        "    dual = {}\n",
        "    a = dual_number.real\n",
        "    sa = np.exp(a)\n",
        "    for key in dual_number.dual:\n",
        "        dual[key] = dual_number.dual[key]*sa\n",
        "    return Tensor(sa, dual)\n",
        "\n",
        "def sin_d(dual_number):\n",
        "    #FIXME: implement the derivative of sin\n",
        "    dual = {}\n",
        "    a = dual_number.real\n",
        "    sa = np.sin(a)\n",
        "    for key in dual_number.dual:\n",
        "        dual[key] = dual_number.dual[key] * np.cos(a)\n",
        "    return Tensor(sa, dual)\n",
        "\n",
        "def cos_d(dual_number):\n",
        "    #FIXME: implement the derivative of cos\n",
        "    dual = {}\n",
        "    a = dual_number.real\n",
        "    sa = np.cos(a)\n",
        "    for key in dual_number.dual:\n",
        "        dual[key] = - dual_number.dual[key] * np.sin(a)\n",
        "    return Tensor(sa, dual)\n",
        "\n",
        "def sigmoid_d(dual_number):\n",
        "    #FIXME: implement the derivative of sigmoid\n",
        "    dual = {}\n",
        "    a = dual_number.real\n",
        "    sa = 1 / (1 + np.exp(-a))\n",
        "    for key in dual_number.dual:\n",
        "        dual[key] = dual_number.dual[key] * np.exp(-a) / ((1 + np.exp(-a))**2)\n",
        "    return Tensor(sa, dual)\n",
        "\n",
        "def tanh_d(dual_number):\n",
        "    #FIXME: implement the derivative of tanh\n",
        "    dual = {}\n",
        "    a = dual_number.real\n",
        "    sa = np.tanh(a)\n",
        "    for key in dual_number.dual:\n",
        "        dual[key] = 1 - sa**2\n",
        "    return Tensor(sa, dual)\n",
        "\n",
        "def tan_d(dual_number):\n",
        "    #FIXME: implement the derivative of tan\n",
        "    dual = {}\n",
        "    a = dual_number.real\n",
        "    sa = np.tan(a)\n",
        "    for key in dual_number.dual:\n",
        "        dual[key] = dual_number.dual[key] * (1 + sa**2)\n",
        "    return Tensor(sa, dual)\n",
        "\n",
        "def sqrt_d(dual_number):\n",
        "    #FIXME: implement the derivative of sqrt\n",
        "    dual = {}\n",
        "    a = dual_number.real\n",
        "    sa = np.sqrt(a)\n",
        "    for key in dual_number.dual:\n",
        "        dual[key] = dual_number.dual[key] / (2 * sa)\n",
        "    return Tensor(sa, dual)\n",
        "\n",
        "def pow_d(dual_number, power):\n",
        "    #FIXME: implement the derivative of pow\n",
        "    dual = {}\n",
        "    a = dual_number.real\n",
        "    sa = a ** power\n",
        "    for key in dual_number.dual:\n",
        "        dual[key] = power * dual_number.dual[key] * a**(power - 1)\n",
        "    return Tensor(sa, dual)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n36rZNFLAwkA"
      },
      "source": [
        "### Main test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZQ-3A4CaAwkA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Define the function for which we want to compute the gradient\n",
        "def f(x):\n",
        "    return x**2 + 2*x + 1\n",
        "\n",
        "# Cubic function\n",
        "def f7(x):\n",
        "    return x**3 + 3*x**2 - 2*x + 5\n",
        "\n",
        "# Quartic function\n",
        "def f8(x):\n",
        "    return x**4 + 4*x**3 - 3*x**2 + 6*x + 7\n",
        "\n",
        "# Quintic function\n",
        "def f9(x):\n",
        "    return x**5 + 5*x**4 - 4*x**3 + 8*x**2 + 9*x + 10\n",
        "\n",
        "# Exponential function\n",
        "def f10(x):\n",
        "    return exp_d(x)\n",
        "\n",
        "def f10_torch(x):\n",
        "    return torch.exp(x)\n",
        "\n",
        "# Logarithmic function\n",
        "def f11(x):\n",
        "    return log_d(x)\n",
        "\n",
        "def f11_torch(x):\n",
        "    return torch.log(x)\n",
        "\n",
        "# Sinusoidal function\n",
        "def f12(x):\n",
        "    return sin_d(x)\n",
        "\n",
        "def f12_torch(x):\n",
        "    return torch.sin(x)\n",
        "\n",
        "# Cosinusoidal function\n",
        "def f13(x):\n",
        "    return cos_d(x)\n",
        "\n",
        "def f13_torch(x):\n",
        "    return torch.cos(x)\n",
        "\n",
        "# Tangent function\n",
        "def f14(x):\n",
        "    return tan_d(x)\n",
        "\n",
        "def f14_torch(x):\n",
        "    return torch.tan(x)\n",
        "\n",
        "# Hyperbolic tangent function\n",
        "def f17(x):\n",
        "    return tanh_d(x)\n",
        "\n",
        "def f17_torch(x):\n",
        "    return torch.tanh(x)\n",
        "\n",
        "# Inverse function\n",
        "def f18(x): # FIXME : this one gives out wrong result ?\n",
        "    return 1/x\n",
        "\n",
        "# Square root function\n",
        "def f19(x):\n",
        "    return sqrt_d(x)\n",
        "\n",
        "def f19_torch(x):\n",
        "    return torch.sqrt(x)\n",
        "\n",
        "# Natural logarithm function\n",
        "def f22(x):\n",
        "    return log_d(x)\n",
        "\n",
        "def f22_torch(x):\n",
        "    return torch.log(x)\n",
        "\n",
        "# Define the function for which we want to compute the gradient\n",
        "def f(x):\n",
        "    return x**2 + 2*x + 1\n",
        "\n",
        "# Cubic function with sin\n",
        "def f27(x):\n",
        "    return x**3 + 3*x**2 - 2*x + sin_d(x) + 5\n",
        "\n",
        "def f27_torch(x):\n",
        "    return x**3 + 3*x**2 - 2*x + torch.sin(x) + 5\n",
        "\n",
        "# Quartic function with cos\n",
        "def f28(x):\n",
        "    return x**4 + 4*x**3 - 3*x**2 + 6*x + cos_d(x) + 7\n",
        "\n",
        "def f28_torch(x):\n",
        "    return x**4 + 4*x**3 - 3*x**2 + 6*x + torch.cos(x) + 7\n",
        "\n",
        "# Quintic function with sin and cos\n",
        "def f29(x):\n",
        "    return x**5 + 5*x**4 - 4*x**3 + 8*x**2 + 9*x + sin_d(x) + cos_d(x) + 10\n",
        "\n",
        "def f29_torch(x):\n",
        "    return x**5 + 5*x**4 - 4*x**3 + 8*x**2 + 9*x + torch.sin(x) + torch.cos(x) + 10\n",
        "\n",
        "# Function with sin^2 and cos^2\n",
        "def f30(x):\n",
        "    return sin_d(x)**2 + cos_d(x)**2 + x**2\n",
        "\n",
        "def f30_torch(x):\n",
        "    return torch.sin(x)**2 + torch.cos(x)**2 + x**2\n",
        "\n",
        "# Function with sin^3 and cos^3\n",
        "def f31(x):\n",
        "    return sin_d(x)**3 + cos_d(x)**3 + x**3\n",
        "\n",
        "def f31_torch(x):\n",
        "    return torch.sin(x)**3 + torch.cos(x)**3 + x**3\n",
        "\n",
        "### define a list of functions\n",
        "functions = [f, f7, f8, f9, f10, f11, f12, f13, f14, f17, f18, f19, f22, f27, f28, f29, f30, f31]\n",
        "functions_torch = [f, f7, f8, f9, f10_torch, f11_torch, f12_torch, f13_torch, f14_torch, f17_torch, f18, f19_torch, f22_torch, f27_torch, f28_torch, f29_torch, f30_torch, f31_torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XK6RpyZVAwkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b550c4-eb91-433b-c22a-0f57ed7abe26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient computed by Forward Mode: {'x': 2.2}\n",
            "Gradient computed by PyTorch: 2.200000047683716\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': -1.3699999999999999}\n",
            "Gradient computed by PyTorch: -1.3700000047683716\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': 5.524}\n",
            "Gradient computed by PyTorch: 5.52400016784668\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': 10.5005}\n",
            "Gradient computed by PyTorch: 10.500500679016113\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': 1.1051709180756477}\n",
            "Gradient computed by PyTorch: 1.1051709651947021\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': 10.0}\n",
            "Gradient computed by PyTorch: 10.0\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': 0.9950041652780258}\n",
            "Gradient computed by PyTorch: 0.9950041770935059\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': -0.09983341664682815}\n",
            "Gradient computed by PyTorch: -0.0998334214091301\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': 1.0100670464224948}\n",
            "Gradient computed by PyTorch: 1.010067105293274\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': 0.9900662908474398}\n",
            "Gradient computed by PyTorch: 0.9900662899017334\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': 1.0}\n",
            "Gradient computed by PyTorch: -100.0\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': 1.5811388300841895}\n",
            "Gradient computed by PyTorch: 1.5811388492584229\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': 10.0}\n",
            "Gradient computed by PyTorch: 10.0\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': -0.37499583472197406}\n",
            "Gradient computed by PyTorch: -0.37499579787254333\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': 5.424166583353172}\n",
            "Gradient computed by PyTorch: 5.424166679382324\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': 11.395670748631199}\n",
            "Gradient computed by PyTorch: 11.395671844482422\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': 0.2}\n",
            "Gradient computed by PyTorch: 0.20000000298023224\n",
            "\n",
            "Gradient computed by Forward Mode: {'x': -0.23676446036681106}\n",
            "Gradient computed by PyTorch: -0.23676446080207825\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Create a tensor with a single value\n",
        "    x = Tensor(0.1, {'x': 1})\n",
        "\n",
        "    for f, f_torch in zip(functions, functions_torch):\n",
        "        # Compute the gradient using Wolf Forward Mode\n",
        "        x_prime = f(x)\n",
        "        print(\"Gradient computed by Forward Mode:\", x_prime.dual)\n",
        "\n",
        "        # Convert the tensor to a PyTorch tensor\n",
        "        x_torch = torch.tensor(x.real)\n",
        "        x_torch.requires_grad_(True)\n",
        "        y_torch = f_torch(x_torch)\n",
        "        y_torch.backward()\n",
        "        print(\"Gradient computed by PyTorch:\", x_torch.grad.item())\n",
        "        print()\n",
        "\n",
        "        #assert np.isclose(x_prime.dual['x'], x_torch.grad.item(), atol=1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue-ylWyiAwkA"
      },
      "source": [
        "### Test d'autodifférentiation avec deux variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0sBnRqPAwkA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "# Quartic function with cos\n",
        "def f1(x, y):\n",
        "    return x**4 + 6*y\n",
        "\n",
        "def f1_torch(x, y):\n",
        "    return x**4 + 6*y\n",
        "\n",
        "# Quintic function with sin and cos\n",
        "def f2(x, y):\n",
        "    return x**5 + 5*y**4 - 4*x**3 + 8*x**2 + 9*x + sin_d(y) + cos_d(x) + 10\n",
        "\n",
        "def f2_torch(x, y):\n",
        "    return x**5 + 5*y**4 - 4*x**3 + 8*x**2 + 9*x + torch.sin(y) + torch.cos(x) + 10\n",
        "\n",
        "# Function with sin^2 and cos^2\n",
        "def f3(x, y):\n",
        "    return sin_d(y)**2 + cos_d(x)**2 + y**3\n",
        "\n",
        "# Function with sin^2 and cos^2\n",
        "def f3_torch(x, y):\n",
        "    return torch.sin(y)**2 + torch.cos(x)**2 + y**3\n",
        "\n",
        "functions = [f1, f2, f3]\n",
        "functions_torch = [f1_torch, f2_torch, f3_torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJr7AhH_AwkB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Create a tensor with a single value\n",
        "    for f, f_torch in zip(functions, functions_torch):\n",
        "\n",
        "        x = Tensor(0.1, {'x': 1.0})\n",
        "        y = Tensor(0.2, {'y': 1.0})\n",
        "\n",
        "        # Compute the gradient using Wolf Forward Mode\n",
        "        z = f(x, y)\n",
        "        print(\"Gradient computed by Wolf Forward Mode:\", z.dual)\n",
        "\n",
        "        # Convert the tensor to a PyTorch tensor\n",
        "        x_torch = torch.tensor(x.real)\n",
        "        x_torch.requires_grad_(True)\n",
        "\n",
        "        y_torch = torch.tensor(y.real)\n",
        "        y_torch.requires_grad_(True)\n",
        "\n",
        "        z_torch = f_torch(x_torch, y_torch)\n",
        "        z_torch.backward()\n",
        "        print(f\"Gradient computed by PyTorch - x.grad: {x_torch.grad.item()} y.grad: {y_torch.grad.item()}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4dzqUncAwkB"
      },
      "source": [
        "### Réseau de neurones multi-couche simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWn155XEAwkB"
      },
      "outputs": [],
      "source": [
        "# define a neural network, MLP with 1 hidden layer\n",
        "def func_nn(x):\n",
        "    W1 = Tensor(0.1, {'W1': 1})\n",
        "    b1 = Tensor(0.2, {'b1': 1})\n",
        "    W2 = Tensor(0.3, {'W2': 1})\n",
        "    b2 = Tensor(0.4, {'b2': 1})\n",
        "\n",
        "    # FIXME: Implement the forward pass of the neural network\n",
        "    return y\n",
        "\n",
        "\n",
        "## with torch\n",
        "def func_nn_torch(x):\n",
        "    W1 = torch.tensor(0.1, requires_grad=True)\n",
        "    b1 = torch.tensor(0.2, requires_grad=True)\n",
        "    W2 = torch.tensor(0.3, requires_grad=True)\n",
        "    b2 = torch.tensor(0.4, requires_grad=True)\n",
        "\n",
        "    # FIXME: Implement the forward pass of the neural network\n",
        "    return y\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Create a tensor with a single value\n",
        "    x = Tensor(0.1, {'x': 1})\n",
        "\n",
        "    # Compute the gradient using Wolf Forward Mode\n",
        "    y = func_nn(x)\n",
        "    print(\"Gradient computed by Wolf Forward Mode:\", y.dual)\n",
        "\n",
        "    # Convert the tensor to a PyTorch tensor\n",
        "    x_torch = torch.tensor(x.real)\n",
        "    x_torch.requires_grad_(True)\n",
        "    y_torch = func_nn_torch(x_torch)\n",
        "    y_torch.backward()\n",
        "    print(\"Gradient computed by PyTorch:\", x_torch.grad.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SndDrZIJAwkB"
      },
      "source": [
        "### Descente de gradient stochastique sur un réseau de neurone multi-couches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv54f99MAwkB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# define a neural network, MLP with 1 hidden layer\n",
        "def func_nn(x, W1, b1, W2, b2):\n",
        "    # FIXME: Implement the forward pass of the neural network\n",
        "    return y\n",
        "\n",
        "def mse(y, y_hat):\n",
        "    # FIXME: Implement the mean squared error\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    ## generate data\n",
        "    np.random.seed(0)\n",
        "    x = np.linspace(-2, 2, 100)\n",
        "    y = 2*x + 1 + np.random.randn(100)*0.1\n",
        "\n",
        "    ## Parameters\n",
        "    W1 = Tensor(0.1, {'W1': 1})\n",
        "    b1 = Tensor(0.2, {'b1': 1})\n",
        "    W2 = Tensor(0.3, {'W2': 1})\n",
        "    b2 = Tensor(0.4, {'b2': 1})\n",
        "\n",
        "    lr = 0.01\n",
        "    nb_epoch = 100\n",
        "    for _ in range(nb_epoch):\n",
        "\n",
        "        lst_loss = []\n",
        "        for i in range(len(x)):\n",
        "            x_i = Tensor(x[i], {'x': 1})\n",
        "            y_i = Tensor(y[i], {'y': 1})\n",
        "\n",
        "            y_hat = func_nn(x_i, W1, b1, W2, b2)\n",
        "            loss = mse(y_i, y_hat)\n",
        "            lst_loss.append(loss.real)\n",
        "\n",
        "            # Compute the gradient\n",
        "            W1 = W1 - lr*loss.dual['W1']\n",
        "            b1 = b1 - lr*loss.dual['b1']\n",
        "            W2 = W2 - lr*loss.dual['W2']\n",
        "            b2 = b2 - lr*loss.dual['b2']\n",
        "\n",
        "        print(np.mean(lst_loss))\n",
        "\n",
        "        # learning rate decay\n",
        "        lr *= 0.1"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llms",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}