{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naVSFXGCgWwb"
      },
      "source": [
        "## TP : Implémentation de l'Autodifférentiation en Mode Reverse (Backpropagation) par surcharge d'opérateurs\n",
        "\n",
        "### Objectif\n",
        "\n",
        "L'objectif de ce TP est d'implémenter l'autodifférentiation en mode reverse (aussi appelé backpropagation) en utilisant la surcharge d'opérateurs. Vous allez compléter une classe Python qui représente un tenseur et implémenter les opérations arithmétiques de base (addition, soustraction, multiplication, division, puissance) ainsi que la différentiation automatique en mode reverse.\n",
        "\n",
        "### Principe de la méthode d'autodifférentiation en mode reverse (Backpropagation)\n",
        "\n",
        "L'autodifférentiation (ou différentiation automatique) est une technique utilisée pour calculer les dérivées de fonctions définies par des programmes informatiques. Contrairement aux méthodes de dérivation symbolique ou numérique, l'autodifférentiation est précise et efficace. Il existe deux modes principaux : le mode forward et le mode reverse.\n",
        "\n",
        "En mode reverse (ou backpropagation), l'idée est de propager des valeurs de dérivées à travers chaque opération élémentaire de la fonction, mais dans le sens inverse de l'évaluation de la fonction. Voici comment cela fonctionne :\n",
        "\n",
        "1. **Évaluation de la fonction** : On évalue la fonction en utilisant les valeurs d'entrée.\n",
        "\n",
        "2. **Initialisation des dérivées sortantes** : Pour chaque variable de sortie, on associe une dérivée sortante initiale. Habituellement, on utilise 1 pour la variable de sortie d'intérêt et 0 pour les autres.\n",
        "\n",
        "3. **Propagation inverse (Backpropagation)** : À partir des dérivées sortantes, on calcule les dérivées entrantes pour chaque opération élémentaire, en utilisant la règle de dérivation des fonctions composées (aussi appelée règle de la chaîne).\n",
        "\n",
        "Par exemple, pour une opération élémentaire $z = x + y$, on aurait :\n",
        "   - Dérivée entrante pour $x$ : $\\frac{dz}{dx} = \\frac{dz}{dz} \\cdot \\frac{dz}{dx} = 1 \\cdot 1 = 1$\n",
        "   - Dérivée entrante pour $y$ : $\\frac{dz}{dy} = \\frac{dz}{dz} \\cdot \\frac{dz}{dy} = 1 \\cdot 1 = 1$\n",
        "\n",
        "   Pour une multiplication $z = x \\cdot y$, on aurait :\n",
        "   - Dérivée entrante pour $x$ : $\\frac{dz}{dx} = \\frac{dz}{dz} \\cdot \\frac{dz}{dx} = 1 \\cdot y = y$\n",
        "   - Dérivée entrante pour $y$ : $\\frac{dz}{dy} = \\frac{dz}{dz} \\cdot \\frac{dz}{dy} = 1 \\cdot x = x$\n",
        "\n",
        "4. **Résultat** : À la fin de la propagation inverse, on obtient les dérivées de la fonction par rapport à chaque variable d'entrée.\n",
        "\n",
        "### Exemple simple\n",
        "\n",
        "Supposons que nous voulons dériver la fonction $f(x) = x^2 + 3x$ par rapport à $x$ en utilisant la backpropagation.\n",
        "\n",
        "1. **Évaluation de la fonction** :\n",
        "   - $x = x$\n",
        "   - $u = x^2$\n",
        "   - $v = 3x$\n",
        "   - $f = u + v$\n",
        "\n",
        "2. **Initialisation des dérivées sortantes** :\n",
        "   - $\\frac{df}{df} = 1$ (puisque nous dérivons par rapport à $f$)\n",
        "\n",
        "3. **Propagation inverse (Backpropagation)** :\n",
        "   - Première opération inverse : $f = u + v$\n",
        "     - $\\frac{du}{df} = \\frac{du}{du} \\cdot \\frac{du}{df} = 1 \\cdot 1 = 1$\n",
        "     - $\\frac{dv}{df} = \\frac{dv}{dv} \\cdot \\frac{dv}{df} = 1 \\cdot 1 = 1$\n",
        "   - Deuxième opération inverse : $u = x^2$\n",
        "     - $\\frac{dx}{du} = \\frac{dx}{dx} \\cdot \\frac{dx}{du} = 1 \\cdot 2x = 2x$\n",
        "   - Troisième opération inverse : $v = 3x$\n",
        "     - $\\frac{dx}{dv} = \\frac{dx}{dx} \\cdot \\frac{dx}{dv} = 1 \\cdot 3 = 3$\n",
        "\n",
        "4. **Résultat** :\n",
        "   - La valeur de la fonction est $f(x) = x^2 + 3x$\n",
        "   - La dérivée de la fonction est $\\frac{df}{dx} = 2x + 3$\n",
        "\n",
        "### Description du Projet\n",
        "\n",
        "Vous allez implémenter une classe `Tensor` qui représente un tenseur. Chaque `Tensor` contient deux attributs :\n",
        "- `value` : la valeur du tenseur.\n",
        "- `gradients` : un dictionnaire représentant les gradients (dérivées), où les clés sont des instances de `Tensor` et les valeurs sont les gradients correspondants.\n",
        "\n",
        "### Méthodes à Implémenter\n",
        "\n",
        "1. **Constructeur (`__init__`)** :\n",
        "    - Initialise un `Tensor` avec une valeur et un dictionnaire de gradients vides.\n",
        "\n",
        "2. **Évaluation (`eval`)** :\n",
        "    - Évalue le tenseur et retourne sa valeur.\n",
        "\n",
        "3. **Addition (`__add__` et `__radd__`)** :\n",
        "    - Permet l'addition de deux tenseurs ou l'addition d'un scalaire à un tenseur.\n",
        "\n",
        "4. **Soustraction (`__sub__` et `__rsub__`)** :\n",
        "    - Permet la soustraction de deux tenseurs ou la soustraction d'un scalaire d'un tenseur.\n",
        "\n",
        "5. **Multiplication (`__mul__` et `__rmul__`)** :\n",
        "    - Permet la multiplication de deux tenseurs ou la multiplication d'un scalaire par un tenseur.\n",
        "\n",
        "6. **Division (`__truediv__` et `__rtruediv__`)** :\n",
        "    - Permet la division de deux tenseurs ou la division d'un scalaire par un tenseur.\n",
        "\n",
        "7. **Puissance (`__pow__`)** :\n",
        "    - Permet d'élever un tenseur à une puissance donnée.\n",
        "\n",
        "8. **Négation (`__neg__`)** :\n",
        "    - Permet de négativer un tenseur.\n",
        "\n",
        "9. **Méthode auxiliaire `zero_grad`** :\n",
        "    - Permet de remettre à zéro tous les gradients.\n",
        "\n",
        "10. **Méthode auxiliaire `backward`** :\n",
        "    - Permet d'effectuer la propagation inverse (backpropagation) à partir d'un gradient sortant donné.\n",
        "\n",
        "11. **Représentation en chaîne (`__str__`)** :\n",
        "    - Retourne une représentation en chaîne de caractères du tenseur.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Implémentez la classe `Tensor` avec les méthodes décrites ci-dessus.\n",
        "2. Testez chaque méthode avec des exemples concrets pour vérifier leur bon fonctionnement.\n",
        "3. Documentez votre code et commentez chaque méthode pour expliquer son fonctionnement.\n",
        "\n",
        "### Évaluation\n",
        "\n",
        "Votre projet sera évalué sur les critères suivants :\n",
        "- Fonctionnalité : Toutes les méthodes doivent fonctionner correctement et produire les résultats attendus.\n",
        "- Qualité du code : Votre code doit être bien structuré, lisible et commenté.\n",
        "- Tests : Vous devez fournir des exemples de tests démontrant le bon fonctionnement de chaque méthode.\n",
        "- Documentation : Votre documentation doit être clair, bien organisé et expliquer votre démarche et vos résultats.\n",
        "\n",
        "Bon travail et bonne programmation !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq-uSks4gWwc"
      },
      "source": [
        "### Préliminaire, surcharge des opérateurs et plot du graphe de calcul\n",
        "\n",
        "Dans cet exercice, vous allez créer une classe `Tensor` en Python qui représente un tenseur scalaire et son gradient. Vous allez ensuite définir plusieurs opérations sur ces tenseurs et visualiser le graphe de calcul associé à ces opérations à l'aide de la bibliothèque `graphviz`.\n",
        "\n",
        "#### Instructions\n",
        "\n",
        "1. **Définition de la Classe `Tensor`**:\n",
        "    - Implémentez la classe `Tensor` avec un constructeur pour initialiser les données et les variables internes pour la construction du graphe d'autograd.\n",
        "    - Définissez les opérations suivantes pour la classe `Tensor` :\n",
        "        - Addition (`__add__`)\n",
        "        - Multiplication (`__mul__`)\n",
        "        - Puissance (`__pow__`)\n",
        "        - Fonction ReLU (`relu`)\n",
        "        - Négation (`__neg__`)\n",
        "        - Soustraction (`__sub__`)\n",
        "        - Division (`__truediv__`)\n",
        "    - Implémentez la méthode `backward` pour effectuer la rétropropagation et calculer les gradients.\n",
        "\n",
        "2. **Création et Visualisation du Graphe de Calcul**:\n",
        "    - Créez des instances de `Tensor` pour les valeurs 1.0, 2.0 et 3.0.\n",
        "    - Effectuez les opérations suivantes sur ces tenseurs :\n",
        "        - `a = x + y`\n",
        "        - `b = a * z`\n",
        "        - `c = b ** 2`\n",
        "        - `d = c.relu()`\n",
        "    - Utilisez la méthode `plot_graph` pour visualiser le graphe de calcul des tenseurs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pHygOZv1gWwc",
        "outputId": "77def3d0-d055-4805-aa91-ea6169c4d695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(data=3.0) Op: \n",
            "Tensor(data=2.0) Op: \n",
            "Tensor(data=1.0) Op: \n",
            "Tensor(data=3.0) Op: +\n",
            "Tensor(data=9.0) Op: *\n",
            "Tensor(data=81.0) Op: **2\n",
            "Tensor(data=81.0) Op: ReLU\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"488pt\" height=\"332pt\"\n viewBox=\"0.00 0.00 488.38 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-328 484.38,-328 484.38,4 -4,4\"/>\n<!-- 139833376858336 -->\n<g id=\"node1\" class=\"node\">\n<title>139833376858336</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"190.69\" cy=\"-18\" rx=\"115.88\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"190.69\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Tensor(data=81.0) Op: ReLU</text>\n</g>\n<!-- 139833376862272 -->\n<g id=\"node2\" class=\"node\">\n<title>139833376862272</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"190.69\" cy=\"-90\" rx=\"107.48\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"190.69\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">Tensor(data=81.0) Op: **2</text>\n</g>\n<!-- 139833376862272&#45;&gt;139833376858336 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139833376862272&#45;&gt;139833376858336</title>\n<path fill=\"none\" stroke=\"black\" d=\"M190.69,-71.7C190.69,-63.98 190.69,-54.71 190.69,-46.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"194.19,-46.1 190.69,-36.1 187.19,-46.1 194.19,-46.1\"/>\n</g>\n<!-- 139833376850080 -->\n<g id=\"node3\" class=\"node\">\n<title>139833376850080</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"190.69\" cy=\"-162\" rx=\"94.48\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"190.69\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">Tensor(data=9.0) Op: *</text>\n</g>\n<!-- 139833376850080&#45;&gt;139833376862272 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139833376850080&#45;&gt;139833376862272</title>\n<path fill=\"none\" stroke=\"black\" d=\"M190.69,-143.7C190.69,-135.98 190.69,-126.71 190.69,-118.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"194.19,-118.1 190.69,-108.1 187.19,-118.1 194.19,-118.1\"/>\n</g>\n<!-- 139833376858528 -->\n<g id=\"node4\" class=\"node\">\n<title>139833376858528</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"89.69\" cy=\"-234\" rx=\"89.88\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"89.69\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">Tensor(data=3.0) Op: </text>\n</g>\n<!-- 139833376858528&#45;&gt;139833376850080 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139833376858528&#45;&gt;139833376850080</title>\n<path fill=\"none\" stroke=\"black\" d=\"M113.37,-216.59C126.82,-207.26 143.85,-195.46 158.5,-185.31\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"160.59,-188.12 166.81,-179.55 156.6,-182.37 160.59,-188.12\"/>\n</g>\n<!-- 139833376858432 -->\n<g id=\"node5\" class=\"node\">\n<title>139833376858432</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"292.69\" cy=\"-234\" rx=\"94.78\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"292.69\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">Tensor(data=3.0) Op: +</text>\n</g>\n<!-- 139833376858432&#45;&gt;139833376850080 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139833376858432&#45;&gt;139833376850080</title>\n<path fill=\"none\" stroke=\"black\" d=\"M268.52,-216.41C254.93,-207.08 237.78,-195.32 223.04,-185.2\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"224.92,-182.24 214.69,-179.47 220.96,-188.01 224.92,-182.24\"/>\n</g>\n<!-- 139830209141760 -->\n<g id=\"node6\" class=\"node\">\n<title>139830209141760</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"193.69\" cy=\"-306\" rx=\"89.88\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"193.69\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">Tensor(data=2.0) Op: </text>\n</g>\n<!-- 139830209141760&#45;&gt;139833376858432 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139830209141760&#45;&gt;139833376858432</title>\n<path fill=\"none\" stroke=\"black\" d=\"M216.9,-288.59C229.97,-279.35 246.47,-267.68 260.74,-257.59\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"263.14,-260.18 269.28,-251.55 259.1,-254.47 263.14,-260.18\"/>\n</g>\n<!-- 139830209145696 -->\n<g id=\"node7\" class=\"node\">\n<title>139830209145696</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"390.69\" cy=\"-306\" rx=\"89.88\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"390.69\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">Tensor(data=1.0) Op: </text>\n</g>\n<!-- 139830209145696&#45;&gt;139833376858432 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139830209145696&#45;&gt;139833376858432</title>\n<path fill=\"none\" stroke=\"black\" d=\"M367.47,-288.41C354.53,-279.17 338.24,-267.54 324.17,-257.49\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"325.92,-254.43 315.75,-251.47 321.85,-260.13 325.92,-254.43\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7f2d7ed447f0>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import graphviz as gv\n",
        "from IPython.display import display\n",
        "\n",
        "class Tensor:\n",
        "    \"\"\" stores a single scalar Tensor and its gradient \"\"\"\n",
        "\n",
        "    def __init__(self, data, _children=(), _op=''):\n",
        "\n",
        "        self.data = data\n",
        "\n",
        "        # internal variables used for autograd graph construction\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op  # the op that produced this node, for graphviz / debugging / etc\n",
        "        self._backward = lambda x: x\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "        out = Tensor(self.data + other.data, [self, other], '+')\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "        out = Tensor(self.data * other.data, [self, other], '*')\n",
        "        return out\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "        out = Tensor(self.data**other, [self], f'**{other}')\n",
        "        return out\n",
        "\n",
        "    def relu(self):\n",
        "        out = Tensor(0 if self.data < 0 else self.data, [self], 'ReLU')\n",
        "        return out\n",
        "\n",
        "    def __neg__(self):  # -self\n",
        "        return self * -1\n",
        "\n",
        "    def __radd__(self, other):  # other + self\n",
        "        return self + other\n",
        "\n",
        "    def __sub__(self, other):  # self - other\n",
        "        return self + (-other)\n",
        "\n",
        "    def __rsub__(self, other):  # other - self\n",
        "        return other + (-self)\n",
        "\n",
        "    def __rmul__(self, other):  # other * self\n",
        "        return self * other\n",
        "\n",
        "    def __truediv__(self, other):  # self / other\n",
        "        return self * other**-1\n",
        "\n",
        "    def __rtruediv__(self, other):  # other / self\n",
        "        return other * self**-1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Tensor(data={self.data}) Op: {self._op}\"\n",
        "\n",
        "    def build_topo(self, visited=None, topo=None):\n",
        "        if self not in visited:\n",
        "            visited.add(self)\n",
        "            for child in self._prev:\n",
        "                child.build_topo(visited=visited, topo=topo)\n",
        "            topo.append(self)\n",
        "        return topo\n",
        "\n",
        "    def backward(self):\n",
        "        # topological order all of the children in the graph\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        topo = self.build_topo(topo=topo, visited=visited)\n",
        "\n",
        "        # go one variable at a time and apply the chain rule to get its gradient\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            v._backward()\n",
        "\n",
        "    def plot_graph(self, filename='graph.png'):\n",
        "        # create a graphviz digraph\n",
        "        g = gv.Digraph(comment='Tensor Computation Graph')\n",
        "\n",
        "        # add a node for this tensor\n",
        "        g.node(str(id(self)), str(self))\n",
        "\n",
        "        # recursively add nodes for all children\n",
        "        def add_children(v):\n",
        "            for child in v._prev:\n",
        "                g.node(str(id(child)), str(child))\n",
        "                g.edge(str(id(child)), str(id(v)))\n",
        "                add_children(child)\n",
        "        add_children(self)\n",
        "        display(gv.Source(g.source))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # create some tensors\n",
        "    x = Tensor(1.0)\n",
        "    y = Tensor(2.0)\n",
        "    z = Tensor(3.0)\n",
        "\n",
        "    # compute some operations\n",
        "    a = x + y\n",
        "    b = a * z\n",
        "    c = b ** 2\n",
        "    d = c.relu()\n",
        "\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    topo = d.build_topo(topo=topo, visited=visited)\n",
        "    print(\"\\n\".join([str(item) for item in topo]))\n",
        "\n",
        "    # plot the computation graph\n",
        "    d.plot_graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-41idbLkgWwd"
      },
      "source": [
        "### Exercice de Programmation en Python : Implémentation d'un Tenseur Autograd\n",
        "\n",
        "#### Sujet\n",
        "\n",
        "Dans cet exercice, vous allez implémenter une classe `Tensor` en Python qui représente un tenseur scalaire et son gradient, avec la capacité de construire un graphe de calcul et de calculer les gradients par rétropropagation.\n",
        "\n",
        "#### Instructions\n",
        "\n",
        "1. **Définition de la Classe `Tensor`**:\n",
        "    - Implémentez la classe `Tensor` avec un constructeur pour initialiser les données, le gradient, et les variables internes pour la construction du graphe d'autograd.\n",
        "    - Implémentez les méthodes pour les opérations arithmétiques, la fonction ReLU, et la rétropropagation.\n",
        "\n",
        "2. **Création des Tenseurs et Calcul des Opérations**:\n",
        "    - Créez des instances de `Tensor` avec les valeurs 1.0, 2.0 et 3.0.\n",
        "    - Effectuez les opérations suivantes sur ces tenseurs :\n",
        "        - `a = x + y`\n",
        "        - `b = a * z`\n",
        "        - `c = b ** 2`\n",
        "        - `d = c.relu()`\n",
        "\n",
        "3. **Calcul des Gradients**:\n",
        "    - Appelez la méthode `backward` sur le tenseur final pour calculer les gradients de tous les tenseurs dans le graphe de calcul.\n",
        "\n",
        "4. **Affichage des Résultats**:\n",
        "    - Affichez les tenseurs avec leurs gradients respectifs après la rétropropagation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1jfhyFNXgWwd"
      },
      "outputs": [],
      "source": [
        "class Tensor:\n",
        "\n",
        "    \"\"\" stores a single scalar Tensor and its gradient \"\"\"\n",
        "\n",
        "    def __init__(self, data, _children=(), _op=''):\n",
        "\n",
        "        self.data = data\n",
        "        self.grad = 0.0\n",
        "\n",
        "        # internal variables used for autograd graph construction\n",
        "        self._backward = lambda: None\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "\n",
        "        out = Tensor(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += out.grad\n",
        "            other.grad += out.grad\n",
        "\n",
        "        out._backward = _backward\n",
        "\n",
        "        out._prev = set([self, other])\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "\n",
        "        out = Tensor(self.data * other.data, (self, other), '*')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "\n",
        "        out = Tensor(self.data ** other, (self, other), '**')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other * out.grad * self.data ** (other - 1)\n",
        "\n",
        "        out._backward = _backward\n",
        "\n",
        "        out._prev = set([self])\n",
        "        return out\n",
        "\n",
        "    def relu(self):\n",
        "      out = Tensor(0 if self.data < 0 else self.data, [self], 'ReLU')\n",
        "\n",
        "      def _backward():\n",
        "          if self.data > 0:\n",
        "              self.grad += out.grad\n",
        "\n",
        "      out._backward = _backward\n",
        "\n",
        "      out._prev = set([self])\n",
        "      return out\n",
        "\n",
        "    def build_topo(self, visited=None, topo=None):\n",
        "        if self not in visited:\n",
        "            visited.add(self)\n",
        "            for child in self._prev:\n",
        "                child.build_topo(visited=visited, topo=topo)\n",
        "            topo.append(self)\n",
        "        return topo\n",
        "\n",
        "    def backward(self):\n",
        "        # topological order all of the children in the graph\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        topo = self.build_topo(topo=topo, visited=visited)\n",
        "\n",
        "        # go one variable at a time and apply the chain rule to get its gradient\n",
        "        self.grad = 1.0\n",
        "        for v in reversed(topo):\n",
        "            v._backward()\n",
        "\n",
        "    def __neg__(self): # -self\n",
        "        return self * -1\n",
        "\n",
        "    def __radd__(self, other): # other + self\n",
        "        return self + other\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)\n",
        "\n",
        "    def __rsub__(self, other): # other - self\n",
        "        return other + (-self)\n",
        "\n",
        "    def __rmul__(self, other): # other * self\n",
        "        return self * other\n",
        "\n",
        "    def __truediv__(self, other): # self / other\n",
        "        return self * other**-1\n",
        "\n",
        "    def __rtruediv__(self, other): # other / self\n",
        "        return other * self**-1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Tensor(data={self.data}, grad={self.grad})\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-zxFsbKgWwd"
      },
      "source": [
        "### Test Unitaire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "uGmJclb2gWwe",
        "outputId": "46e6d402-a298-4894-ad91-c5df4f47fc12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.grad, y.grad\n",
            "2.0 12.0\n",
            "out\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    x = Tensor(1.0)\n",
        "    y = Tensor(2.0)\n",
        "    out = Tensor.relu(x * 2.0 + y ** 3.0)\n",
        "    out.backward()\n",
        "\n",
        "    print(\"x.grad, y.grad\")\n",
        "    print(x.grad, y.grad)\n",
        "\n",
        "    print(\"out\")\n",
        "    print(out.grad)\n",
        "\n",
        "    x_t = torch.tensor(1.0, requires_grad=True)\n",
        "    y_t = torch.tensor(2.0, requires_grad=True)\n",
        "    out_t = torch.relu(x_t * 2.0 + y_t ** 3.0)\n",
        "    out_t.backward()\n",
        "\n",
        "    assert np.isclose(x_t.grad, x.grad)\n",
        "    assert np.isclose(y_t.grad, y.grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45H9HeYFgWwe"
      },
      "source": [
        "### Fonctions Mathématiques pour l'Autodifférentiation en mode reverse\n",
        "\n",
        "#### Objectif\n",
        "\n",
        "L'objectif de cette partie du TP est d'implémenter des fonctions mathématiques couramment utilisées dans le contexte de l'autodifférentiation en mode reverse. Vous allez créer des fonctions qui calculent à la fois la valeur d'une fonction mathématique et sa dérivée.\n",
        "\n",
        "Vous allez implémenter plusieurs fonctions mathématiques en utilisant des nombres duals représentés par la classe Tensor. Chaque fonction doit calculer la valeur de la fonction et la dérivée, et renvoyer un objet Tensor contenant ces informations.\n",
        "\n",
        "#### Fonctions à Implémenter\n",
        "\n",
        "1. **Logarithme naturel (`log_d`)** :\n",
        "    - Calcule la valeur du logarithme naturel et sa dérivée.\n",
        "\n",
        "2. **Exponentielle (`exp_d`)** :\n",
        "    - Calcule la valeur de l'exponentielle et sa dérivée.\n",
        "\n",
        "3. **Sinus (`sin_d`)** :\n",
        "    - Calcule la valeur du sinus et sa dérivée.\n",
        "\n",
        "4. **Cosinus (`cos_d`)** :\n",
        "    - Calcule la valeur du cosinus et sa dérivée.\n",
        "\n",
        "5. **Sigmoïde (`sigmoid_d`)** :\n",
        "    - Calcule la valeur de la fonction sigmoïde et sa dérivée.\n",
        "\n",
        "6. **Tangente hyperbolique (`tanh_d`)** :\n",
        "    - Calcule la valeur de la tangente hyperbolique et sa dérivée.\n",
        "\n",
        "7. **Tangente (`tan_d`)** :\n",
        "    - Calcule la valeur de la tangente et sa dérivée.\n",
        "\n",
        "8. **Racine carrée (`sqrt_d`)** :\n",
        "    - Calcule la valeur de la racine carrée et sa dérivée.\n",
        "\n",
        "9. **Puissance (`pow_d`)** :\n",
        "    - Calcule la valeur de la fonction puissance et sa dérivée.\n",
        "\n",
        "\n",
        "#### Instructions\n",
        "\n",
        "1. Implémentez les fonctions décrites ci-dessus.\n",
        "2. Testez chaque fonction avec des exemples concrets pour vérifier leur bon fonctionnement.\n",
        "3. Documentez votre code et commentez chaque fonction pour expliquer son fonctionnement.\n",
        "4. Créez un rapport détaillé décrivant votre implémentation, les tests effectués et les résultats obtenus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cqlJrMTIgWwe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def log_d(dual_number: Tensor):\n",
        "    out = Tensor(np.log(dual_number.data), (dual_number,), 'log')\n",
        "\n",
        "    def _backward():\n",
        "        dual_number.grad += (1 / dual_number.data) * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "def exp_d(dual_number: Tensor):\n",
        "    out = Tensor(np.exp(dual_number.data), (dual_number,), 'exp')\n",
        "\n",
        "    def _backward():\n",
        "        dual_number.grad += np.exp(dual_number.data) * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "def sin_d(dual_number: Tensor):\n",
        "    # FIXME: implement sin_d\n",
        "    return out\n",
        "\n",
        "def cos_d(dual_number: Tensor):\n",
        "    # FIXME: implement cos_d\n",
        "    return out\n",
        "\n",
        "def sigmoid_d(dual_number: Tensor):\n",
        "    # FIXME: implement sigmoid_d\n",
        "    return out\n",
        "\n",
        "def tanh_d(dual_number: Tensor):\n",
        "    tanh = np.tanh(dual_number.data)\n",
        "\n",
        "    out = Tensor(tanh, (dual_number,), 'tanh')\n",
        "\n",
        "    def _backward():\n",
        "        dual_number.grad += (1 - tanh*2) * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "def tan_d(dual_number: Tensor):\n",
        "    # FIXME: implement tan_d\n",
        "    return out\n",
        "\n",
        "def sqrt_d(dual_number: Tensor):\n",
        "    # FIXME: implement sqrt_d\n",
        "    return out\n",
        "\n",
        "def pow_d(dual_number: Tensor, power: int):\n",
        "    # FIXME: implement pow_d\n",
        "    return out\n",
        "\n",
        "def softmax_d(dual_number: Tensor):\n",
        "    # FIXME: implement softmax_d\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T8TtHCBgWwe"
      },
      "source": [
        "### Main test\n",
        "\n",
        "Dans cette section, vous allez définir une collection de fonctions de test pour valider votre implémentation des fonctions mathématiques et de la rétropropagation dans la classe `Tensor`. Vous comparerez les gradients obtenus avec ceux calculés par PyTorch, une bibliothèque populaire pour le calcul automatique des gradients.\n",
        "\n",
        "#### Objectif\n",
        "\n",
        "L'objectif est de s'assurer que les gradients calculés par vos méthodes `Tensor` sont corrects en les comparant avec les gradients obtenus via PyTorch. Cette validation est essentielle pour garantir la précision de votre implémentation.\n",
        "\n",
        "#### Validation des Gradients\n",
        "\n",
        "Nous utiliserons des assertions pour vérifier que les gradients calculés par votre implémentation sont proches de ceux de PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00vrY9hpgWwe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Define the function for which we want to compute the gradient\n",
        "def f(x):\n",
        "    return x**2 + 2*x + 1\n",
        "\n",
        "# Cubic function\n",
        "def f7(x):\n",
        "    return x**3 + 3*x**2 - 2*x + 5\n",
        "\n",
        "# Quartic function\n",
        "def f8(x):\n",
        "    return x**4 + 4*x**3 - 3*x**2 + 6*x + 7\n",
        "\n",
        "# Quintic function\n",
        "def f9(x):\n",
        "    return x**5 + 5*x**4 - 4*x**3 + 8*x**2 + 9*x + 10\n",
        "\n",
        "# Exponential function\n",
        "def f10(x):\n",
        "    return exp_d(x)\n",
        "\n",
        "def f10_torch(x):\n",
        "    return torch.exp(x)\n",
        "\n",
        "# Logarithmic function\n",
        "def f11(x):\n",
        "    return log_d(x)\n",
        "\n",
        "def f11_torch(x):\n",
        "    return torch.log(x)\n",
        "\n",
        "# Sinusoidal function\n",
        "def f12(x):\n",
        "    return sin_d(x)\n",
        "\n",
        "def f12_torch(x):\n",
        "    return torch.sin(x)\n",
        "\n",
        "# Cosinusoidal function\n",
        "def f13(x):\n",
        "    return cos_d(x)\n",
        "\n",
        "def f13_torch(x):\n",
        "    return torch.cos(x)\n",
        "\n",
        "# Tangent function\n",
        "def f14(x):\n",
        "    return tan_d(x)\n",
        "\n",
        "def f14_torch(x):\n",
        "    return torch.tan(x)\n",
        "\n",
        "# Hyperbolic tangent function\n",
        "def f17(x):\n",
        "    return tanh_d(x)\n",
        "\n",
        "def f17_torch(x):\n",
        "    return torch.tanh(x)\n",
        "\n",
        "# Inverse function\n",
        "def f18(x):\n",
        "    return 1/x\n",
        "\n",
        "# Square root function\n",
        "def f19(x):\n",
        "    return sqrt_d(x)\n",
        "\n",
        "def f19_torch(x):\n",
        "    return torch.sqrt(x)\n",
        "\n",
        "# Natural logarithm function\n",
        "def f22(x):\n",
        "    return log_d(x)\n",
        "\n",
        "def f22_torch(x):\n",
        "    return torch.log(x)\n",
        "\n",
        "# Define the function for which we want to compute the gradient\n",
        "def f(x):\n",
        "    return x**2 + 2*x + 1\n",
        "\n",
        "# Cubic function with sin\n",
        "def f27(x):\n",
        "    return x**3 + 3*x**2 - 2*x + sin_d(x) + 5\n",
        "\n",
        "def f27_torch(x):\n",
        "    return x**3 + 3*x**2 - 2*x + torch.sin(x) + 5\n",
        "\n",
        "# Quartic function with cos\n",
        "def f28(x):\n",
        "    return x**4 + 4*x**3 - 3*x**2 + 6*x + cos_d(x) + 7\n",
        "\n",
        "def f28_torch(x):\n",
        "    return x**4 + 4*x**3 - 3*x**2 + 6*x + torch.cos(x) + 7\n",
        "\n",
        "# Quintic function with sin and cos\n",
        "def f29(x):\n",
        "    return x**5 + 5*x**4 - 4*x**3 + 8*x**2 + 9*x + sin_d(x) + cos_d(x) + 10\n",
        "\n",
        "def f29_torch(x):\n",
        "    return x**5 + 5*x**4 - 4*x**3 + 8*x**2 + 9*x + torch.sin(x) + torch.cos(x) + 10\n",
        "\n",
        "# Function with sin^2 and cos^2\n",
        "def f30(x):\n",
        "    return sin_d(x)**2 + cos_d(x)**2 + x**2\n",
        "\n",
        "def f30_torch(x):\n",
        "    return torch.sin(x)**2 + torch.cos(x)**2 + x**2\n",
        "\n",
        "# Function with sin^3 and cos^3\n",
        "def f31(x):\n",
        "    return sin_d(x)**3 + cos_d(x)**3 + x**3\n",
        "\n",
        "def f31_torch(x):\n",
        "    return torch.sin(x)**3 + torch.cos(x)**3 + x**3\n",
        "\n",
        "### define a list of functions\n",
        "functions = [f, f7, f8, f9, f10, f11, f12, f13, f14, f17, f18, f19, f22, f27, f28, f29, f30, f31]\n",
        "functions_torch = [f, f7, f8, f9, f10_torch, f11_torch, f12_torch, f13_torch, f14_torch, f17_torch, f18, f19_torch, f22_torch, f27_torch, f28_torch, f29_torch, f30_torch, f31_torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtDnhdprgWwf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Liste des fonctions personnalisées\n",
        "functions = [log_d, exp_d, sin_d, cos_d, sigmoid_d, tanh_d, tan_d, sqrt_d, pow_d, softmax_d]\n",
        "\n",
        "# Liste des fonctions équivalentes dans PyTorch\n",
        "functions_torch = [torch.log, torch.exp, torch.sin, torch.cos, torch.sigmoid, torch.tanh, torch.tan, torch.sqrt, lambda x, p: x.pow(p), torch.softmax]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for f, f_torch in zip(functions, functions_torch):\n",
        "        # Create a tensor with a single value\n",
        "        x = Tensor(0.1)\n",
        "        x_torch = torch.tensor(x.data, requires_grad=True)\n",
        "\n",
        "        # Compute the function value\n",
        "        y = f(x)\n",
        "\n",
        "        # For pow_d, we need to pass an additional argument\n",
        "        if f is pow_d:\n",
        "            y_torch = f_torch(x_torch, 2)  # Example with power of 2\n",
        "        else:\n",
        "            y_torch = f_torch(x_torch)\n",
        "\n",
        "        # Compute the gradient\n",
        "        y.backward()\n",
        "        y_torch.backward()\n",
        "\n",
        "        # Compare the gradients\n",
        "        print(f\"Custom Tensor grad: {x.grad}\")\n",
        "        print(f\"PyTorch Tensor grad: {x_torch.grad.item()}\")\n",
        "        print()\n",
        "\n",
        "        assert np.isclose(x.grad, x_torch.grad.item(), atol=1e-6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omT9gkQDgWwf"
      },
      "source": [
        "### Réseau de Neurones avec une Couche Cachée\n",
        "\n",
        "#### Objectif\n",
        "\n",
        "L'objectif de cette section est de définir et de tester un réseau de neurones multicouche (MLP) avec une seule couche cachée en utilisant la classe `Tensor` que vous avez développée. Vous comparerez les résultats de la rétropropagation avec ceux obtenus par PyTorch pour valider l'exactitude de votre implémentation.\n",
        "\n",
        "#### Instructions\n",
        "\n",
        "1. **Définition du Réseau de Neurones** :\n",
        "    - Définissez un réseau de neurones avec une seule couche cachée en utilisant votre classe `Tensor`. Utilisez la fonction d'activation hyperbolique tangente (`tanh`).\n",
        "\n",
        "2. **Implémentation avec PyTorch** :\n",
        "    - Définissez le même réseau de neurones en utilisant PyTorch pour comparer les résultats.\n",
        "\n",
        "3. **Calcul des Gradients** :\n",
        "    - Calculez les gradients en utilisant la méthode `backward()` de votre classe `Tensor`.\n",
        "    - Calculez les gradients en utilisant PyTorch.\n",
        "\n",
        "4. **Validation des Résultats** :\n",
        "    - Comparez les gradients obtenus par votre implémentation avec ceux obtenus par PyTorch pour vous assurer qu'ils sont similaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jLczLY8ygWwf",
        "outputId": "82fa546a-1e7e-4aa2-e3c9-eba5a86d7d51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient computed by backward Mode: 0.002398676843944102\n",
            "Gradient computed by PyTorch: 0.023361356928944588\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-a807ee527413>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gradient computed by PyTorch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# define a neural network, MLP with 1 hidden layer\n",
        "def func_nn(x):\n",
        "    # FIXME: implement the forward pass\n",
        "    W1 = Tensor(0.1)\n",
        "    b1 = Tensor(0.2)\n",
        "    W2 = Tensor(0.3)\n",
        "    b2 = Tensor(0.4)\n",
        "\n",
        "    h1 = tanh_d(W1*x + b1)\n",
        "    y = tanh_d(W2*h1 + b2)\n",
        "    return y\n",
        "\n",
        "## with torch\n",
        "def func_nn_torch(x):\n",
        "    W1 = torch.tensor(0.1, requires_grad=True)\n",
        "    b1 = torch.tensor(0.2, requires_grad=True)\n",
        "    W2 = torch.tensor(0.3, requires_grad=True)\n",
        "    b2 = torch.tensor(0.4, requires_grad=True)\n",
        "\n",
        "    h1 = torch.tanh(W1*x + b1)\n",
        "    y = torch.tanh(W2*h1 + b2)\n",
        "    return y\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Create a tensor with a single value\n",
        "    x = Tensor(0.1)\n",
        "\n",
        "    # Compute the gradient using Wolf Forward Mode\n",
        "    y = func_nn(x)\n",
        "    y.backward()\n",
        "    print(\"Gradient computed by backward Mode:\", x.grad)\n",
        "\n",
        "    # Convert the tensor to a PyTorch tensor\n",
        "    x_torch = torch.tensor(x.data)\n",
        "    x_torch.requires_grad_(True)\n",
        "\n",
        "    y_torch = func_nn_torch(x_torch)\n",
        "    y_torch.backward()\n",
        "    print(\"Gradient computed by PyTorch:\", x_torch.grad.item())\n",
        "\n",
        "    assert np.isclose(x.grad, x_torch.grad.item(), atol=1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLEgX96KgWwf"
      },
      "source": [
        "### Entraînement d'un Réseau de Neurones avec une Couche Cachée\n",
        "\n",
        "#### Objectif\n",
        "\n",
        "L'objectif de cette section est de définir, entraîner et évaluer un réseau de neurones multicouche (MLP) avec une seule couche cachée en utilisant la classe `Tensor`. Vous allez également comparer les résultats de l'entraînement avec ceux obtenus par PyTorch pour valider l'exactitude de votre implémentation.\n",
        "\n",
        "#### Instructions\n",
        "\n",
        "1. **Définition du Réseau de Neurones** :\n",
        "    - Définissez un réseau de neurones avec une seule couche cachée en utilisant votre classe `Tensor`.\n",
        "\n",
        "2. **Fonction de Perte** :\n",
        "    - Implémentez la fonction de perte d'erreur quadratique moyenne (MSE).\n",
        "\n",
        "3. **Entraînement du Réseau** :\n",
        "    - Entraînez le réseau de neurones en utilisant une simple boucle d'entraînement. Mettez à jour les poids et les biais en utilisant la rétropropagation et une descente de gradient.\n",
        "\n",
        "4. **Comparaison avec PyTorch** :\n",
        "    - Implémentez le même réseau de neurones et l'entraînement en utilisant PyTorch pour comparer les résultats.\n",
        "\n",
        "5. **Validation des Résultats** :\n",
        "    - Comparez les pertes et les gradients obtenus par votre implémentation avec ceux obtenus par PyTorch pour vous assurer qu'ils sont similaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W49zfAxWgWwf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a neural network, MLP with 1 hidden layer\n",
        "def func_nn(x, W1, b1, W2, b2):\n",
        "    # Forward pass\n",
        "    # FIXME: implement the forward pass\n",
        "    return y_hat\n",
        "\n",
        "def mse(y, y_hat):\n",
        "    # Mean squared error\n",
        "    diff = y - y_hat\n",
        "    loss = (diff * diff).sum() / y.data.size\n",
        "    return loss\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ## Generate data\n",
        "    np.random.seed(0)\n",
        "    x = np.linspace(-2, 2, 100)\n",
        "    y = 2 * x + 1 + np.random.randn(100) * 0.1\n",
        "\n",
        "    ## Parameters\n",
        "    W1 = Tensor(0.1)\n",
        "    b1 = Tensor(0.2)\n",
        "    W2 = Tensor(0.3)\n",
        "    b2 = Tensor(0.4)\n",
        "\n",
        "    lr = 0.01\n",
        "    nb_epoch = 100\n",
        "    for _ in range(nb_epoch):\n",
        "        lst_loss = []\n",
        "        for i in range(len(x)):\n",
        "            x_i = Tensor(x[i])\n",
        "            y_i = Tensor(y[i])\n",
        "\n",
        "            y_hat = func_nn(x_i, W1, b1, W2, b2)\n",
        "            loss = mse(y_i, y_hat)\n",
        "            loss.backward()\n",
        "\n",
        "            lst_loss.append(loss.data)\n",
        "\n",
        "            # Update parameters with the gradient\n",
        "            W1.data -= lr * W1.grad\n",
        "            b1.data -= lr * b1.grad\n",
        "            W2.data -= lr * W2.grad\n",
        "            b2.data -= lr * b2.grad\n",
        "\n",
        "            # Reset gradients\n",
        "            W1.grad = 0\n",
        "            b1.grad = 0\n",
        "            W2.grad = 0\n",
        "            b2.grad = 0\n",
        "\n",
        "        print(np.mean(lst_loss))\n",
        "\n",
        "        # Learning rate decay\n",
        "        lr *= 0.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkZLDpfLgWwf"
      },
      "source": [
        "### Entraînement d'un Réseau Récurrent (RNN) pour la Prédiction de Séries Temporelles\n",
        "\n",
        "#### Objectif\n",
        "\n",
        "L'objectif de cette section est de définir, entraîner et évaluer un réseau de neurones récurrent (RNN) simple pour la prédiction de séries temporelles en utilisant la classe `Tensor`. Vous allez comparer les résultats de l'entraînement avec ceux obtenus par PyTorch pour valider l'exactitude de votre implémentation.\n",
        "\n",
        "#### Instructions\n",
        "\n",
        "1. **Définition du Réseau Récurrent** :\n",
        "    - Définissez un réseau de neurones récurrent simple en utilisant votre classe `Tensor`.\n",
        "\n",
        "2. **Fonction de Perte** :\n",
        "    - Implémentez la fonction de perte d'erreur quadratique moyenne (MSE).\n",
        "\n",
        "3. **Entraînement du Réseau** :\n",
        "    - Entraînez le réseau de neurones récurrent en utilisant une simple boucle d'entraînement. Mettez à jour les poids et les biais en utilisant la rétropropagation et une descente de gradient.\n",
        "    - Réinitialisez les gradients après chaque séquence pour éviter l'accumulation des gradients.\n",
        "\n",
        "4. **Comparaison avec PyTorch** :\n",
        "    - Implémentez le même réseau de neurones et l'entraînement en utilisant PyTorch pour comparer les résultats.\n",
        "\n",
        "5. **Validation des Résultats** :\n",
        "    - Comparez les pertes et les gradients obtenus par votre implémentation avec ceux obtenus par PyTorch pour vous assurer qu'ils sont similaires.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRedmkllgWwf"
      },
      "outputs": [],
      "source": [
        "def mse(y, y_hat):\n",
        "    # Implement the mean squared error\n",
        "    diff = y - y_hat\n",
        "    loss = (diff * diff) / y.data.size\n",
        "    return loss\n",
        "\n",
        "def func_rnn(x, h, Wx, Wh, b):\n",
        "    # Implement the forward pass for the RNN\n",
        "    # FIME: implement the forward pass\n",
        "    return new_h\n",
        "\n",
        "def func_nn(h, Wy, by):\n",
        "    # Implement the forward pass for the output layer\n",
        "    # FIXME: implement the forward pass\n",
        "    return y_hat\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(0)\n",
        "\n",
        "    # Generate time series prediction dataset\n",
        "    seq_len = 10\n",
        "    num_samples = 100\n",
        "    x = np.linspace(0, 4 * np.pi, num_samples)\n",
        "    y = np.sin(x) + 0.1 * np.random.randn(num_samples)\n",
        "\n",
        "    # Parameters\n",
        "    Wx = Tensor(0.01)\n",
        "    Wh = Tensor(0.01)\n",
        "    b =  Tensor(0.01)\n",
        "    Wy = Tensor(0.01)\n",
        "    by = Tensor(0.01)\n",
        "\n",
        "    lr = 0.01\n",
        "    nb_epoch = 100\n",
        "    hidden_size = 1\n",
        "\n",
        "    for epoch in range(nb_epoch):\n",
        "        lst_loss = []\n",
        "        for i in range(num_samples - seq_len):\n",
        "            h = Tensor(0.0)  # Initial hidden state\n",
        "            loss_seq = 0\n",
        "            for t in range(seq_len):\n",
        "                x_i = Tensor(x[i + t])\n",
        "                y_i = Tensor(y[i + t])\n",
        "\n",
        "                h = func_rnn(x_i, h, Wx, Wh, b)\n",
        "                y_hat = func_nn(h, Wy, by)\n",
        "                loss = mse(y_i, y_hat)\n",
        "                loss_seq += loss.data\n",
        "                loss.backward()\n",
        "\n",
        "                # Update with the gradient the recurrent layer\n",
        "                Wx.data -= lr * Wx.grad\n",
        "                Wh.data -= lr * Wh.grad\n",
        "                b.data -= lr * b.grad\n",
        "\n",
        "                # Compute the gradient for output layer weights\n",
        "                Wy.data -= lr * Wy.grad\n",
        "                by.data -= lr * by.grad\n",
        "\n",
        "                # Reset gradients\n",
        "                Wx.grad = 0\n",
        "                Wh.grad = 0\n",
        "                b.grad = 0\n",
        "                Wy.grad = 0\n",
        "                by.grad = 0\n",
        "\n",
        "            lst_loss.append(loss_seq / seq_len)\n",
        "            print(f\"Epoch {epoch}, Loss: {np.mean(lst_loss)}\")\n",
        "\n",
        "        # Learning rate decay\n",
        "        lr *= 0.001\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}