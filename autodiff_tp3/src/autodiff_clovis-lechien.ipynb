{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ***TP3 - Clovis Lechien***\n",
    "\n",
    "1. [Utils](#Utils)\n",
    "2. [SGD](#SGD)\n",
    "3. [RMSProp](#RMSProp)\n",
    "4. [Adagrad](#Adagrad)\n",
    "5. [Adam](#Adam)\n",
    "6. [AdamW](#AdamW)\n",
    "7. [Evaluation des Optimiseurs](#evaluation-des-optimiseurs) FIXME\n",
    "8. [Réseau de Neurones](#réseau-de-neurones) FIXME\n",
    "9. [Scheduler de Taux d'Apprentissage](#schedulers) FIXME"
   ],
   "id": "e418516f7da7ad36"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-20T17:10:20.046732Z",
     "start_time": "2025-01-20T17:10:20.044499Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:10:20.062833Z",
     "start_time": "2025-01-20T17:10:20.060329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Génération du jeu de données linéaire\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "x_linear = np.linspace(-10, 10, n_samples)\n",
    "y_linear = 3 * x_linear + 5 + np.random.normal(0, 2, n_samples)\n",
    "\n",
    " # Génération du jeu de données non linéaire\n",
    "y_nonlinear = 0.5 * x_linear **2 - 4 * x_linear + np.random.normal(0 ,5 ,n_samples)"
   ],
   "id": "5b054a38d6955342",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Custom Tensor class***",
   "id": "add1065b5a810b45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:51:37.100540Z",
     "start_time": "2025-01-20T17:51:37.094866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tensor:\n",
    "\n",
    "    \"\"\" stores a single scalar Tensor and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "\n",
    "        out = Tensor(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        out._prev = set([self, other])\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "\n",
    "        out = Tensor(self.data * other.data, [self, other], '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "\n",
    "        out = Tensor(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        # FIXME: implement relu\n",
    "        pass\n",
    "\n",
    "    def build_topo(self, visited=None, topo=None):\n",
    "        if self not in visited:\n",
    "            visited.add(self)\n",
    "            for child in self._prev:\n",
    "                child.build_topo(visited=visited, topo=topo)\n",
    "            topo.append(self)\n",
    "        return topo\n",
    "\n",
    "    def backward(self):\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        topo = self.build_topo(topo=topo, visited=visited)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad})\""
   ],
   "id": "7002f36d0bcccbac",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ***Custom operations***",
   "id": "29d81e5677e344c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:51:39.152507Z",
     "start_time": "2025-01-20T17:51:39.145464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def log_d(dual_number: Tensor):\n",
    "    out = Tensor(np.log(dual_number.data), (dual_number,), 'log')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += (1 / dual_number.data) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def exp_d(dual_number: Tensor):\n",
    "    out = Tensor(np.exp(dual_number.data), (dual_number,), 'exp')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += np.exp(dual_number.data) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def sin_d(dual_number: Tensor):\n",
    "    out = Tensor(np.sin(dual_number.data), (dual_number,), 'sin')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += np.cos(dual_number.data) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def cos_d(dual_number: Tensor):\n",
    "    out = Tensor(np.cos(dual_number.data), (dual_number,), 'cos')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += -np.sin(dual_number.data) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def sigmoid_d(dual_number: Tensor):\n",
    "    sig = 1 / (1 + np.exp(-dual_number.data))\n",
    "    out = Tensor(sig, (dual_number,), 'sigmoid')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += sig * (1 - sig) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def tanh_d(dual_number: Tensor):\n",
    "    tanh = np.tanh(dual_number.data)\n",
    "    out = Tensor(tanh, (dual_number,), 'tanh')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += (1 - tanh**2) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def tan_d(dual_number: Tensor):\n",
    "    out = Tensor(np.tan(dual_number.data), (dual_number,), 'tan')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += (1 / np.cos(dual_number.data)**2) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def sqrt_d(dual_number: Tensor):\n",
    "    out = Tensor(np.sqrt(dual_number.data), (dual_number,), 'sqrt')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += (0.5 / np.sqrt(dual_number.data)) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "\n",
    "def pow_d(dual_number: Tensor, power: int):\n",
    "    out = Tensor(dual_number.data**power, (dual_number,), f'pow{power}')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += (power * dual_number.data**(power-1)) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def softmax_d(dual_number: Tensor):\n",
    "    e = np.exp(dual_number.data - np.max(dual_number.data))\n",
    "    out = Tensor(e / np.sum(e), (dual_number,), 'softmax')\n",
    "\n",
    "    def _backward():\n",
    "        for i in range(len(dual_number.data)):\n",
    "            for j in range(len(dual_number.data)):\n",
    "                if i == j:\n",
    "                    dual_number.grad[i] += out.data[i] * (1 - out.data[i]) * out.grad[i]\n",
    "                else:\n",
    "                    dual_number.grad[i] += -out.data[i] * out.data[j] * out.grad[j]\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out"
   ],
   "id": "52846f4c7ab39c0",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Utils<a name=\"Utils\"></a>***",
   "id": "6ec84e42849b4a17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:17:15.142482Z",
     "start_time": "2025-01-20T18:17:15.138675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimizer_testing_loop(parameters : dict[str,]):\n",
    "    model = parameters['model']\n",
    "\n",
    "    criterion = parameters['criterion']\n",
    "    optimizer = parameters['optimizer']\n",
    "\n",
    "    x_tensor = parameters['x_tensor']\n",
    "    y_tensor = parameters['y_tensor']\n",
    "\n",
    "    epochs = parameters['epochs']\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x_tensor)\n",
    "        loss = criterion(predictions, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.data}\")\n",
    "\n",
    "\n",
    "def check_diffs(a : list[float], b : list[float], tol : float = 1e-4):\n",
    "    res = np.allclose(a, b, atol=tol)\n",
    "    if res:\n",
    "        print(f\"All elements between\\n{a}\\and\\n{b}\\nare close within a tolerance of {tol}\")\n",
    "    else:\n",
    "        print(\"Test failed\")"
   ],
   "id": "14a6a420243220be",
   "outputs": [],
   "execution_count": 154
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***SGD<a name=\"SGD\"></a>***",
   "id": "8dfdde3bc692cd33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de SGD**",
   "id": "5eb8e9d42bd537a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:10:20.256051Z",
     "start_time": "2025-01-20T17:10:20.252847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SGD_torch(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        hyperparams = {'lr': learning_rate}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                theta_t -= lr * theta_t.grad\n",
    "\n",
    "\n",
    "class SGD_custom:\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.data -= self.learning_rate * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ],
   "id": "9b1604245ddcf6ee",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de SGD**",
   "id": "b9e407ea443cb5d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:10:20.329100Z",
     "start_time": "2025-01-20T17:10:20.302322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': SGD_torch(model.parameters(), learning_rate=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "6f319e016a6d00cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 18.837690353393555\n",
      "Epoch 20, Loss: 13.9168119430542\n",
      "Epoch 30, Loss: 10.63158893585205\n",
      "Epoch 40, Loss: 8.438353538513184\n",
      "Epoch 50, Loss: 6.974130153656006\n",
      "Epoch 60, Loss: 5.996603965759277\n",
      "Epoch 70, Loss: 5.343999862670898\n",
      "Epoch 80, Loss: 4.908313751220703\n",
      "Epoch 90, Loss: 4.617447376251221\n",
      "Epoch 100, Loss: 4.4232635498046875\n",
      "weight: tensor([[2.9703]])\n",
      "bias: tensor([4.5076])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:10:20.385065Z",
     "start_time": "2025-01-20T17:10:20.357146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': SGD_torch(model.parameters(), learning_rate=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "fe10005fb82a84f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 464.37384033203125\n",
      "Epoch 20, Loss: 402.1067199707031\n",
      "Epoch 30, Loss: 360.53668212890625\n",
      "Epoch 40, Loss: 332.7842102050781\n",
      "Epoch 50, Loss: 314.2564697265625\n",
      "Epoch 60, Loss: 301.8872375488281\n",
      "Epoch 70, Loss: 293.62933349609375\n",
      "Epoch 80, Loss: 288.1163635253906\n",
      "Epoch 90, Loss: 284.43585205078125\n",
      "Epoch 100, Loss: 281.97869873046875\n",
      "weight: tensor([[-4.1445]])\n",
      "bias: tensor([15.2363])\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***RMSProp<a name=\"RMSProp\"></a>***",
   "id": "152e498551a6e055"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de RMSProp**",
   "id": "777cec9838bb901a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:54:31.270719Z",
     "start_time": "2025-01-20T17:54:31.264581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RMSProp_torch(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01, decay=0.9):\n",
    "        hyperparams = {'lr': learning_rate, 'decay': decay}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            decay = group['decay']\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'square_avg' not in state:\n",
    "                    state['square_avg'] = torch.zeros_like(theta_t)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                square_avg = decay * square_avg + (1 - decay) * (theta_t.grad ** 2)\n",
    "                state['square_avg'] = square_avg\n",
    "\n",
    "                theta_t -= lr * theta_t.grad / square_avg.sqrt()\n",
    "\n",
    "\n",
    "class RMSProp_custom:\n",
    "    def __init__(self, params, learning_rate=0.01, decay=0.9):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.state = {param: {'square_avg': Tensor(0.0)} for param in params}\n",
    "\n",
    "    def step(self):\n",
    "        for theta_t in self.params:\n",
    "            if theta_t.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[theta_t]\n",
    "\n",
    "            square_avg = state['square_avg']\n",
    "            square_avg.data = self.decay * square_avg.data + (1 - self.decay) * (theta_t.grad ** 2)\n",
    "            state['square_avg'] = square_avg\n",
    "\n",
    "            theta_t.data -= self.learning_rate * theta_t.grad / np.sqrt(square_avg.data)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ],
   "id": "aa87be320c6bac1c",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de RMSProp**",
   "id": "1eeb4b122e967c04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:10:20.481600Z",
     "start_time": "2025-01-20T17:10:20.453413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': RMSProp_torch(model.parameters(), learning_rate=0.01, decay=0.8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "94571c79790e8b25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 416.08563232421875\n",
      "Epoch 20, Loss: 392.09796142578125\n",
      "Epoch 30, Loss: 369.3656921386719\n",
      "Epoch 40, Loss: 347.38226318359375\n",
      "Epoch 50, Loss: 326.0961608886719\n",
      "Epoch 60, Loss: 305.5015869140625\n",
      "Epoch 70, Loss: 285.5976257324219\n",
      "Epoch 80, Loss: 266.3839416503906\n",
      "Epoch 90, Loss: 247.86012268066406\n",
      "Epoch 100, Loss: 230.02578735351562\n",
      "weight: tensor([[0.4664]])\n",
      "bias: tensor([1.7960])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:10:20.529847Z",
     "start_time": "2025-01-20T17:10:20.500226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': RMSProp_torch(model.parameters(), learning_rate=0.01, decay=0.8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "91785a0139d74176",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 924.3912353515625\n",
      "Epoch 20, Loss: 900.039306640625\n",
      "Epoch 30, Loss: 876.9500122070312\n",
      "Epoch 40, Loss: 854.6093139648438\n",
      "Epoch 50, Loss: 832.9650268554688\n",
      "Epoch 60, Loss: 812.0113525390625\n",
      "Epoch 70, Loss: 791.7471923828125\n",
      "Epoch 80, Loss: 772.171875\n",
      "Epoch 90, Loss: 753.2850952148438\n",
      "Epoch 100, Loss: 735.08642578125\n",
      "weight: tensor([[-1.9983]])\n",
      "bias: tensor([0.1038])\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Adagrad<a name=\"Adagrad\"></a>***",
   "id": "f010754e9cd887d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de Adagrad**",
   "id": "4242814cd90f5d60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:55:03.572520Z",
     "start_time": "2025-01-20T17:55:03.568609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adagrad_torch(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        hyperparams = {'lr': learning_rate}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'sum_squared_grads' not in state:\n",
    "                    state['sum_squared_grads'] = torch.zeros_like(theta_t)\n",
    "\n",
    "                sum_squared_grads = state['sum_squared_grads']\n",
    "                sum_squared_grads += theta_t.grad ** 2\n",
    "                state['sum_squared_grads'] = sum_squared_grads\n",
    "\n",
    "                adjusted_lr = lr / sum_squared_grads.sqrt()\n",
    "\n",
    "                theta_t -= adjusted_lr * theta_t.grad\n",
    "\n",
    "\n",
    "class Adagrad_custom:\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.state = {param: {'sum_squared_grads': Tensor(0.0)} for param in params}\n",
    "\n",
    "    def step(self):\n",
    "        for theta_t in self.params:\n",
    "            if theta_t.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[theta_t]\n",
    "\n",
    "            sum_squared_grads = state['sum_squared_grads']\n",
    "            sum_squared_grads.data += theta_t.grad ** 2\n",
    "            state['sum_squared_grads'] = sum_squared_grads\n",
    "\n",
    "            adjusted_lr = self.learning_rate / np.sqrt(sum_squared_grads.data)\n",
    "\n",
    "            theta_t.data -= adjusted_lr * theta_t.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ],
   "id": "1368be27bac6048a",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de Adagrad**",
   "id": "8a5105ec47c4d5e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:10:20.631068Z",
     "start_time": "2025-01-20T17:10:20.601176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adagrad_torch(model.parameters(), learning_rate=0.5),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "c43614bd41ea6d64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 60.79481887817383\n",
      "Epoch 20, Loss: 17.981473922729492\n",
      "Epoch 30, Loss: 8.222616195678711\n",
      "Epoch 40, Loss: 5.550739765167236\n",
      "Epoch 50, Loss: 4.680315017700195\n",
      "Epoch 60, Loss: 4.341108322143555\n",
      "Epoch 70, Loss: 4.1887288093566895\n",
      "Epoch 80, Loss: 4.114058494567871\n",
      "Epoch 90, Loss: 4.075798034667969\n",
      "Epoch 100, Loss: 4.055779933929443\n",
      "weight: tensor([[2.9690]])\n",
      "bias: tensor([4.9744])\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:10:20.682215Z",
     "start_time": "2025-01-20T17:10:20.653190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adagrad_torch(model.parameters(), learning_rate=0.5),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "ba6af84cb07e158b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 728.67822265625\n",
      "Epoch 20, Loss: 556.0093994140625\n",
      "Epoch 30, Loss: 476.72149658203125\n",
      "Epoch 40, Loss: 433.33746337890625\n",
      "Epoch 50, Loss: 406.7573547363281\n",
      "Epoch 60, Loss: 388.84918212890625\n",
      "Epoch 70, Loss: 375.7533874511719\n",
      "Epoch 80, Loss: 365.5209045410156\n",
      "Epoch 90, Loss: 357.1200866699219\n",
      "Epoch 100, Loss: 349.97967529296875\n",
      "weight: tensor([[-4.0441]])\n",
      "bias: tensor([8.9320])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Adam<a name=\"Adam\"></a>***",
   "id": "b13a43547f37f00b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de Adam**",
   "id": "b6d5b0e1de91cdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:59:41.731918Z",
     "start_time": "2025-01-20T17:59:41.726199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adam_torch(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        hyperparams = {'lr': learning_rate, 'beta1': beta1, 'beta2': beta2, 'epsilon': epsilon}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'm' not in state: # Moment d'ordre 1\n",
    "                    state['m'] = torch.zeros_like(theta_t)\n",
    "                if 'v' not in state: # Moment d'ordre 2\n",
    "                    state['v'] = torch.zeros_like(theta_t)\n",
    "                if 't' not in state: # Temps\n",
    "                    state['t'] = 0\n",
    "\n",
    "                # Premier Moment\n",
    "                m = state['m']\n",
    "                m_t = beta1 * m + (1 - beta1) * theta_t.grad\n",
    "                state['m'] = m_t\n",
    "\n",
    "                # Second Moment\n",
    "                v = state['v']\n",
    "                v_t = beta2 * v + (1 - beta2) * theta_t.grad ** 2\n",
    "                state['v'] = v_t\n",
    "\n",
    "                # Temps\n",
    "                t = state['t'] + 1\n",
    "                state['t'] = t\n",
    "\n",
    "                # Correction des biais\n",
    "                m_hat = m_t / (1 - beta1 ** t)\n",
    "                v_hat = v_t / (1 - beta2 ** t)\n",
    "\n",
    "                theta_t -= lr * m_hat / (v_hat.sqrt() + epsilon)\n",
    "\n",
    "\n",
    "class Adam_custom:\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.state = {param: {'m': Tensor(0.0), 'v': Tensor(0.0), 't': 0} for param in params}\n",
    "\n",
    "    def step(self):\n",
    "        for theta_t in self.params:\n",
    "            if theta_t.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[theta_t]\n",
    "\n",
    "            # Premier Moment\n",
    "            m = state['m']\n",
    "            m_t = self.beta1 * m.data + (1 - self.beta1) * theta_t.grad\n",
    "            state['m'].data = m_t\n",
    "\n",
    "            # Second Moment\n",
    "            v = state['v']\n",
    "            v_t = self.beta2 * v.data + (1 - self.beta2) * theta_t.grad ** 2\n",
    "            state['v'].data = v_t\n",
    "\n",
    "            # Temps\n",
    "            t = state['t'] + 1\n",
    "            state['t'] = t\n",
    "\n",
    "            # Correction des biais\n",
    "            m_hat = m_t / (1 - self.beta1 ** t)\n",
    "            v_hat = v_t / (1 - self.beta2 ** t)\n",
    "\n",
    "            theta_t.data -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ],
   "id": "b6573846cf44e8b3",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de Adam**",
   "id": "c210eec6a98cf32e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:10:20.791438Z",
     "start_time": "2025-01-20T17:10:20.757806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adam_torch(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "521702270449bd96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 306.93963623046875\n",
      "Epoch 20, Loss: 145.0484619140625\n",
      "Epoch 30, Loss: 54.37504577636719\n",
      "Epoch 40, Loss: 15.743021011352539\n",
      "Epoch 50, Loss: 5.091212272644043\n",
      "Epoch 60, Loss: 4.114846706390381\n",
      "Epoch 70, Loss: 4.423768997192383\n",
      "Epoch 80, Loss: 4.31403923034668\n",
      "Epoch 90, Loss: 4.114806652069092\n",
      "Epoch 100, Loss: 4.03961181640625\n",
      "weight: tensor([[2.9796]])\n",
      "bias: tensor([5.1556])\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:10:20.844615Z",
     "start_time": "2025-01-20T17:10:20.808986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adam_torch(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "ff677ad6cf158b95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1138.9935302734375\n",
      "Epoch 20, Loss: 869.0379638671875\n",
      "Epoch 30, Loss: 676.70654296875\n",
      "Epoch 40, Loss: 551.13232421875\n",
      "Epoch 50, Loss: 475.0643310546875\n",
      "Epoch 60, Loss: 430.46612548828125\n",
      "Epoch 70, Loss: 402.9716491699219\n",
      "Epoch 80, Loss: 383.5935974121094\n",
      "Epoch 90, Loss: 367.9411315917969\n",
      "Epoch 100, Loss: 354.38629150390625\n",
      "weight: tensor([[-4.1813]])\n",
      "bias: tensor([8.6936])\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***AdamW<a name=\"AdamW\"></a>***",
   "id": "40fbceea56e83cc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:02:29.493602Z",
     "start_time": "2025-01-20T18:02:29.487403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AdamW_torch(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):\n",
    "        hyperparams = {'lr': learning_rate, 'beta1': beta1, 'beta2': beta2, 'epsilon': epsilon, 'weight_decay': weight_decay}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'm' not in state: # Moment d'ordre 1\n",
    "                    state['m'] = torch.zeros_like(theta_t)\n",
    "                if 'v' not in state: # Moment d'ordre 2\n",
    "                    state['v'] = torch.zeros_like(theta_t)\n",
    "                if 't' not in state: # Temps\n",
    "                    state['t'] = 0\n",
    "\n",
    "                # Premier Moment\n",
    "                m = state['m']\n",
    "                m_t = beta1 * m + (1 - beta1) * theta_t.grad\n",
    "                state['m'] = m_t\n",
    "\n",
    "                # Second Moment\n",
    "                v = state['v']\n",
    "                v_t = beta2 * v + (1 - beta2) * theta_t.grad ** 2\n",
    "                state['v'] = v_t\n",
    "\n",
    "                # Temps\n",
    "                t = state['t'] + 1\n",
    "                state['t'] = t\n",
    "\n",
    "                # Correction des biais\n",
    "                m_hat = m_t / (1 - beta1 ** t)\n",
    "                v_hat = v_t / (1 - beta2 ** t)\n",
    "\n",
    "                theta_t -= lr * m_hat / (np.sqrt(v_hat) + epsilon) - lr * weight_decay * theta_t\n",
    "\n",
    "\n",
    "class AdamW_custom:\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        self.state = {param: {'m': Tensor(0.0), 'v': Tensor(0.0), 't': 0} for param in params}\n",
    "\n",
    "    def step(self):\n",
    "        for theta_t in self.params:\n",
    "            if theta_t.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[theta_t]\n",
    "\n",
    "            # Premier Moment\n",
    "            m = state['m']\n",
    "            m_t = self.beta1 * m.data + (1 - self.beta1) * theta_t.grad\n",
    "            state['m'].data = m_t\n",
    "\n",
    "            # Second Moment\n",
    "            v = state['v']\n",
    "            v_t = self.beta2 * v.data + (1 - self.beta2) * theta_t.grad ** 2\n",
    "            state['v'].data = v_t\n",
    "\n",
    "            # Temps\n",
    "            t = state['t'] + 1\n",
    "            state['t'] = t\n",
    "\n",
    "            # Correction des biais\n",
    "            m_hat = m_t / (1 - self.beta1 ** t)\n",
    "            v_hat = v_t / (1 - self.beta2 ** t)\n",
    "\n",
    "            theta_t.data -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon) - self.learning_rate * self.weight_decay * theta_t.data\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ],
   "id": "94d7052ef2a65b78",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ***Test de AdamW***",
   "id": "505fef5fa0888d50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:02:32.121003Z",
     "start_time": "2025-01-20T18:02:32.081047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': AdamW_torch(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "8b4093aa3bfe36d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 210.1480255126953\n",
      "Epoch 20, Loss: 82.62165832519531\n",
      "Epoch 30, Loss: 22.42963218688965\n",
      "Epoch 40, Loss: 5.376733303070068\n",
      "Epoch 50, Loss: 4.686898231506348\n",
      "Epoch 60, Loss: 5.464333534240723\n",
      "Epoch 70, Loss: 4.954035758972168\n",
      "Epoch 80, Loss: 4.336297988891602\n",
      "Epoch 90, Loss: 4.101994037628174\n",
      "Epoch 100, Loss: 4.055665969848633\n",
      "weight: tensor([[2.9842]])\n",
      "bias: tensor([5.2393])\n"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:02:32.883301Z",
     "start_time": "2025-01-20T18:02:32.843273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': AdamW_torch(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "f9a3cc8059159a66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 118.39867401123047\n",
      "Epoch 20, Loss: 35.853355407714844\n",
      "Epoch 30, Loss: 10.578374862670898\n",
      "Epoch 40, Loss: 8.456700325012207\n",
      "Epoch 50, Loss: 7.222285747528076\n",
      "Epoch 60, Loss: 5.000164031982422\n",
      "Epoch 70, Loss: 4.1139140129089355\n",
      "Epoch 80, Loss: 4.039061069488525\n",
      "Epoch 90, Loss: 4.043933868408203\n",
      "Epoch 100, Loss: 4.068148612976074\n",
      "weight: tensor([[2.9916]])\n",
      "bias: tensor([5.2697])\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Evaluation des Optimiseurs<a name=\"evaluation-des-optimiseurs\"></a>***",
   "id": "195056380131d973"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:02:33.999298Z",
     "start_time": "2025-01-20T18:02:33.997082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def f(x : torch.Tensor | Tensor):\n",
    "    return (x - 2) ** 2\n",
    "\n",
    "\n",
    "def f_nonconvexe(x : torch.Tensor | Tensor):\n",
    "    return 3*x ** 2 - 2*x"
   ],
   "id": "ee1532fe2701e9b7",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:21:42.247502Z",
     "start_time": "2025-01-20T18:21:42.242788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_optim(x : torch.Tensor | Tensor, convexe : bool = True, scheduler : bool = False):\n",
    "    if convexe:\n",
    "        print(f\"Optimisation de la fonction convexe f(x) = (x - 2)²\")\n",
    "        y = f(x)\n",
    "    else:\n",
    "        print(f\"Optimisation de la fonction non convexe f(x) = 3x² - 2x\")\n",
    "        y = f_nonconvexe(x)\n",
    "\n",
    "    y.backward()\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        print(f\"Gradient de f en x={x.item()}: x.grad={x.grad.item()}\")\n",
    "    else:\n",
    "        print(f\"Gradient de f en x={x.data}: x.grad={x.grad}\")\n",
    "\n",
    "    resulting_x = []\n",
    "    resulting_fx = []\n",
    "\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        optimizers_torch = [\n",
    "            SGD_torch,\n",
    "            RMSProp_torch,\n",
    "            Adagrad_torch,\n",
    "            Adam_torch,\n",
    "            AdamW_torch\n",
    "        ]\n",
    "\n",
    "        for optimizer in optimizers_torch:\n",
    "            optimizer = optimizer([x])\n",
    "            if scheduler:\n",
    "                scheduler = LRSchedulerOnPlateau(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "            for i in range(100):\n",
    "                optimizer.zero_grad()\n",
    "                if convexe:\n",
    "                    y = f(x)\n",
    "                else:\n",
    "                    y = f_nonconvexe(x)\n",
    "                y.backward()\n",
    "                optimizer.step()\n",
    "                if scheduler:\n",
    "                    scheduler.step(y)\n",
    "            print(f\"Optimiseur {optimizer.__class__.__name__}: x={x.item()}, f(x)={f(x).item()}\")\n",
    "            resulting_x.append(x.item())\n",
    "            resulting_fx.append(f(x).item())\n",
    "\n",
    "    else:\n",
    "        optimizers_custom = [\n",
    "            SGD_custom,\n",
    "            RMSProp_custom,\n",
    "            Adagrad_custom,\n",
    "            Adam_custom,\n",
    "            AdamW_custom\n",
    "        ]\n",
    "\n",
    "        for optimizer in optimizers_custom:\n",
    "            optimizer = optimizer([x])\n",
    "            if scheduler:\n",
    "                scheduler = LRSchedulerOnPlateau(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "            for i in range(100):\n",
    "                optimizer.zero_grad()\n",
    "                if convexe:\n",
    "                    y = f(x)\n",
    "                else:\n",
    "                    y = f_nonconvexe(x)\n",
    "                y.backward()\n",
    "                optimizer.step()\n",
    "                if scheduler:\n",
    "                    scheduler.step(y)\n",
    "            print(f\"Optimiseur {optimizer.__class__.__name__}: x={x.data}, f(x)={f(x).data}\")\n",
    "            resulting_x.append(x.data)\n",
    "            resulting_fx.append(f(x).data)\n",
    "\n",
    "    return resulting_x, resulting_fx"
   ],
   "id": "a4424025c79c9f87",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:21:42.975983Z",
     "start_time": "2025-01-20T18:21:42.790638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([69.], requires_grad=True)\n",
    "conv_torch_x, conv_torch_fx = eval_optim(x, convexe=True)\n",
    "print()\n",
    "nonconv_torch_x, nonconv_torch_fx = eval_optim(x, convexe=False)"
   ],
   "id": "85f4258055c8e2fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la fonction convexe f(x) = (x - 2)²\n",
      "Gradient de f en x=69.0: x.grad=134.0\n",
      "Optimiseur SGD_torch: x=10.885512351989746, f(x)=78.95233154296875\n",
      "Optimiseur RMSProp_torch: x=9.80399227142334, f(x)=60.90229415893555\n",
      "Optimiseur Adagrad_torch: x=9.618916511535645, f(x)=58.047889709472656\n",
      "Optimiseur Adam_torch: x=9.519140243530273, f(x)=56.537471771240234\n",
      "Optimiseur AdamW_torch: x=9.428813934326172, f(x)=55.18727493286133\n",
      "\n",
      "Optimisation de la fonction non convexe f(x) = 3x² - 2x\n",
      "Gradient de f en x=9.428813934326172: x.grad=69.43231201171875\n",
      "Optimiseur SGD_torch: x=0.3520234227180481, f(x)=2.715826988220215\n",
      "Optimiseur RMSProp_torch: x=0.33838367462158203, f(x)=2.7609689235687256\n",
      "Optimiseur Adagrad_torch: x=0.3333333432674408, f(x)=2.777777671813965\n",
      "Optimiseur Adam_torch: x=0.3333333432674408, f(x)=2.777777671813965\n",
      "Optimiseur AdamW_torch: x=0.3333345949649811, f(x)=2.77777361869812\n"
     ]
    }
   ],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:21:43.598540Z",
     "start_time": "2025-01-20T18:21:43.583807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = Tensor(69.)\n",
    "conv_custom_x, conv_custom_fx = eval_optim(x, convexe=True)\n",
    "print()\n",
    "nonconv_custom_x, nonconv_custom_fx = eval_optim(x, convexe=False)"
   ],
   "id": "f74e4aa2f7e56bd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la fonction convexe f(x) = (x - 2)²\n",
      "Gradient de f en x=69.0: x.grad=134.0\n",
      "Optimiseur SGD_custom: x=10.885510244948467, f(x)=78.95229231308416\n",
      "Optimiseur RMSProp_custom: x=9.803987775902403, f(x)=60.90222520643413\n",
      "Optimiseur Adagrad_custom: x=9.618913070757662, f(x)=58.04783637976195\n",
      "Optimiseur Adam_custom: x=9.519135827365043, f(x)=56.53740359036458\n",
      "Optimiseur AdamW_custom: x=9.428814427498079, f(x)=55.18728379820361\n",
      "\n",
      "Optimisation de la fonction non convexe f(x) = 3x² - 2x\n",
      "Gradient de f en x=9.428814427498079: x.grad=69.43231722986997\n",
      "Optimiseur SGD_custom: x=0.3520234079595069, f(x)=2.715826847913398\n",
      "Optimiseur RMSProp_custom: x=0.3383837368874569, f(x)=2.760968605840092\n",
      "Optimiseur Adagrad_custom: x=0.3333333333333333, f(x)=2.777777777777778\n",
      "Optimiseur Adam_custom: x=0.3333333333333333, f(x)=2.777777777777778\n",
      "Optimiseur AdamW_custom: x=0.33333456306680137, f(x)=2.77777367866773\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:22:41.302704Z",
     "start_time": "2025-01-20T18:22:41.300391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "check_diffs(conv_torch_x, conv_custom_x, tol=1e-4)\n",
    "print()\n",
    "check_diffs(conv_torch_fx, conv_custom_fx, tol=1e-4)"
   ],
   "id": "15c9a8928c275420",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements between\n",
      "[10.885512351989746, 9.80399227142334, 9.618916511535645, 9.519140243530273, 9.428813934326172]\u0007nd\n",
      "[10.885510244948467, 9.803987775902403, 9.618913070757662, 9.519135827365043, 9.428814427498079]\n",
      "are close within a tolerance of 0.0001\n",
      "\n",
      "All elements between\n",
      "[78.95233154296875, 60.90229415893555, 58.047889709472656, 56.537471771240234, 55.18727493286133]\u0007nd\n",
      "[78.95229231308416, 60.90222520643413, 58.04783637976195, 56.53740359036458, 55.18728379820361]\n",
      "are close within a tolerance of 0.0001\n"
     ]
    }
   ],
   "execution_count": 165
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Réseau de Neurones<a name=\"réseau-de-neurones\"></a>***",
   "id": "1d95d3f325e5b75f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:07:46.612691Z",
     "start_time": "2025-01-20T18:07:46.610195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def func_nn(x, W1, b1, W2, b2):\n",
    "    h1 = W1 * x + b1\n",
    "    y = W2 * h1 + b2\n",
    "    return y\n",
    "\n",
    "\n",
    "def mse(y, y_hat):\n",
    "    return (y - y_hat) ** 2"
   ],
   "id": "f1369e9cd3e0638b",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:23:52.914513Z",
     "start_time": "2025-01-20T18:23:52.905546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_nn_optim(scheduler : bool = False, custom : bool = True):\n",
    "    results = []\n",
    "\n",
    "    if not custom:\n",
    "        optimizers = [\n",
    "            SGD_torch,\n",
    "            RMSProp_torch,\n",
    "            Adagrad_torch,\n",
    "            Adam_torch,\n",
    "            AdamW_torch\n",
    "        ]\n",
    "\n",
    "        for optimizer in optimizers:\n",
    "            W1 = torch.tensor([1.], requires_grad=True)\n",
    "            b1 = torch.tensor([1.], requires_grad=True)\n",
    "            W2 = torch.tensor([1.], requires_grad=True)\n",
    "            b2 = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "            x = torch.tensor([1.], requires_grad=True)\n",
    "            y = torch.tensor([10.])\n",
    "\n",
    "            optimizer = optimizer([W1, b1, W2, b2])\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler = LRSchedulerOnPlateau(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "\n",
    "            for i in range(100):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_hat = func_nn(x, W1, b1, W2, b2)\n",
    "                loss = mse(y, y_hat)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step(loss)\n",
    "\n",
    "            print(f\"Optimiseur {optimizer.__class__.__name__}:\\nW1={W1.item()}, b1={b1.item()}, W2={W2.item()}, b2={b2.item()}\")\n",
    "            results.append([W1.item(), b1.item(), W2.item(), b2.item()])\n",
    "\n",
    "    else:\n",
    "        optimizers = [\n",
    "            SGD_custom,\n",
    "            RMSProp_custom,\n",
    "            Adagrad_custom,\n",
    "            Adam_custom,\n",
    "            AdamW_custom\n",
    "        ]\n",
    "\n",
    "        for optimizer in optimizers:\n",
    "            W1 = Tensor(1.)\n",
    "            b1 = Tensor(1.)\n",
    "            W2 = Tensor(1.)\n",
    "            b2 = Tensor(1.)\n",
    "\n",
    "            x = Tensor(1.)\n",
    "            y = Tensor(10.)\n",
    "\n",
    "            optimizer = optimizer([W1, b1, W2, b2])\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler = LRSchedulerOnPlateau(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "\n",
    "            for i in range(100):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_hat = func_nn(x, W1, b1, W2, b2)\n",
    "                loss = mse(y, y_hat)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step(loss)\n",
    "\n",
    "            print(f\"Optimiseur {optimizer.__class__.__name__}:\\nW1={W1.data}, b1={b1.data}, W2={W2.data}, b2={b2.data}\")\n",
    "            results.append([W1.data, b1.data, W2.data, b2.data])\n",
    "\n",
    "    return results"
   ],
   "id": "da4600406c78abac",
   "outputs": [],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:24:25.900189Z",
     "start_time": "2025-01-20T18:24:25.753309Z"
    }
   },
   "cell_type": "code",
   "source": "torch_nn = eval_nn_optim(scheduler=False, custom=False)",
   "id": "7e9a1e2f9cc247a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimiseur SGD_torch:\n",
      "W1=1.7965515851974487, b1=1.7965515851974487, W2=2.356534481048584, b2=1.532727837562561\n",
      "Optimiseur RMSProp_torch:\n",
      "W1=1.956400752067566, b1=1.956400752067566, W2=1.956400752067566, b2=1.899566411972046\n",
      "Optimiseur Adagrad_torch:\n",
      "W1=1.186563491821289, b1=1.186563491821289, W2=1.186563491821289, b2=1.1806892156600952\n",
      "Optimiseur Adam_torch:\n",
      "W1=1.1003371477127075, b1=1.1003371477127075, W2=1.1003371477127075, b2=1.0987093448638916\n",
      "Optimiseur AdamW_torch:\n",
      "W1=1.1013890504837036, b1=1.1013890504837036, W2=1.1013890504837036, b2=1.0997445583343506\n"
     ]
    }
   ],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:24:26.938635Z",
     "start_time": "2025-01-20T18:24:26.925113Z"
    }
   },
   "cell_type": "code",
   "source": "custom_nn = eval_nn_optim(scheduler=False, custom=True)",
   "id": "408b64606de25ff0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimiseur SGD_custom:\n",
      "W1=1.7965517126874235, b1=1.7965517126874235, W2=2.3565344500454675, b2=1.532727995527799\n",
      "Optimiseur RMSProp_custom:\n",
      "W1=1.9564006987933535, b1=1.9564006987933535, W2=1.9564006987933535, b2=1.8995662439967032\n",
      "Optimiseur Adagrad_custom:\n",
      "W1=1.1865635594156694, b1=1.1865635594156694, W2=1.1865635594156694, b2=1.1806890649673805\n",
      "Optimiseur Adam_custom:\n",
      "W1=1.1003370383873736, b1=1.1003370383873736, W2=1.1003370384224467, b2=1.0987096217296624\n",
      "Optimiseur AdamW_custom:\n",
      "W1=1.1013890816161964, b1=1.1013890816161964, W2=1.1013890816512846, b2=1.0997449288859633\n"
     ]
    }
   ],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:24:35.282265Z",
     "start_time": "2025-01-20T18:24:35.279908Z"
    }
   },
   "cell_type": "code",
   "source": "check_diffs(torch_nn, custom_nn, tol=1e-4)",
   "id": "777fc773f3bc260a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements between\n",
      "[[1.7965515851974487, 1.7965515851974487, 2.356534481048584, 1.532727837562561], [1.956400752067566, 1.956400752067566, 1.956400752067566, 1.899566411972046], [1.186563491821289, 1.186563491821289, 1.186563491821289, 1.1806892156600952], [1.1003371477127075, 1.1003371477127075, 1.1003371477127075, 1.0987093448638916], [1.1013890504837036, 1.1013890504837036, 1.1013890504837036, 1.0997445583343506]]\u0007nd\n",
      "[[1.7965517126874235, 1.7965517126874235, 2.3565344500454675, 1.532727995527799], [1.9564006987933535, 1.9564006987933535, 1.9564006987933535, 1.8995662439967032], [1.1865635594156694, 1.1865635594156694, 1.1865635594156694, 1.1806890649673805], [1.1003370383873736, 1.1003370383873736, 1.1003370384224467, 1.0987096217296624], [1.1013890816161964, 1.1013890816161964, 1.1013890816512846, 1.0997449288859633]]\n",
      "are close within a tolerance of 0.0001\n"
     ]
    }
   ],
   "execution_count": 169
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Scheduler de Taux d'apprentissage<a name=\"schedulers\"></a>***",
   "id": "5b3df25542164dbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de LRScheduler**",
   "id": "533f3bef6468a3a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:27:28.685162Z",
     "start_time": "2025-01-20T18:27:28.682626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LRScheduler:\n",
    "    def __init__(self, optimizer, initial_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    def set_lr(self, lr):\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group['lr'] = lr"
   ],
   "id": "1a5628f99bfe8a7f",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de LRSchedulerOnPlateau**",
   "id": "4c7b175f2dd4c96e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:27:31.742054Z",
     "start_time": "2025-01-20T18:27:31.738184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LRSchedulerOnPlateau(LRScheduler):\n",
    "    def __init__(self, optimizer, initial_lr, patience=10, factor=0.1, min_lr=1e-6, mode='min', threshold=1e-4):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.best_value = None\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, current_value):\n",
    "        if self.best_value is None:\n",
    "            self.best_value = current_value\n",
    "            return\n",
    "\n",
    "        if self.mode == 'min':\n",
    "            improvement = self.best_value - current_value\n",
    "        elif self.mode == 'max':\n",
    "            improvement = current_value - self.best_value\n",
    "        else:\n",
    "            raise ValueError(\"Mode must be either 'min' (minimize) or 'max' (maximize).\")\n",
    "\n",
    "        if improvement > self.threshold:\n",
    "            self.best_value = current_value\n",
    "            self.num_bad_epochs = 0\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            self.reduce_lr()\n",
    "\n",
    "    def reduce_lr(self):\n",
    "        current_lr = self.get_lr()\n",
    "        new_lr = max(current_lr * self.factor, self.min_lr)\n",
    "        if new_lr < current_lr:\n",
    "            print(f\"Reducing learning rate: {current_lr:.6f} -> {new_lr:.6f}\")\n",
    "            self.set_lr(new_lr)\n",
    "        self.num_bad_epochs = 0"
   ],
   "id": "720c56dbb5332b80",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de LRSchedulerOnPlateau**",
   "id": "a0015f5a3eab0528"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:27:33.308728Z",
     "start_time": "2025-01-20T18:27:33.100323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([69.], requires_grad=True)\n",
    "conv_torch_scheduler_x, conv_torch_scheduler_fx = eval_optim(x, convexe=True, scheduler=True)\n",
    "print()\n",
    "nonconv_torch_scheduler_x, nonconv_torch_scheduler_fx = eval_optim(x, convexe=False, scheduler=True)"
   ],
   "id": "e38504ba155084ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la fonction convexe f(x) = (x - 2)²\n",
      "Gradient de f en x=69.0: x.grad=134.0\n",
      "Optimiseur SGD_torch: x=10.885512351989746, f(x)=78.95233154296875\n",
      "Optimiseur RMSProp_torch: x=9.80399227142334, f(x)=60.90229415893555\n",
      "Optimiseur Adagrad_torch: x=9.618916511535645, f(x)=58.047889709472656\n",
      "Optimiseur Adam_torch: x=9.519140243530273, f(x)=56.537471771240234\n",
      "Optimiseur AdamW_torch: x=9.428813934326172, f(x)=55.18727493286133\n",
      "\n",
      "Optimisation de la fonction non convexe f(x) = 3x² - 2x\n",
      "Gradient de f en x=9.428813934326172: x.grad=69.43231201171875\n",
      "Optimiseur SGD_torch: x=0.3520234227180481, f(x)=2.715826988220215\n",
      "Reducing learning rate: 0.010000 -> 0.005000\n",
      "Reducing learning rate: 0.005000 -> 0.002500\n",
      "Reducing learning rate: 0.002500 -> 0.001250\n",
      "Reducing learning rate: 0.001250 -> 0.000625\n",
      "Reducing learning rate: 0.000625 -> 0.000313\n",
      "Reducing learning rate: 0.000313 -> 0.000156\n",
      "Reducing learning rate: 0.000156 -> 0.000078\n",
      "Reducing learning rate: 0.000078 -> 0.000039\n",
      "Reducing learning rate: 0.000039 -> 0.000020\n",
      "Reducing learning rate: 0.000020 -> 0.000010\n",
      "Reducing learning rate: 0.000010 -> 0.000005\n",
      "Reducing learning rate: 0.000005 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Reducing learning rate: 0.000001 -> 0.000001\n",
      "Optimiseur RMSProp_torch: x=0.3333333432674408, f(x)=2.777777671813965\n",
      "Reducing learning rate: 0.010000 -> 0.005000\n",
      "Reducing learning rate: 0.005000 -> 0.002500\n",
      "Reducing learning rate: 0.002500 -> 0.001250\n",
      "Reducing learning rate: 0.001250 -> 0.000625\n",
      "Reducing learning rate: 0.000625 -> 0.000313\n",
      "Reducing learning rate: 0.000313 -> 0.000156\n",
      "Reducing learning rate: 0.000156 -> 0.000078\n",
      "Reducing learning rate: 0.000078 -> 0.000039\n",
      "Reducing learning rate: 0.000039 -> 0.000020\n",
      "Reducing learning rate: 0.000020 -> 0.000010\n",
      "Reducing learning rate: 0.000010 -> 0.000005\n",
      "Reducing learning rate: 0.000005 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Reducing learning rate: 0.000001 -> 0.000001\n",
      "Optimiseur Adagrad_torch: x=nan, f(x)=nan\n",
      "Reducing learning rate: 0.001000 -> 0.000500\n",
      "Reducing learning rate: 0.000500 -> 0.000250\n",
      "Reducing learning rate: 0.000250 -> 0.000125\n",
      "Reducing learning rate: 0.000125 -> 0.000063\n",
      "Reducing learning rate: 0.000063 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000016\n",
      "Reducing learning rate: 0.000016 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur Adam_torch: x=nan, f(x)=nan\n",
      "Reducing learning rate: 0.001000 -> 0.000500\n",
      "Reducing learning rate: 0.000500 -> 0.000250\n",
      "Reducing learning rate: 0.000250 -> 0.000125\n",
      "Reducing learning rate: 0.000125 -> 0.000063\n",
      "Reducing learning rate: 0.000063 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000016\n",
      "Reducing learning rate: 0.000016 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur AdamW_torch: x=nan, f(x)=nan\n"
     ]
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:27:34.075006Z",
     "start_time": "2025-01-20T18:27:34.042061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = Tensor(69.)\n",
    "conv_custom_scheduler_x, conv_custom_scheduler_fx = eval_optim(x, convexe=True, scheduler=True)\n",
    "print()\n",
    "nonconv_custom_scheduler_x, nonconv_custom_scheduler_fx = eval_optim(x, convexe=False, scheduler=True)"
   ],
   "id": "e938b0f6a751f60b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la fonction convexe f(x) = (x - 2)²\n",
      "Gradient de f en x=69.0: x.grad=134.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'Tensor' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[177], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m x \u001B[38;5;241m=\u001B[39m Tensor(\u001B[38;5;241m69.\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m conv_custom_scheduler_x, conv_custom_scheduler_fx \u001B[38;5;241m=\u001B[39m \u001B[43meval_optim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvexe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m()\n\u001B[1;32m      4\u001B[0m nonconv_custom_scheduler_x, nonconv_custom_scheduler_fx \u001B[38;5;241m=\u001B[39m eval_optim(x, convexe\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, scheduler\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[158], line 67\u001B[0m, in \u001B[0;36meval_optim\u001B[0;34m(x, convexe, scheduler)\u001B[0m\n\u001B[1;32m     65\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m scheduler:\n\u001B[0;32m---> 67\u001B[0m         \u001B[43mscheduler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimiseur \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moptimizer\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: x=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, f(x)=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mf(x)\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     69\u001B[0m resulting_x\u001B[38;5;241m.\u001B[39mappend(x\u001B[38;5;241m.\u001B[39mdata)\n",
      "Cell \u001B[0;32mIn[175], line 25\u001B[0m, in \u001B[0;36mLRSchedulerOnPlateau.step\u001B[0;34m(self, current_value)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMode must be either \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m (minimize) or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m (maximize).\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mimprovement\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mthreshold\u001B[49m:\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_value \u001B[38;5;241m=\u001B[39m current_value\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_bad_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[0;31mTypeError\u001B[0m: '>' not supported between instances of 'Tensor' and 'float'"
     ]
    }
   ],
   "execution_count": 177
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:10:22.105872Z",
     "start_time": "2025-01-20T17:10:21.955068Z"
    }
   },
   "cell_type": "code",
   "source": "eval_nn_optim(scheduler=True)",
   "id": "454c7f191b933243",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing learning rate: 0.010000 -> 0.005000\n",
      "Reducing learning rate: 0.005000 -> 0.002500\n",
      "Reducing learning rate: 0.002500 -> 0.001250\n",
      "Reducing learning rate: 0.001250 -> 0.000625\n",
      "Reducing learning rate: 0.000625 -> 0.000313\n",
      "Reducing learning rate: 0.000313 -> 0.000156\n",
      "Reducing learning rate: 0.000156 -> 0.000078\n",
      "Reducing learning rate: 0.000078 -> 0.000039\n",
      "Reducing learning rate: 0.000039 -> 0.000020\n",
      "Reducing learning rate: 0.000020 -> 0.000010\n",
      "Reducing learning rate: 0.000010 -> 0.000005\n",
      "Reducing learning rate: 0.000005 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Reducing learning rate: 0.000001 -> 0.000001\n",
      "Optimiseur SGD_torch:\n",
      "W1=1.796550989151001, b1=1.796550989151001, W2=2.3565328121185303, b2=1.5327274799346924\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.796550989151001, b1=1.796550989151001, W2=2.3565328121185303, b2=1.5327274799346924\n",
      "\n",
      "Optimiseur RMSProp_torch:\n",
      "W1=1.956400752067566, b1=1.956400752067566, W2=1.956400752067566, b2=1.899566411972046\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.956400752067566, b1=1.956400752067566, W2=1.956400752067566, b2=1.899566411972046\n",
      "\n",
      "Optimiseur Adagrad_torch:\n",
      "W1=1.186563491821289, b1=1.186563491821289, W2=1.186563491821289, b2=1.1806892156600952\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.186563491821289, b1=1.186563491821289, W2=1.186563491821289, b2=1.1806892156600952\n",
      "\n",
      "Optimiseur Adam_torch:\n",
      "W1=1.1003371477127075, b1=1.1003371477127075, W2=1.1003371477127075, b2=1.0987093448638916\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.1003371477127075, b1=1.1003371477127075, W2=1.1003371477127075, b2=1.0987093448638916\n",
      "\n",
      "Optimiseur AdamW_torch:\n",
      "W1=1.1013890504837036, b1=1.1013890504837036, W2=1.1013890504837036, b2=1.0997445583343506\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.1013890504837036, b1=1.1013890504837036, W2=1.1013890504837036, b2=1.0997445583343506\n",
      "\n"
     ]
    }
   ],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
