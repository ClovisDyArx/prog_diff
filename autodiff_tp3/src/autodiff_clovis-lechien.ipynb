{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e418516f7da7ad36",
   "metadata": {},
   "source": [
    "# ***TP3 - Clovis Lechien***\n",
    "\n",
    "## **Sommaire**\n",
    "1. [Utils](#Utils)\n",
    "2. [SGD](#SGD)\n",
    "3. [RMSProp](#RMSProp)\n",
    "4. [Adagrad](#Adagrad)\n",
    "5. [Adam](#Adam)\n",
    "6. [AdamW](#AdamW)\n",
    "7. [Evaluations](#Evaluations)\n",
    "    1. Evaluation des Optimiseurs\n",
    "    2. Réseau de Neurones\n",
    "10. [Schedulers](#Schedulers)\n",
    "\n",
    "## **Mise en contexte**\n",
    "Pour chaque implémentation d'optimiseur et de lr scheduler, j'ai pris la décision d'implémenter 2 versions *\"from-scratch\"* :\n",
    "1. Utilisation de torch : j'ai utilisé l'autograd de torch pour la première version\n",
    "2. Vanilla : j'ai repris la classe Tensor en backward differentiation ainsi que les fonctions usuelles associées pour implémenter cette version *vraiment* ***from-scratch***.\n",
    "\n",
    "Ces versions seront utilisées tout au long du notebook pour comparer les résultats et vérifier que mes implémentations sont bonnes.\n",
    "\n",
    "J'ai également mis en place des tests intermédiaires orientés uniquement sur l'implémentation torch pour chaque optimiseur afin de vérifier leur bon fonctionnement avant la partie [Evaluations](#Evaluations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd73e25f-aa1f-436b-86e3-0e4e990d16b9",
   "metadata": {},
   "source": [
    "# ***Imports des librairies nécessaires***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:14.700085Z",
     "start_time": "2025-01-20T18:47:14.698122Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch                         # Utilisée pour l'implémentation des Optimizers torch \"from-scratch\"\n",
    "import torch.nn as nn                # Utilisée pour l'implémentation des Optimizers torch \"from-scratch\"\n",
    "from torch.optim import Optimizer    # Utilisée pour l'implémentation des Optimizers torch \"from-scratch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b054a38d6955342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:14.747889Z",
     "start_time": "2025-01-20T18:47:14.745463Z"
    }
   },
   "outputs": [],
   "source": [
    "# Génération du jeu de données linéaire\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "x_linear = np.linspace(-10, 10, n_samples)\n",
    "y_linear = 3 * x_linear + 5 + np.random.normal(0, 2, n_samples)\n",
    "\n",
    " # Génération du jeu de données non linéaire\n",
    "y_nonlinear = 0.5 * x_linear **2 - 4 * x_linear + np.random.normal(0 ,5 ,n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add1065b5a810b45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ***Custom Tensor class***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7002f36d0bcccbac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:14.800318Z",
     "start_time": "2025-01-20T18:47:14.792842Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "\n",
    "    \"\"\" stores a single scalar Tensor and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "\n",
    "        out = Tensor(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        out._prev = set([self, other])\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "\n",
    "        out = Tensor(self.data * other.data, [self, other], '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "\n",
    "        out = Tensor(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        # FIXME: implement relu\n",
    "        pass\n",
    "\n",
    "    def build_topo(self, visited=None, topo=None):\n",
    "        if self not in visited:\n",
    "            visited.add(self)\n",
    "            for child in self._prev:\n",
    "                child.build_topo(visited=visited, topo=topo)\n",
    "            topo.append(self)\n",
    "        return topo\n",
    "\n",
    "    def backward(self):\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        topo = self.build_topo(topo=topo, visited=visited)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d81e5677e344c4",
   "metadata": {},
   "source": [
    "## ***Custom operations***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52846f4c7ab39c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:14.851767Z",
     "start_time": "2025-01-20T18:47:14.845564Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_d(dual_number: Tensor):\n",
    "    out = Tensor(np.log(dual_number.data), (dual_number,), 'log')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += (1 / dual_number.data) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def exp_d(dual_number: Tensor):\n",
    "    out = Tensor(np.exp(dual_number.data), (dual_number,), 'exp')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += np.exp(dual_number.data) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def sin_d(dual_number: Tensor):\n",
    "    out = Tensor(np.sin(dual_number.data), (dual_number,), 'sin')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += np.cos(dual_number.data) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def cos_d(dual_number: Tensor):\n",
    "    out = Tensor(np.cos(dual_number.data), (dual_number,), 'cos')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += -np.sin(dual_number.data) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def sigmoid_d(dual_number: Tensor):\n",
    "    sig = 1 / (1 + np.exp(-dual_number.data))\n",
    "    out = Tensor(sig, (dual_number,), 'sigmoid')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += sig * (1 - sig) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def tanh_d(dual_number: Tensor):\n",
    "    tanh = np.tanh(dual_number.data)\n",
    "    out = Tensor(tanh, (dual_number,), 'tanh')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += (1 - tanh**2) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def tan_d(dual_number: Tensor):\n",
    "    out = Tensor(np.tan(dual_number.data), (dual_number,), 'tan')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += (1 / np.cos(dual_number.data)**2) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def sqrt_d(dual_number: Tensor):\n",
    "    out = Tensor(np.sqrt(dual_number.data), (dual_number,), 'sqrt')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += (0.5 / np.sqrt(dual_number.data)) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "\n",
    "def pow_d(dual_number: Tensor, power: int):\n",
    "    out = Tensor(dual_number.data**power, (dual_number,), f'pow{power}')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += (power * dual_number.data**(power-1)) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def softmax_d(dual_number: Tensor):\n",
    "    e = np.exp(dual_number.data - np.max(dual_number.data))\n",
    "    out = Tensor(e / np.sum(e), (dual_number,), 'softmax')\n",
    "\n",
    "    def _backward():\n",
    "        for i in range(len(dual_number.data)):\n",
    "            for j in range(len(dual_number.data)):\n",
    "                if i == j:\n",
    "                    dual_number.grad[i] += out.data[i] * (1 - out.data[i]) * out.grad[i]\n",
    "                else:\n",
    "                    dual_number.grad[i] += -out.data[i] * out.data[j] * out.grad[j]\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec84e42849b4a17",
   "metadata": {},
   "source": [
    "# ***Utils<a name=\"Utils\"></a>***\n",
    "\n",
    "```python\n",
    "def optimizer_testing_loop(parameters : dict[str,])\n",
    "```\n",
    "Cette fonction est responsable des tests intermédiaires mentionnés en haut de page.\n",
    "Elle mimique le comportement d'une boucle d'entraînement.\n",
    "Elle est utilisée uniquement pour tester l'implémentation des optimiseurs implémentés à l'aide de l'autograd de torch.\n",
    "\n",
    "Exemple d'utilisation :\n",
    "```python\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': SGD_torch(model.parameters(), learning_rate=0.015),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def check_diffs(a : list[float], b : list[float], tol : float = 1e-4)\n",
    "```\n",
    "Cette fonction sert à comparer les deux listes passées en paramètres et de vérifier que la différence absolue entre deux valeurs respectives est inférieure au seuil précisé.\n",
    "\n",
    "Exemple d'utilisation :\n",
    "```python\n",
    "check_diffs(losses_A, losses_B, tol=1e-4)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "14a6a420243220be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:14.902359Z",
     "start_time": "2025-01-20T18:47:14.899012Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimizer_testing_loop(parameters : dict[str,]):\n",
    "    model = parameters['model']\n",
    "\n",
    "    criterion = parameters['criterion']\n",
    "    optimizer = parameters['optimizer']\n",
    "\n",
    "    x_tensor = parameters['x_tensor']\n",
    "    y_tensor = parameters['y_tensor']\n",
    "\n",
    "    epochs = parameters['epochs']\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x_tensor)\n",
    "        loss = criterion(predictions, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.data}\")\n",
    "\n",
    "\n",
    "def check_diffs(a : list[float], b : list[float], tol : float = 1e-4):\n",
    "    res = np.allclose(a, b, atol=tol)\n",
    "    if res:\n",
    "        print(f\"All elements between\\n{a}\\nand\\n{b}\\nare close within a tolerance of {tol}\")\n",
    "    else:\n",
    "        print(\"Test failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfdde3bc692cd33",
   "metadata": {},
   "source": [
    "# ***SGD<a name=\"SGD\"></a>***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb8e9d42bd537a9",
   "metadata": {},
   "source": [
    "## **Implementation de SGD**\n",
    "\n",
    "```python\n",
    "class SGD_torch(Optimizer)\n",
    "```\n",
    "TODO: expliquer l'implémentation\n",
    "\n",
    "Exemple d'utilisation :\n",
    "```python\n",
    "sgd = SGD_torch(model.parameters(), learning_rate=0.015)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "class SGD_custom\n",
    "```\n",
    "TODO: expliquer l'implémentation\n",
    "\n",
    "Exemple d'utilisation :\n",
    "```python\n",
    "A = Tensor(1.)\n",
    "B = Tensor(69.)\n",
    "C = Tensor(420.)\n",
    "sgd = SGD_custom([A, B, C], learning_rate=0.015)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b1604245ddcf6ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:14.948618Z",
     "start_time": "2025-01-20T18:47:14.945511Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD_torch(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.015):\n",
    "        hyperparams = {'lr': learning_rate}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                theta_t -= lr * theta_t.grad\n",
    "\n",
    "\n",
    "class SGD_custom:\n",
    "    def __init__(self, params, learning_rate=0.015):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.data -= self.learning_rate * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e407ea443cb5d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Test de SGD_torch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f319e016a6d00cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.015074Z",
     "start_time": "2025-01-20T18:47:14.991940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 16.184354782104492\n",
      "Epoch 20, Loss: 10.640948295593262\n",
      "Epoch 30, Loss: 7.626477241516113\n",
      "Epoch 40, Loss: 5.987224102020264\n",
      "Epoch 50, Loss: 5.095807075500488\n",
      "Epoch 60, Loss: 4.611059188842773\n",
      "Epoch 70, Loss: 4.347457408905029\n",
      "Epoch 80, Loss: 4.204110622406006\n",
      "Epoch 90, Loss: 4.12615966796875\n",
      "Epoch 100, Loss: 4.083771228790283\n",
      "weight: tensor([[2.9703]])\n",
      "bias: tensor([4.9016])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': SGD_torch(model.parameters(), learning_rate=0.015),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe10005fb82a84f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.062573Z",
     "start_time": "2025-01-20T18:47:15.039147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 467.55975341796875\n",
      "Epoch 20, Loss: 380.6451416015625\n",
      "Epoch 30, Loss: 333.38153076171875\n",
      "Epoch 40, Loss: 307.6798095703125\n",
      "Epoch 50, Loss: 293.703369140625\n",
      "Epoch 60, Loss: 286.1030578613281\n",
      "Epoch 70, Loss: 281.97003173828125\n",
      "Epoch 80, Loss: 279.7225341796875\n",
      "Epoch 90, Loss: 278.5003662109375\n",
      "Epoch 100, Loss: 277.83575439453125\n",
      "weight: tensor([[-4.1445]])\n",
      "bias: tensor([16.5501])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': SGD_torch(model.parameters(), learning_rate=0.015),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e498551a6e055",
   "metadata": {},
   "source": [
    "# ***RMSProp<a name=\"RMSProp\"></a>***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777cec9838bb901a",
   "metadata": {},
   "source": [
    "## **Implementation de RMSProp**\n",
    "\n",
    "```python\n",
    "class RMSProp_torch(Optimizer)\n",
    "```\n",
    "TODO: expliquer l'implémentation\n",
    "\n",
    "Exemple d'utilisation :\n",
    "```python\n",
    "rms = RMSProp_torch(model.parameters(), learning_rate=0.05, decay=0.5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "class RMSProp_custom\n",
    "```\n",
    "TODO: expliquer l'implémentation\n",
    "\n",
    "Exemple d'utilisation :\n",
    "```python\n",
    "A = Tensor(1.)\n",
    "B = Tensor(69.)\n",
    "C = Tensor(420.)\n",
    "rms = RMSProp_custom([A, B, C], learning_rate=0.05, decay=0.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa87be320c6bac1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.090154Z",
     "start_time": "2025-01-20T18:47:15.086149Z"
    }
   },
   "outputs": [],
   "source": [
    "class RMSProp_torch(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.05, decay=0.5):\n",
    "        hyperparams = {'lr': learning_rate, 'decay': decay}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            decay = group['decay']\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'square_avg' not in state:\n",
    "                    state['square_avg'] = torch.zeros_like(theta_t)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                square_avg = decay * square_avg + (1 - decay) * (theta_t.grad ** 2)\n",
    "                state['square_avg'] = square_avg\n",
    "\n",
    "                theta_t -= lr * theta_t.grad / square_avg.sqrt()\n",
    "\n",
    "\n",
    "class RMSProp_custom:\n",
    "    def __init__(self, params, learning_rate=0.05, decay=0.5):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.state = {param: {'square_avg': Tensor(0.0)} for param in params}\n",
    "\n",
    "    def step(self):\n",
    "        for theta_t in self.params:\n",
    "            if theta_t.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[theta_t]\n",
    "\n",
    "            square_avg = state['square_avg']\n",
    "            square_avg.data = self.decay * square_avg.data + (1 - self.decay) * (theta_t.grad ** 2)\n",
    "            state['square_avg'] = square_avg\n",
    "\n",
    "            theta_t.data -= self.learning_rate * theta_t.grad / np.sqrt(square_avg.data)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb4b122e967c04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Test de RMSProp_torch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94571c79790e8b25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.158944Z",
     "start_time": "2025-01-20T18:47:15.132657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 180.6050567626953\n",
      "Epoch 20, Loss: 112.90521240234375\n",
      "Epoch 30, Loss: 62.201744079589844\n",
      "Epoch 40, Loss: 28.272367477416992\n",
      "Epoch 50, Loss: 10.602396011352539\n",
      "Epoch 60, Loss: 6.3285231590271\n",
      "Epoch 70, Loss: 5.107961654663086\n",
      "Epoch 80, Loss: 4.370968341827393\n",
      "Epoch 90, Loss: 4.07150936126709\n",
      "Epoch 100, Loss: 4.054510593414307\n",
      "weight: tensor([[2.9453]])\n",
      "bias: tensor([5.0998])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': RMSProp_torch(model.parameters(), learning_rate=0.05, decay=0.5),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91785a0139d74176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.206358Z",
     "start_time": "2025-01-20T18:47:15.179622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1101.2764892578125\n",
      "Epoch 20, Loss: 959.2235717773438\n",
      "Epoch 30, Loss: 834.4452514648438\n",
      "Epoch 40, Loss: 726.88427734375\n",
      "Epoch 50, Loss: 636.4846801757812\n",
      "Epoch 60, Loss: 563.16259765625\n",
      "Epoch 70, Loss: 506.7803955078125\n",
      "Epoch 80, Loss: 467.0693664550781\n",
      "Epoch 90, Loss: 443.2617492675781\n",
      "Epoch 100, Loss: 429.9840087890625\n",
      "weight: tensor([[-4.1391]])\n",
      "bias: tensor([5.0963])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': RMSProp_torch(model.parameters(), learning_rate=0.05, decay=0.5),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f010754e9cd887d7",
   "metadata": {},
   "source": [
    "# ***Adagrad<a name=\"Adagrad\"></a>***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242814cd90f5d60",
   "metadata": {},
   "source": [
    "## **Implementation de Adagrad**\n",
    "\n",
    "```python\n",
    "class Adagrad_torch(Optimizer)\n",
    "```\n",
    "TODO: expliquer l'implémentation\n",
    "\n",
    "Exemple d'utilisation :\n",
    "```python\n",
    "adagrad = Adagrad_torch(model.parameters(), learning_rate=0.9)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "class Adagrad_custom\n",
    "```\n",
    "TODO: expliquer l'implémentation\n",
    "\n",
    "Exemple d'utilisation :\n",
    "```python\n",
    "A = Tensor(1.)\n",
    "B = Tensor(69.)\n",
    "C = Tensor(420.)\n",
    "adagrad = Adagrad_custom([A, B, C], learning_rate=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1368be27bac6048a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.229589Z",
     "start_time": "2025-01-20T18:47:15.225688Z"
    }
   },
   "outputs": [],
   "source": [
    "class Adagrad_torch(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.9):\n",
    "        hyperparams = {'lr': learning_rate}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'sum_squared_grads' not in state:\n",
    "                    state['sum_squared_grads'] = torch.zeros_like(theta_t)\n",
    "\n",
    "                sum_squared_grads = state['sum_squared_grads']\n",
    "                sum_squared_grads += theta_t.grad ** 2\n",
    "                state['sum_squared_grads'] = sum_squared_grads\n",
    "\n",
    "                adjusted_lr = lr / sum_squared_grads.sqrt()\n",
    "\n",
    "                theta_t -= adjusted_lr * theta_t.grad\n",
    "\n",
    "\n",
    "class Adagrad_custom:\n",
    "    def __init__(self, params, learning_rate=0.9):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.state = {param: {'sum_squared_grads': Tensor(0.0)} for param in params}\n",
    "\n",
    "    def step(self):\n",
    "        for theta_t in self.params:\n",
    "            if theta_t.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[theta_t]\n",
    "\n",
    "            sum_squared_grads = state['sum_squared_grads']\n",
    "            sum_squared_grads.data += theta_t.grad ** 2\n",
    "            state['sum_squared_grads'] = sum_squared_grads\n",
    "\n",
    "            adjusted_lr = self.learning_rate / np.sqrt(sum_squared_grads.data)\n",
    "\n",
    "            theta_t.data -= adjusted_lr * theta_t.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5105ec47c4d5e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Test de Adagrad_torch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c43614bd41ea6d64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.297398Z",
     "start_time": "2025-01-20T18:47:15.272206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 7.353924751281738\n",
      "Epoch 20, Loss: 4.580388069152832\n",
      "Epoch 30, Loss: 4.131178855895996\n",
      "Epoch 40, Loss: 4.050938129425049\n",
      "Epoch 50, Loss: 4.036445617675781\n",
      "Epoch 60, Loss: 4.033823013305664\n",
      "Epoch 70, Loss: 4.033348560333252\n",
      "Epoch 80, Loss: 4.033262729644775\n",
      "Epoch 90, Loss: 4.033247470855713\n",
      "Epoch 100, Loss: 4.0332441329956055\n",
      "weight: tensor([[2.9703]])\n",
      "bias: tensor([5.1189])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adagrad_torch(model.parameters(), learning_rate=0.9),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba6af84cb07e158b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.345087Z",
     "start_time": "2025-01-20T18:47:15.319330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 437.4356689453125\n",
      "Epoch 20, Loss: 384.6959228515625\n",
      "Epoch 30, Loss: 356.72442626953125\n",
      "Epoch 40, Loss: 337.80859375\n",
      "Epoch 50, Loss: 324.18017578125\n",
      "Epoch 60, Loss: 314.0190124511719\n",
      "Epoch 70, Loss: 306.27508544921875\n",
      "Epoch 80, Loss: 300.28387451171875\n",
      "Epoch 90, Loss: 295.5985412597656\n",
      "Epoch 100, Loss: 291.9052734375\n",
      "weight: tensor([[-4.1445]])\n",
      "bias: tensor([13.6006])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adagrad_torch(model.parameters(), learning_rate=0.9),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a43547f37f00b",
   "metadata": {},
   "source": [
    "# ***Adam<a name=\"Adam\"></a>***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d5b0e1de91cdd",
   "metadata": {},
   "source": [
    "## **Implementation de Adam**\n",
    "\n",
    "```python\n",
    "class Adam_torch(Optimizer)\n",
    "```\n",
    "TODO: expliquer l'implémentation\n",
    "\n",
    "Exemple d'utilisation :\n",
    "```python\n",
    "adam = Adam_torch(model.parameters(), learning_rate=0.25, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "class Adam_custom\n",
    "```\n",
    "TODO: expliquer l'implémentation\n",
    "\n",
    "Exemple d'utilisation :\n",
    "```python\n",
    "A = Tensor(1.)\n",
    "B = Tensor(69.)\n",
    "C = Tensor(420.)\n",
    "adam = Adam_custom([A, B, C], learning_rate=0.25, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b6573846cf44e8b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.371564Z",
     "start_time": "2025-01-20T18:47:15.365801Z"
    }
   },
   "outputs": [],
   "source": [
    "class Adam_torch(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.25, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        hyperparams = {'lr': learning_rate, 'beta1': beta1, 'beta2': beta2, 'epsilon': epsilon}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'm' not in state: # Moment d'ordre 1\n",
    "                    state['m'] = torch.zeros_like(theta_t)\n",
    "                if 'v' not in state: # Moment d'ordre 2\n",
    "                    state['v'] = torch.zeros_like(theta_t)\n",
    "                if 't' not in state: # Temps\n",
    "                    state['t'] = 0\n",
    "\n",
    "                # Premier Moment\n",
    "                m = state['m']\n",
    "                m_t = beta1 * m + (1 - beta1) * theta_t.grad\n",
    "                state['m'] = m_t\n",
    "\n",
    "                # Second Moment\n",
    "                v = state['v']\n",
    "                v_t = beta2 * v + (1 - beta2) * theta_t.grad ** 2\n",
    "                state['v'] = v_t\n",
    "\n",
    "                # Temps\n",
    "                t = state['t'] + 1\n",
    "                state['t'] = t\n",
    "\n",
    "                # Correction des biais\n",
    "                m_hat = m_t / (1 - beta1 ** t)\n",
    "                v_hat = v_t / (1 - beta2 ** t)\n",
    "\n",
    "                theta_t -= lr * m_hat / (v_hat.sqrt() + epsilon)\n",
    "\n",
    "\n",
    "class Adam_custom:\n",
    "    def __init__(self, params, learning_rate=0.25, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.state = {param: {'m': Tensor(0.0), 'v': Tensor(0.0), 't': 0} for param in params}\n",
    "\n",
    "    def step(self):\n",
    "        for theta_t in self.params:\n",
    "            if theta_t.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[theta_t]\n",
    "\n",
    "            # Premier Moment\n",
    "            m = state['m']\n",
    "            m_t = self.beta1 * m.data + (1 - self.beta1) * theta_t.grad\n",
    "            state['m'].data = m_t\n",
    "\n",
    "            # Second Moment\n",
    "            v = state['v']\n",
    "            v_t = self.beta2 * v.data + (1 - self.beta2) * theta_t.grad ** 2\n",
    "            state['v'].data = v_t\n",
    "\n",
    "            # Temps\n",
    "            t = state['t'] + 1\n",
    "            state['t'] = t\n",
    "\n",
    "            # Correction des biais\n",
    "            m_hat = m_t / (1 - self.beta1 ** t)\n",
    "            v_hat = v_t / (1 - self.beta2 ** t)\n",
    "\n",
    "            theta_t.data -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c210eec6a98cf32e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Test de Adam_torch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "521702270449bd96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.448702Z",
     "start_time": "2025-01-20T18:47:15.419225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 18.474105834960938\n",
      "Epoch 20, Loss: 17.985536575317383\n",
      "Epoch 30, Loss: 4.627365589141846\n",
      "Epoch 40, Loss: 5.256790637969971\n",
      "Epoch 50, Loss: 4.720978260040283\n",
      "Epoch 60, Loss: 4.059943675994873\n",
      "Epoch 70, Loss: 4.084419250488281\n",
      "Epoch 80, Loss: 4.069782257080078\n",
      "Epoch 90, Loss: 4.037606239318848\n",
      "Epoch 100, Loss: 4.033542156219482\n",
      "weight: tensor([[2.9711]])\n",
      "bias: tensor([5.1368])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adam_torch(model.parameters(), learning_rate=0.25, beta1=0.9, beta2=0.999, epsilon=1e-8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ff677ad6cf158b95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.496371Z",
     "start_time": "2025-01-20T18:47:15.466061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 627.2946166992188\n",
      "Epoch 20, Loss: 427.98394775390625\n",
      "Epoch 30, Loss: 393.12933349609375\n",
      "Epoch 40, Loss: 345.67926025390625\n",
      "Epoch 50, Loss: 314.45587158203125\n",
      "Epoch 60, Loss: 298.862548828125\n",
      "Epoch 70, Loss: 287.8588562011719\n",
      "Epoch 80, Loss: 282.3156433105469\n",
      "Epoch 90, Loss: 279.3364562988281\n",
      "Epoch 100, Loss: 277.9549865722656\n",
      "weight: tensor([[-4.1269]])\n",
      "bias: tensor([16.5093])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adam_torch(model.parameters(), learning_rate=0.25, beta1=0.9, beta2=0.999, epsilon=1e-8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fbceea56e83cc5",
   "metadata": {},
   "source": [
    "# ***AdamW<a name=\"AdamW\"></a>***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41fba5-d0bb-4e11-9202-ad883bfff01a",
   "metadata": {},
   "source": [
    "## ***Implémentation de AdamW***\n",
    "\n",
    "```python\n",
    "class AdamW_torch(Optimizer)\n",
    "```\n",
    "TODO: expliquer l'implémentation\n",
    "\n",
    "Exemple d'utilisation :\n",
    "```python\n",
    "adamw = AdamW_torch(model.parameters(), learning_rate=0.25, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "class AdamW_custom\n",
    "```\n",
    "TODO: expliquer l'implémentation\n",
    "\n",
    "Exemple d'utilisation :\n",
    "```python\n",
    "A = Tensor(1.)\n",
    "B = Tensor(69.)\n",
    "C = Tensor(420.)\n",
    "adamw = AdamW_custom([A, B, C], learning_rate=0.25, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "94d7052ef2a65b78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.518452Z",
     "start_time": "2025-01-20T18:47:15.512479Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdamW_torch(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.25, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):\n",
    "        hyperparams = {'lr': learning_rate, 'beta1': beta1, 'beta2': beta2, 'epsilon': epsilon, 'weight_decay': weight_decay}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'm' not in state: # Moment d'ordre 1\n",
    "                    state['m'] = torch.zeros_like(theta_t)\n",
    "                if 'v' not in state: # Moment d'ordre 2\n",
    "                    state['v'] = torch.zeros_like(theta_t)\n",
    "                if 't' not in state: # Temps\n",
    "                    state['t'] = 0\n",
    "\n",
    "                # Premier Moment\n",
    "                m = state['m']\n",
    "                m_t = beta1 * m + (1 - beta1) * theta_t.grad\n",
    "                state['m'] = m_t\n",
    "\n",
    "                # Second Moment\n",
    "                v = state['v']\n",
    "                v_t = beta2 * v + (1 - beta2) * theta_t.grad ** 2\n",
    "                state['v'] = v_t\n",
    "\n",
    "                # Temps\n",
    "                t = state['t'] + 1\n",
    "                state['t'] = t\n",
    "\n",
    "                # Correction des biais\n",
    "                m_hat = m_t / (1 - beta1 ** t)\n",
    "                v_hat = v_t / (1 - beta2 ** t)\n",
    "\n",
    "                theta_t -= lr * m_hat / (np.sqrt(v_hat) + epsilon) - lr * weight_decay * theta_t\n",
    "\n",
    "\n",
    "class AdamW_custom:\n",
    "    def __init__(self, params, learning_rate=0.25, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        self.state = {param: {'m': Tensor(0.0), 'v': Tensor(0.0), 't': 0} for param in params}\n",
    "\n",
    "    def step(self):\n",
    "        for theta_t in self.params:\n",
    "            if theta_t.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[theta_t]\n",
    "\n",
    "            # Premier Moment\n",
    "            m = state['m']\n",
    "            m_t = self.beta1 * m.data + (1 - self.beta1) * theta_t.grad\n",
    "            state['m'].data = m_t\n",
    "\n",
    "            # Second Moment\n",
    "            v = state['v']\n",
    "            v_t = self.beta2 * v.data + (1 - self.beta2) * theta_t.grad ** 2\n",
    "            state['v'].data = v_t\n",
    "\n",
    "            # Temps\n",
    "            t = state['t'] + 1\n",
    "            state['t'] = t\n",
    "\n",
    "            # Correction des biais\n",
    "            m_hat = m_t / (1 - self.beta1 ** t)\n",
    "            v_hat = v_t / (1 - self.beta2 ** t)\n",
    "\n",
    "            theta_t.data -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon) - self.learning_rate * self.weight_decay * theta_t.data\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505fef5fa0888d50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ***Test de AdamW***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8b4093aa3bfe36d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.600881Z",
     "start_time": "2025-01-20T18:47:15.565784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 97.443359375\n",
      "Epoch 20, Loss: 6.934173583984375\n",
      "Epoch 30, Loss: 25.01974868774414\n",
      "Epoch 40, Loss: 7.812467575073242\n",
      "Epoch 50, Loss: 4.84361457824707\n",
      "Epoch 60, Loss: 4.524322509765625\n",
      "Epoch 70, Loss: 4.178420066833496\n",
      "Epoch 80, Loss: 4.323681831359863\n",
      "Epoch 90, Loss: 4.06089973449707\n",
      "Epoch 100, Loss: 4.044063568115234\n",
      "weight: tensor([[2.9760]])\n",
      "bias: tensor([5.2175])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': AdamW_torch(model.parameters(), learning_rate=0.25, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f9a3cc8059159a66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.648158Z",
     "start_time": "2025-01-20T18:47:15.612995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 579.9456787109375\n",
      "Epoch 20, Loss: 454.680908203125\n",
      "Epoch 30, Loss: 410.2872314453125\n",
      "Epoch 40, Loss: 348.4303894042969\n",
      "Epoch 50, Loss: 317.8584899902344\n",
      "Epoch 60, Loss: 297.220947265625\n",
      "Epoch 70, Loss: 286.00286865234375\n",
      "Epoch 80, Loss: 279.979248046875\n",
      "Epoch 90, Loss: 277.4600524902344\n",
      "Epoch 100, Loss: 277.0702209472656\n",
      "weight: tensor([[-4.1685]])\n",
      "bias: tensor([17.5873])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': AdamW_torch(model.parameters(), learning_rate=0.25, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1a462-46da-41e6-adb1-49d1b54049b3",
   "metadata": {},
   "source": [
    "# ***Evaluations<a name=\"Evaluations\"></a>***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195056380131d973",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ***Evaluation des Optimiseurs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ee1532fe2701e9b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.665509Z",
     "start_time": "2025-01-20T18:47:15.663234Z"
    }
   },
   "outputs": [],
   "source": [
    "def f(x : torch.Tensor | Tensor):\n",
    "    return (x - 2) ** 2\n",
    "\n",
    "\n",
    "def f_nonconvexe(x : torch.Tensor | Tensor):\n",
    "    return 3*x ** 2 - 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a4424025c79c9f87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.714455Z",
     "start_time": "2025-01-20T18:47:15.709126Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_optim(x : torch.Tensor | Tensor, convexe : bool = True, scheduler : bool = False):\n",
    "    if convexe:\n",
    "        print(f\"Optimisation de la fonction convexe f(x) = (x - 2)²\")\n",
    "        y = f(x)\n",
    "    else:\n",
    "        print(f\"Optimisation de la fonction non convexe f(x) = 3x² - 2x\")\n",
    "        y = f_nonconvexe(x)\n",
    "\n",
    "    y.backward()\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        print(f\"Gradient de f en x={x.item()}: x.grad={x.grad.item()}\")\n",
    "    else:\n",
    "        print(f\"Gradient de f en x={x.data}: x.grad={x.grad}\")\n",
    "\n",
    "    resulting_x = []\n",
    "    resulting_fx = []\n",
    "\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        optimizers_torch = [\n",
    "            SGD_torch,\n",
    "            RMSProp_torch,\n",
    "            Adagrad_torch,\n",
    "            Adam_torch,\n",
    "            AdamW_torch\n",
    "        ]\n",
    "\n",
    "        for optimizer in optimizers_torch:\n",
    "            optimizer = optimizer([x])\n",
    "            if scheduler:\n",
    "                scheduler = LRSchedulerOnPlateauTorch(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "            for i in range(100):\n",
    "                optimizer.zero_grad()\n",
    "                if convexe:\n",
    "                    y = f(x)\n",
    "                else:\n",
    "                    y = f_nonconvexe(x)\n",
    "                y.backward()\n",
    "                optimizer.step()\n",
    "                if scheduler:\n",
    "                    scheduler.step(y)\n",
    "            print(f\"Optimiseur {optimizer.__class__.__name__}: x={x.item()}, f(x)={f(x).item()}\")\n",
    "            resulting_x.append(x.item())\n",
    "            resulting_fx.append(f(x).item())\n",
    "\n",
    "    else:\n",
    "        optimizers_custom = [\n",
    "            SGD_custom,\n",
    "            RMSProp_custom,\n",
    "            Adagrad_custom,\n",
    "            Adam_custom,\n",
    "            AdamW_custom\n",
    "        ]\n",
    "\n",
    "        for optimizer in optimizers_custom:\n",
    "            optimizer = optimizer([x])\n",
    "            if scheduler:\n",
    "                scheduler = LRSchedulerOnPlateauCustom(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "            for i in range(100):\n",
    "                optimizer.zero_grad()\n",
    "                if convexe:\n",
    "                    y = f(x)\n",
    "                else:\n",
    "                    y = f_nonconvexe(x)\n",
    "                y.backward()\n",
    "                optimizer.step()\n",
    "                if scheduler:\n",
    "                    scheduler.step(y)\n",
    "            print(f\"Optimiseur {optimizer.__class__.__name__}: x={x.data}, f(x)={f(x).data}\")\n",
    "            resulting_x.append(x.data)\n",
    "            resulting_fx.append(f(x).data)\n",
    "\n",
    "    return resulting_x, resulting_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "85f4258055c8e2fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.947978Z",
     "start_time": "2025-01-20T18:47:15.759840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la fonction convexe f(x) = (x - 2)²\n",
      "Gradient de f en x=1.0: x.grad=-2.0\n",
      "Optimiseur SGD_torch: x=1.8673806190490723, f(x)=0.017587900161743164\n",
      "Optimiseur RMSProp_torch: x=2.0249998569488525, f(x)=0.000624992826487869\n",
      "Optimiseur Adagrad_torch: x=2.0, f(x)=0.0\n",
      "Optimiseur Adam_torch: x=2.0, f(x)=0.0\n",
      "Optimiseur AdamW_torch: x=2.0017940998077393, f(x)=3.218794063286623e-06\n",
      "\n",
      "Optimisation de la fonction non convexe f(x) = 3x² - 2x\n",
      "Gradient de f en x=2.0017940998077393: x.grad=10.012619018554688\n",
      "Optimiseur SGD_torch: x=0.33676183223724365, f(x)=2.7663612365722656\n",
      "Optimiseur RMSProp_torch: x=0.3583333492279053, f(x)=2.6950693130493164\n",
      "Optimiseur Adagrad_torch: x=0.3333333432674408, f(x)=2.777777671813965\n",
      "Optimiseur Adam_torch: x=0.3333333432674408, f(x)=2.777777671813965\n",
      "Optimiseur AdamW_torch: x=0.3331911861896515, f(x)=2.7782516479492188\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.], requires_grad=True)\n",
    "conv_torch_x, conv_torch_fx = eval_optim(x, convexe=True)\n",
    "print()\n",
    "nonconv_torch_x, nonconv_torch_fx = eval_optim(x, convexe=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f74e4aa2f7e56bd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.972083Z",
     "start_time": "2025-01-20T18:47:15.957634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la fonction convexe f(x) = (x - 2)²\n",
      "Gradient de f en x=1.0: x.grad=-2.0\n",
      "Optimiseur SGD_custom: x=1.8673804441052466, f(x)=0.017587946605721615\n",
      "Optimiseur RMSProp_custom: x=2.025, f(x)=0.0006249999999999956\n",
      "Optimiseur Adagrad_custom: x=2.0, f(x)=0.0\n",
      "Optimiseur Adam_custom: x=2.0, f(x)=0.0\n",
      "Optimiseur AdamW_custom: x=2.001794181739699, f(x)=3.2190881150698366e-06\n",
      "\n",
      "Optimisation de la fonction non convexe f(x) = 3x² - 2x\n",
      "Gradient de f en x=2.001794181739699: x.grad=10.012619192458853\n",
      "Optimiseur SGD_custom: x=0.33676181143633, f(x)=2.7663612718965584\n",
      "Optimiseur RMSProp_custom: x=0.3583333333333334, f(x)=2.695069444444444\n",
      "Optimiseur Adagrad_custom: x=0.3333333333333333, f(x)=2.777777777777778\n",
      "Optimiseur Adam_custom: x=0.3333333333333333, f(x)=2.777777777777778\n",
      "Optimiseur AdamW_custom: x=0.3331912013267588, f(x)=2.7782515713345335\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(1.)\n",
    "conv_custom_x, conv_custom_fx = eval_optim(x, convexe=True)\n",
    "print()\n",
    "nonconv_custom_x, nonconv_custom_fx = eval_optim(x, convexe=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "15c9a8928c275420",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.004847Z",
     "start_time": "2025-01-20T18:47:16.002183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements between\n",
      "[1.8673806190490723, 2.0249998569488525, 2.0, 2.0, 2.0017940998077393]\n",
      "and\n",
      "[1.8673804441052466, 2.025, 2.0, 2.0, 2.001794181739699]\n",
      "are close within a tolerance of 0.0001\n",
      "\n",
      "\n",
      "\n",
      "All elements between\n",
      "[0.017587900161743164, 0.000624992826487869, 0.0, 0.0, 3.218794063286623e-06]\n",
      "and\n",
      "[0.017587946605721615, 0.0006249999999999956, 0.0, 0.0, 3.2190881150698366e-06]\n",
      "are close within a tolerance of 0.0001\n"
     ]
    }
   ],
   "source": [
    "check_diffs(conv_torch_x, conv_custom_x, tol=1e-4)\n",
    "print(\"\\n\\n\")\n",
    "check_diffs(conv_torch_fx, conv_custom_fx, tol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d95d3f325e5b75f",
   "metadata": {},
   "source": [
    "## ***Réseau de Neurones***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f1369e9cd3e0638b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.052384Z",
     "start_time": "2025-01-20T18:47:16.049858Z"
    }
   },
   "outputs": [],
   "source": [
    "def func_nn(x, W1, b1, W2, b2):\n",
    "    h1 = W1 * x + b1\n",
    "    y = W2 * h1 + b2\n",
    "    return y\n",
    "\n",
    "\n",
    "def mse(y, y_hat):\n",
    "    return (y - y_hat) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "da4600406c78abac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.100596Z",
     "start_time": "2025-01-20T18:47:16.095674Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_nn_optim(scheduler : bool = False, custom : bool = True):\n",
    "    results = []\n",
    "\n",
    "    if not custom:\n",
    "        optimizers = [\n",
    "            SGD_torch,\n",
    "            RMSProp_torch,\n",
    "            Adagrad_torch,\n",
    "            Adam_torch,\n",
    "            AdamW_torch\n",
    "        ]\n",
    "\n",
    "        for optimizer in optimizers:\n",
    "            W1 = torch.tensor([1.], requires_grad=True)\n",
    "            b1 = torch.tensor([1.], requires_grad=True)\n",
    "            W2 = torch.tensor([1.], requires_grad=True)\n",
    "            b2 = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "            x = torch.tensor([1.], requires_grad=True)\n",
    "            y = torch.tensor([10.])\n",
    "\n",
    "            optimizer = optimizer([W1, b1, W2, b2])\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler = LRSchedulerOnPlateauTorch(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "\n",
    "            for i in range(100):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_hat = func_nn(x, W1, b1, W2, b2)\n",
    "                loss = mse(y, y_hat)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step(loss)\n",
    "\n",
    "            print(f\"Optimiseur {optimizer.__class__.__name__}:\\nW1={W1.item()}, b1={b1.item()}, W2={W2.item()}, b2={b2.item()}\")\n",
    "            results.append([W1.item(), b1.item(), W2.item(), b2.item()])\n",
    "\n",
    "    else:\n",
    "        optimizers = [\n",
    "            SGD_custom,\n",
    "            RMSProp_custom,\n",
    "            Adagrad_custom,\n",
    "            Adam_custom,\n",
    "            AdamW_custom\n",
    "        ]\n",
    "\n",
    "        for optimizer in optimizers:\n",
    "            W1 = Tensor(1.)\n",
    "            b1 = Tensor(1.)\n",
    "            W2 = Tensor(1.)\n",
    "            b2 = Tensor(1.)\n",
    "\n",
    "            x = Tensor(1.)\n",
    "            y = Tensor(10.)\n",
    "\n",
    "            optimizer = optimizer([W1, b1, W2, b2])\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler = LRSchedulerOnPlateauCustom(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "\n",
    "            for i in range(100):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_hat = func_nn(x, W1, b1, W2, b2)\n",
    "                loss = mse(y, y_hat)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step(loss)\n",
    "\n",
    "            print(f\"Optimiseur {optimizer.__class__.__name__}:\\nW1={W1.data}, b1={b1.data}, W2={W2.data}, b2={b2.data}\")\n",
    "            results.append([W1.data, b1.data, W2.data, b2.data])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7e9a1e2f9cc247a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.286654Z",
     "start_time": "2025-01-20T18:47:16.147485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimiseur SGD_torch:\n",
      "W1=1.7965515851974487, b1=1.7965515851974487, W2=2.356534481048584, b2=1.532727837562561\n",
      "Optimiseur RMSProp_torch:\n",
      "W1=1.975722074508667, b1=1.975722074508667, W2=1.975722074508667, b2=1.9654841423034668\n",
      "Optimiseur Adagrad_torch:\n",
      "W1=2.0004096031188965, b1=2.0004096031188965, W2=2.0004096031188965, b2=1.99672532081604\n",
      "Optimiseur Adam_torch:\n",
      "W1=1.8886557817459106, b1=1.8886557817459106, W2=1.8886557817459106, b2=2.8375625610351562\n",
      "Optimiseur AdamW_torch:\n",
      "W1=1.8559832572937012, b1=1.8559832572937012, W2=1.8559832572937012, b2=3.1020750999450684\n"
     ]
    }
   ],
   "source": [
    "torch_nn = eval_nn_optim(scheduler=False, custom=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "408b64606de25ff0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.320711Z",
     "start_time": "2025-01-20T18:47:16.306700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimiseur SGD_custom:\n",
      "W1=1.7965517126874235, b1=1.7965517126874235, W2=2.3565344500454675, b2=1.532727995527799\n",
      "Optimiseur RMSProp_custom:\n",
      "W1=1.975721791587977, b1=1.975721791587977, W2=1.975721791587977, b2=1.965486512797642\n",
      "Optimiseur Adagrad_custom:\n",
      "W1=2.000409291478893, b1=2.000409291478893, W2=2.000409291478893, b2=1.996725333555097\n",
      "Optimiseur Adam_custom:\n",
      "W1=1.8886558814890224, b1=1.8886558814890224, W2=1.8886558820516521, b2=2.8375626842072093\n",
      "Optimiseur AdamW_custom:\n",
      "W1=1.8559833420114675, b1=1.8559833420114675, W2=1.855983342325199, b2=3.1020747059033416\n"
     ]
    }
   ],
   "source": [
    "custom_nn = eval_nn_optim(scheduler=False, custom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "777fc773f3bc260a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.370901Z",
     "start_time": "2025-01-20T18:47:16.368342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements between\n",
      "[[1.7965515851974487, 1.7965515851974487, 2.356534481048584, 1.532727837562561], [1.975722074508667, 1.975722074508667, 1.975722074508667, 1.9654841423034668], [2.0004096031188965, 2.0004096031188965, 2.0004096031188965, 1.99672532081604], [1.8886557817459106, 1.8886557817459106, 1.8886557817459106, 2.8375625610351562], [1.8559832572937012, 1.8559832572937012, 1.8559832572937012, 3.1020750999450684]]\n",
      "and\n",
      "[[1.7965517126874235, 1.7965517126874235, 2.3565344500454675, 1.532727995527799], [1.975721791587977, 1.975721791587977, 1.975721791587977, 1.965486512797642], [2.000409291478893, 2.000409291478893, 2.000409291478893, 1.996725333555097], [1.8886558814890224, 1.8886558814890224, 1.8886558820516521, 2.8375626842072093], [1.8559833420114675, 1.8559833420114675, 1.855983342325199, 3.1020747059033416]]\n",
      "are close within a tolerance of 0.0001\n"
     ]
    }
   ],
   "source": [
    "check_diffs(torch_nn, custom_nn, tol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3df25542164dbe",
   "metadata": {},
   "source": [
    "# ***Schedulers<a name=\"Schedulers\"></a>***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533f3bef6468a3a8",
   "metadata": {},
   "source": [
    "## **Implementation de LRScheduler**\n",
    "\n",
    "```python\n",
    "class LRSchedulerTorch\n",
    "```\n",
    "```python\n",
    "class LRSchedulerCustom\n",
    "```\n",
    "TODO: expliquer l'implémentation\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "class LRSchedulerOnPlateauTorch(LRSchedulerTorch)\n",
    "```\n",
    "```python\n",
    "class LRSchedulerOnPlateauCustom(LRSchedulerCustom)\n",
    "```\n",
    "TODO: expliquer l'implémentation\n",
    "\n",
    "Exemple d'utilisation :\n",
    "```python\n",
    "scheduler = LRSchedulerOnPlateauTorch(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "\n",
    "scheduler = LRSchedulerOnPlateauCustom(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1a5628f99bfe8a7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.420102Z",
     "start_time": "2025-01-20T18:47:16.417276Z"
    }
   },
   "outputs": [],
   "source": [
    "class LRSchedulerTorch:\n",
    "    def __init__(self, optimizer, initial_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    def set_lr(self, lr):\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group['lr'] = lr\n",
    "\n",
    "\n",
    "class LRSchedulerCustom:\n",
    "    def __init__(self, optimizer, initial_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.optimizer.learning_rate\n",
    "\n",
    "    def set_lr(self, lr):\n",
    "        self.optimizer.learning_rate = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b175f2dd4c96e",
   "metadata": {},
   "source": [
    "## **Implementation de LRSchedulerOnPlateau**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "720c56dbb5332b80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.467993Z",
     "start_time": "2025-01-20T18:47:16.462678Z"
    }
   },
   "outputs": [],
   "source": [
    "class LRSchedulerOnPlateauTorch(LRSchedulerTorch):\n",
    "    def __init__(self, optimizer, initial_lr, patience=10, factor=0.1, min_lr=1e-6, mode='min', threshold=1e-4):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.best_value = None\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, current_value):\n",
    "        if self.best_value is None:\n",
    "            self.best_value = current_value\n",
    "            return\n",
    "\n",
    "        if self.mode == 'min':\n",
    "            improvement = self.best_value - current_value\n",
    "        elif self.mode == 'max':\n",
    "            improvement = current_value - self.best_value\n",
    "        else:\n",
    "            raise ValueError(\"Mode must be either 'min' (minimize) or 'max' (maximize).\")\n",
    "\n",
    "        if isinstance(improvement, Tensor):\n",
    "            if improvement.data > self.threshold:\n",
    "                self.best_value = current_value\n",
    "                self.num_bad_epochs = 0\n",
    "            else:\n",
    "                self.num_bad_epochs += 1\n",
    "        else:\n",
    "            if improvement > self.threshold:\n",
    "                self.best_value = current_value\n",
    "                self.num_bad_epochs = 0\n",
    "            else:\n",
    "                self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            self.reduce_lr()\n",
    "\n",
    "    def reduce_lr(self):\n",
    "        current_lr = self.get_lr()\n",
    "        new_lr = max(current_lr * self.factor, self.min_lr)\n",
    "        if new_lr < current_lr:\n",
    "            print(f\"Reducing learning rate: {current_lr:.6f} -> {new_lr:.6f}\")\n",
    "            self.set_lr(new_lr)\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "\n",
    "class LRSchedulerOnPlateauCustom(LRSchedulerCustom):\n",
    "    def __init__(self, optimizer, initial_lr, patience=10, factor=0.1, min_lr=1e-6, mode='min', threshold=1e-4):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.best_value = None\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, current_value):\n",
    "        if self.best_value is None:\n",
    "            self.best_value = current_value\n",
    "            return\n",
    "\n",
    "        if self.mode == 'min':\n",
    "            improvement = self.best_value - current_value\n",
    "        elif self.mode == 'max':\n",
    "            improvement = current_value - self.best_value\n",
    "        else:\n",
    "            raise ValueError(\"Mode must be either 'min' (minimize) or 'max' (maximize).\")\n",
    "\n",
    "        if improvement.data > self.threshold:\n",
    "            self.best_value = current_value\n",
    "            self.num_bad_epochs = 0\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            self.reduce_lr()\n",
    "\n",
    "    def reduce_lr(self):\n",
    "        current_lr = self.get_lr()\n",
    "        new_lr = max(current_lr * self.factor, self.min_lr)\n",
    "        if new_lr < current_lr:\n",
    "            print(f\"Reducing learning rate: {current_lr:.6f} -> {new_lr:.6f}\")\n",
    "            self.set_lr(new_lr)\n",
    "        self.num_bad_epochs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0015f5a3eab0528",
   "metadata": {},
   "source": [
    "## **Test de LRSchedulerOnPlateau**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e38504ba155084ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.700275Z",
     "start_time": "2025-01-20T18:47:16.514220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la fonction convexe f(x) = (x - 2)²\n",
      "Gradient de f en x=69.0: x.grad=134.0\n",
      "Optimiseur SGD_torch: x=10.885512351989746, f(x)=78.95233154296875\n",
      "Optimiseur RMSProp_torch: x=5.890719413757324, f(x)=15.137697219848633\n",
      "Reducing learning rate: 0.900000 -> 0.450000\n",
      "Reducing learning rate: 0.450000 -> 0.225000\n",
      "Reducing learning rate: 0.225000 -> 0.112500\n",
      "Reducing learning rate: 0.112500 -> 0.056250\n",
      "Reducing learning rate: 0.056250 -> 0.028125\n",
      "Reducing learning rate: 0.028125 -> 0.014063\n",
      "Reducing learning rate: 0.014063 -> 0.007031\n",
      "Reducing learning rate: 0.007031 -> 0.003516\n",
      "Reducing learning rate: 0.003516 -> 0.001758\n",
      "Reducing learning rate: 0.001758 -> 0.000879\n",
      "Reducing learning rate: 0.000879 -> 0.000439\n",
      "Optimiseur Adagrad_torch: x=2.0020194053649902, f(x)=4.077997800777666e-06\n",
      "Reducing learning rate: 0.250000 -> 0.125000\n",
      "Reducing learning rate: 0.125000 -> 0.062500\n",
      "Reducing learning rate: 0.062500 -> 0.031250\n",
      "Reducing learning rate: 0.031250 -> 0.015625\n",
      "Reducing learning rate: 0.015625 -> 0.007812\n",
      "Reducing learning rate: 0.007812 -> 0.003906\n",
      "Reducing learning rate: 0.003906 -> 0.001953\n",
      "Reducing learning rate: 0.001953 -> 0.000977\n",
      "Reducing learning rate: 0.000977 -> 0.000488\n",
      "Reducing learning rate: 0.000488 -> 0.000244\n",
      "Reducing learning rate: 0.000244 -> 0.000122\n",
      "Reducing learning rate: 0.000122 -> 0.000061\n",
      "Reducing learning rate: 0.000061 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000015\n",
      "Reducing learning rate: 0.000015 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur Adam_torch: x=2.013941764831543, f(x)=0.00019437281298451126\n",
      "Reducing learning rate: 0.250000 -> 0.125000\n",
      "Reducing learning rate: 0.125000 -> 0.062500\n",
      "Reducing learning rate: 0.062500 -> 0.031250\n",
      "Reducing learning rate: 0.031250 -> 0.015625\n",
      "Reducing learning rate: 0.015625 -> 0.007812\n",
      "Reducing learning rate: 0.007812 -> 0.003906\n",
      "Reducing learning rate: 0.003906 -> 0.001953\n",
      "Reducing learning rate: 0.001953 -> 0.000977\n",
      "Reducing learning rate: 0.000977 -> 0.000488\n",
      "Reducing learning rate: 0.000488 -> 0.000244\n",
      "Reducing learning rate: 0.000244 -> 0.000122\n",
      "Reducing learning rate: 0.000122 -> 0.000061\n",
      "Reducing learning rate: 0.000061 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000015\n",
      "Reducing learning rate: 0.000015 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur AdamW_torch: x=2.010690689086914, f(x)=0.00011429082951508462\n",
      "\n",
      "Optimisation de la fonction non convexe f(x) = 3x² - 2x\n",
      "Gradient de f en x=2.010690689086914: x.grad=10.085525512695312\n",
      "Reducing learning rate: 0.010000 -> 0.005000\n",
      "Reducing learning rate: 0.005000 -> 0.002500\n",
      "Optimiseur SGD_torch: x=0.33755946159362793, f(x)=2.7637085914611816\n",
      "Reducing learning rate: 0.050000 -> 0.025000\n",
      "Reducing learning rate: 0.025000 -> 0.012500\n",
      "Reducing learning rate: 0.012500 -> 0.006250\n",
      "Reducing learning rate: 0.006250 -> 0.003125\n",
      "Reducing learning rate: 0.003125 -> 0.001563\n",
      "Reducing learning rate: 0.001563 -> 0.000781\n",
      "Reducing learning rate: 0.000781 -> 0.000391\n",
      "Reducing learning rate: 0.000391 -> 0.000195\n",
      "Reducing learning rate: 0.000195 -> 0.000098\n",
      "Reducing learning rate: 0.000098 -> 0.000049\n",
      "Reducing learning rate: 0.000049 -> 0.000024\n",
      "Reducing learning rate: 0.000024 -> 0.000012\n",
      "Reducing learning rate: 0.000012 -> 0.000006\n",
      "Reducing learning rate: 0.000006 -> 0.000003\n",
      "Reducing learning rate: 0.000003 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur RMSProp_torch: x=0.3333333432674408, f(x)=2.777777671813965\n",
      "Reducing learning rate: 0.900000 -> 0.450000\n",
      "Reducing learning rate: 0.450000 -> 0.225000\n",
      "Reducing learning rate: 0.225000 -> 0.112500\n",
      "Reducing learning rate: 0.112500 -> 0.056250\n",
      "Reducing learning rate: 0.056250 -> 0.028125\n",
      "Reducing learning rate: 0.028125 -> 0.014063\n",
      "Reducing learning rate: 0.014063 -> 0.007031\n",
      "Reducing learning rate: 0.007031 -> 0.003516\n",
      "Reducing learning rate: 0.003516 -> 0.001758\n",
      "Reducing learning rate: 0.001758 -> 0.000879\n",
      "Reducing learning rate: 0.000879 -> 0.000439\n",
      "Reducing learning rate: 0.000439 -> 0.000220\n",
      "Reducing learning rate: 0.000220 -> 0.000110\n",
      "Reducing learning rate: 0.000110 -> 0.000055\n",
      "Reducing learning rate: 0.000055 -> 0.000027\n",
      "Reducing learning rate: 0.000027 -> 0.000014\n",
      "Reducing learning rate: 0.000014 -> 0.000007\n",
      "Reducing learning rate: 0.000007 -> 0.000003\n",
      "Reducing learning rate: 0.000003 -> 0.000002\n",
      "Optimiseur Adagrad_torch: x=nan, f(x)=nan\n",
      "Reducing learning rate: 0.250000 -> 0.125000\n",
      "Reducing learning rate: 0.125000 -> 0.062500\n",
      "Reducing learning rate: 0.062500 -> 0.031250\n",
      "Reducing learning rate: 0.031250 -> 0.015625\n",
      "Reducing learning rate: 0.015625 -> 0.007812\n",
      "Reducing learning rate: 0.007812 -> 0.003906\n",
      "Reducing learning rate: 0.003906 -> 0.001953\n",
      "Reducing learning rate: 0.001953 -> 0.000977\n",
      "Reducing learning rate: 0.000977 -> 0.000488\n",
      "Reducing learning rate: 0.000488 -> 0.000244\n",
      "Reducing learning rate: 0.000244 -> 0.000122\n",
      "Reducing learning rate: 0.000122 -> 0.000061\n",
      "Reducing learning rate: 0.000061 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000015\n",
      "Reducing learning rate: 0.000015 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur Adam_torch: x=nan, f(x)=nan\n",
      "Reducing learning rate: 0.250000 -> 0.125000\n",
      "Reducing learning rate: 0.125000 -> 0.062500\n",
      "Reducing learning rate: 0.062500 -> 0.031250\n",
      "Reducing learning rate: 0.031250 -> 0.015625\n",
      "Reducing learning rate: 0.015625 -> 0.007812\n",
      "Reducing learning rate: 0.007812 -> 0.003906\n",
      "Reducing learning rate: 0.003906 -> 0.001953\n",
      "Reducing learning rate: 0.001953 -> 0.000977\n",
      "Reducing learning rate: 0.000977 -> 0.000488\n",
      "Reducing learning rate: 0.000488 -> 0.000244\n",
      "Reducing learning rate: 0.000244 -> 0.000122\n",
      "Reducing learning rate: 0.000122 -> 0.000061\n",
      "Reducing learning rate: 0.000061 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000015\n",
      "Reducing learning rate: 0.000015 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur AdamW_torch: x=nan, f(x)=nan\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([69.], requires_grad=True)\n",
    "conv_torch_scheduler_x, conv_torch_scheduler_fx = eval_optim(x, convexe=True, scheduler=True)\n",
    "print()\n",
    "nonconv_torch_scheduler_x, nonconv_torch_scheduler_fx = eval_optim(x, convexe=False, scheduler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e938b0f6a751f60b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.727075Z",
     "start_time": "2025-01-20T18:47:16.709881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la fonction convexe f(x) = (x - 2)²\n",
      "Gradient de f en x=69.0: x.grad=134.0\n",
      "Optimiseur SGD_custom: x=10.885510244948467, f(x)=78.95229231308416\n",
      "Optimiseur RMSProp_custom: x=5.890716009174875, f(x)=15.13767106404967\n",
      "Reducing learning rate: 0.900000 -> 0.450000\n",
      "Reducing learning rate: 0.450000 -> 0.225000\n",
      "Reducing learning rate: 0.225000 -> 0.112500\n",
      "Reducing learning rate: 0.112500 -> 0.056250\n",
      "Reducing learning rate: 0.056250 -> 0.028125\n",
      "Reducing learning rate: 0.028125 -> 0.014063\n",
      "Reducing learning rate: 0.014063 -> 0.007031\n",
      "Reducing learning rate: 0.007031 -> 0.003516\n",
      "Reducing learning rate: 0.003516 -> 0.001758\n",
      "Reducing learning rate: 0.001758 -> 0.000879\n",
      "Reducing learning rate: 0.000879 -> 0.000439\n",
      "Optimiseur Adagrad_custom: x=2.0020189655822986, f(x)=4.076222022506476e-06\n",
      "Reducing learning rate: 0.250000 -> 0.125000\n",
      "Reducing learning rate: 0.125000 -> 0.062500\n",
      "Reducing learning rate: 0.062500 -> 0.031250\n",
      "Reducing learning rate: 0.031250 -> 0.015625\n",
      "Reducing learning rate: 0.015625 -> 0.007812\n",
      "Reducing learning rate: 0.007812 -> 0.003906\n",
      "Reducing learning rate: 0.003906 -> 0.001953\n",
      "Reducing learning rate: 0.001953 -> 0.000977\n",
      "Reducing learning rate: 0.000977 -> 0.000488\n",
      "Reducing learning rate: 0.000488 -> 0.000244\n",
      "Reducing learning rate: 0.000244 -> 0.000122\n",
      "Reducing learning rate: 0.000122 -> 0.000061\n",
      "Reducing learning rate: 0.000061 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000015\n",
      "Reducing learning rate: 0.000015 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur Adam_custom: x=2.0139409069240797, f(x)=0.00019434888586585268\n",
      "Reducing learning rate: 0.250000 -> 0.125000\n",
      "Reducing learning rate: 0.125000 -> 0.062500\n",
      "Reducing learning rate: 0.062500 -> 0.031250\n",
      "Reducing learning rate: 0.031250 -> 0.015625\n",
      "Reducing learning rate: 0.015625 -> 0.007812\n",
      "Reducing learning rate: 0.007812 -> 0.003906\n",
      "Reducing learning rate: 0.003906 -> 0.001953\n",
      "Reducing learning rate: 0.001953 -> 0.000977\n",
      "Reducing learning rate: 0.000977 -> 0.000488\n",
      "Reducing learning rate: 0.000488 -> 0.000244\n",
      "Reducing learning rate: 0.000244 -> 0.000122\n",
      "Reducing learning rate: 0.000122 -> 0.000061\n",
      "Reducing learning rate: 0.000061 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000015\n",
      "Reducing learning rate: 0.000015 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur AdamW_custom: x=2.0106905128327615, f(x)=0.00011428706462743738\n",
      "\n",
      "Optimisation de la fonction non convexe f(x) = 3x² - 2x\n",
      "Gradient de f en x=2.0106905128327615: x.grad=10.085524573616548\n",
      "Reducing learning rate: 0.010000 -> 0.005000\n",
      "Reducing learning rate: 0.005000 -> 0.002500\n",
      "Optimiseur SGD_custom: x=0.3375594228510875, f(x)=2.7637086725512097\n",
      "Reducing learning rate: 0.050000 -> 0.025000\n",
      "Reducing learning rate: 0.025000 -> 0.012500\n",
      "Reducing learning rate: 0.012500 -> 0.006250\n",
      "Reducing learning rate: 0.006250 -> 0.003125\n",
      "Reducing learning rate: 0.003125 -> 0.001563\n",
      "Reducing learning rate: 0.001563 -> 0.000781\n",
      "Reducing learning rate: 0.000781 -> 0.000391\n",
      "Reducing learning rate: 0.000391 -> 0.000195\n",
      "Reducing learning rate: 0.000195 -> 0.000098\n",
      "Reducing learning rate: 0.000098 -> 0.000049\n",
      "Reducing learning rate: 0.000049 -> 0.000024\n",
      "Reducing learning rate: 0.000024 -> 0.000012\n",
      "Reducing learning rate: 0.000012 -> 0.000006\n",
      "Reducing learning rate: 0.000006 -> 0.000003\n",
      "Reducing learning rate: 0.000003 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur RMSProp_custom: x=0.333332833875097, f(x)=2.777779442638815\n",
      "Reducing learning rate: 0.900000 -> 0.450000\n",
      "Reducing learning rate: 0.450000 -> 0.225000\n",
      "Reducing learning rate: 0.225000 -> 0.112500\n",
      "Reducing learning rate: 0.112500 -> 0.056250\n",
      "Reducing learning rate: 0.056250 -> 0.028125\n",
      "Reducing learning rate: 0.028125 -> 0.014063\n",
      "Reducing learning rate: 0.014063 -> 0.007031\n",
      "Reducing learning rate: 0.007031 -> 0.003516\n",
      "Reducing learning rate: 0.003516 -> 0.001758\n",
      "Reducing learning rate: 0.001758 -> 0.000879\n",
      "Reducing learning rate: 0.000879 -> 0.000439\n",
      "Reducing learning rate: 0.000439 -> 0.000220\n",
      "Reducing learning rate: 0.000220 -> 0.000110\n",
      "Reducing learning rate: 0.000110 -> 0.000055\n",
      "Reducing learning rate: 0.000055 -> 0.000027\n",
      "Reducing learning rate: 0.000027 -> 0.000014\n",
      "Reducing learning rate: 0.000014 -> 0.000007\n",
      "Reducing learning rate: 0.000007 -> 0.000003\n",
      "Reducing learning rate: 0.000003 -> 0.000002\n",
      "Optimiseur Adagrad_custom: x=0.33333333333333337, f(x)=2.7777777777777772\n",
      "Reducing learning rate: 0.250000 -> 0.125000\n",
      "Reducing learning rate: 0.125000 -> 0.062500\n",
      "Reducing learning rate: 0.062500 -> 0.031250\n",
      "Reducing learning rate: 0.031250 -> 0.015625\n",
      "Reducing learning rate: 0.015625 -> 0.007812\n",
      "Reducing learning rate: 0.007812 -> 0.003906\n",
      "Reducing learning rate: 0.003906 -> 0.001953\n",
      "Reducing learning rate: 0.001953 -> 0.000977\n",
      "Reducing learning rate: 0.000977 -> 0.000488\n",
      "Reducing learning rate: 0.000488 -> 0.000244\n",
      "Reducing learning rate: 0.000244 -> 0.000122\n",
      "Reducing learning rate: 0.000122 -> 0.000061\n",
      "Reducing learning rate: 0.000061 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000015\n",
      "Reducing learning rate: 0.000015 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur Adam_custom: x=0.33333333333333337, f(x)=2.7777777777777772\n",
      "Reducing learning rate: 0.250000 -> 0.125000\n",
      "Reducing learning rate: 0.125000 -> 0.062500\n",
      "Reducing learning rate: 0.062500 -> 0.031250\n",
      "Reducing learning rate: 0.031250 -> 0.015625\n",
      "Reducing learning rate: 0.015625 -> 0.007812\n",
      "Reducing learning rate: 0.007812 -> 0.003906\n",
      "Reducing learning rate: 0.003906 -> 0.001953\n",
      "Reducing learning rate: 0.001953 -> 0.000977\n",
      "Reducing learning rate: 0.000977 -> 0.000488\n",
      "Reducing learning rate: 0.000488 -> 0.000244\n",
      "Reducing learning rate: 0.000244 -> 0.000122\n",
      "Reducing learning rate: 0.000122 -> 0.000061\n",
      "Reducing learning rate: 0.000061 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000015\n",
      "Reducing learning rate: 0.000015 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur AdamW_custom: x=0.3447519002184118, f(x)=2.7398462718305585\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(69.)\n",
    "conv_custom_scheduler_x, conv_custom_scheduler_fx = eval_optim(x, convexe=True, scheduler=True)\n",
    "print()\n",
    "nonconv_custom_scheduler_x, nonconv_custom_scheduler_fx = eval_optim(x, convexe=False, scheduler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "186f1b7002fb1028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.761052Z",
     "start_time": "2025-01-20T18:47:16.758656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements between\n",
      "[10.885512351989746, 5.890719413757324, 2.0020194053649902, 2.013941764831543, 2.010690689086914]\n",
      "and\n",
      "[10.885510244948467, 5.890716009174875, 2.0020189655822986, 2.0139409069240797, 2.0106905128327615]\n",
      "are close within a tolerance of 0.0001\n",
      "\n",
      "All elements between\n",
      "[78.95233154296875, 15.137697219848633, 4.077997800777666e-06, 0.00019437281298451126, 0.00011429082951508462]\n",
      "and\n",
      "[78.95229231308416, 15.13767106404967, 4.076222022506476e-06, 0.00019434888586585268, 0.00011428706462743738]\n",
      "are close within a tolerance of 0.0001\n"
     ]
    }
   ],
   "source": [
    "check_diffs(conv_torch_scheduler_x, conv_custom_scheduler_x, tol=1e-4)\n",
    "print()\n",
    "check_diffs(conv_torch_scheduler_fx, conv_custom_scheduler_fx, tol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "454c7f191b933243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.953805Z",
     "start_time": "2025-01-20T18:47:16.807772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing learning rate: 0.010000 -> 0.005000\n",
      "Reducing learning rate: 0.005000 -> 0.002500\n",
      "Reducing learning rate: 0.002500 -> 0.001250\n",
      "Reducing learning rate: 0.001250 -> 0.000625\n",
      "Reducing learning rate: 0.000625 -> 0.000313\n",
      "Reducing learning rate: 0.000313 -> 0.000156\n",
      "Reducing learning rate: 0.000156 -> 0.000078\n",
      "Reducing learning rate: 0.000078 -> 0.000039\n",
      "Reducing learning rate: 0.000039 -> 0.000020\n",
      "Reducing learning rate: 0.000020 -> 0.000010\n",
      "Reducing learning rate: 0.000010 -> 0.000005\n",
      "Reducing learning rate: 0.000005 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Reducing learning rate: 0.000001 -> 0.000001\n",
      "Optimiseur SGD_torch:\n",
      "W1=1.796550989151001, b1=1.796550989151001, W2=2.3565328121185303, b2=1.5327274799346924\n",
      "Reducing learning rate: 0.050000 -> 0.025000\n",
      "Reducing learning rate: 0.025000 -> 0.012500\n",
      "Reducing learning rate: 0.012500 -> 0.006250\n",
      "Reducing learning rate: 0.006250 -> 0.003125\n",
      "Reducing learning rate: 0.003125 -> 0.001563\n",
      "Reducing learning rate: 0.001563 -> 0.000781\n",
      "Reducing learning rate: 0.000781 -> 0.000391\n",
      "Reducing learning rate: 0.000391 -> 0.000195\n",
      "Reducing learning rate: 0.000195 -> 0.000098\n",
      "Reducing learning rate: 0.000098 -> 0.000049\n",
      "Reducing learning rate: 0.000049 -> 0.000024\n",
      "Reducing learning rate: 0.000024 -> 0.000012\n",
      "Reducing learning rate: 0.000012 -> 0.000006\n",
      "Reducing learning rate: 0.000006 -> 0.000003\n",
      "Reducing learning rate: 0.000003 -> 0.000002\n",
      "Optimiseur RMSProp_torch:\n",
      "W1=2.0043885707855225, b1=2.0043885707855225, W2=2.0043885707855225, b2=1.9648537635803223\n",
      "Reducing learning rate: 0.900000 -> 0.450000\n",
      "Reducing learning rate: 0.450000 -> 0.225000\n",
      "Reducing learning rate: 0.225000 -> 0.112500\n",
      "Reducing learning rate: 0.112500 -> 0.056250\n",
      "Reducing learning rate: 0.056250 -> 0.028125\n",
      "Reducing learning rate: 0.028125 -> 0.014063\n",
      "Reducing learning rate: 0.014063 -> 0.007031\n",
      "Reducing learning rate: 0.007031 -> 0.003516\n",
      "Reducing learning rate: 0.003516 -> 0.001758\n",
      "Reducing learning rate: 0.001758 -> 0.000879\n",
      "Reducing learning rate: 0.000879 -> 0.000439\n",
      "Reducing learning rate: 0.000439 -> 0.000220\n",
      "Reducing learning rate: 0.000220 -> 0.000110\n",
      "Reducing learning rate: 0.000110 -> 0.000055\n",
      "Optimiseur Adagrad_torch:\n",
      "W1=2.0004093647003174, b1=2.0004093647003174, W2=2.0004093647003174, b2=1.99672532081604\n",
      "Reducing learning rate: 0.250000 -> 0.125000\n",
      "Reducing learning rate: 0.125000 -> 0.062500\n",
      "Reducing learning rate: 0.062500 -> 0.031250\n",
      "Reducing learning rate: 0.031250 -> 0.015625\n",
      "Reducing learning rate: 0.015625 -> 0.007812\n",
      "Reducing learning rate: 0.007812 -> 0.003906\n",
      "Reducing learning rate: 0.003906 -> 0.001953\n",
      "Reducing learning rate: 0.001953 -> 0.000977\n",
      "Reducing learning rate: 0.000977 -> 0.000488\n",
      "Reducing learning rate: 0.000488 -> 0.000244\n",
      "Reducing learning rate: 0.000244 -> 0.000122\n",
      "Reducing learning rate: 0.000122 -> 0.000061\n",
      "Reducing learning rate: 0.000061 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000015\n",
      "Reducing learning rate: 0.000015 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur Adam_torch:\n",
      "W1=1.9199641942977905, b1=1.9199641942977905, W2=1.9199641942977905, b2=2.3495137691497803\n",
      "Reducing learning rate: 0.250000 -> 0.125000\n",
      "Reducing learning rate: 0.125000 -> 0.062500\n",
      "Reducing learning rate: 0.062500 -> 0.031250\n",
      "Reducing learning rate: 0.031250 -> 0.015625\n",
      "Reducing learning rate: 0.015625 -> 0.007812\n",
      "Reducing learning rate: 0.007812 -> 0.003906\n",
      "Reducing learning rate: 0.003906 -> 0.001953\n",
      "Reducing learning rate: 0.001953 -> 0.000977\n",
      "Reducing learning rate: 0.000977 -> 0.000488\n",
      "Reducing learning rate: 0.000488 -> 0.000244\n",
      "Reducing learning rate: 0.000244 -> 0.000122\n",
      "Reducing learning rate: 0.000122 -> 0.000061\n",
      "Reducing learning rate: 0.000061 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000015\n",
      "Reducing learning rate: 0.000015 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur AdamW_torch:\n",
      "W1=1.8980488777160645, b1=1.8980488777160645, W2=1.8980488777160645, b2=2.3067123889923096\n"
     ]
    }
   ],
   "source": [
    "torch_scheduler_nn = eval_nn_optim(scheduler=True, custom=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9d418cf21f19dc93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.990165Z",
     "start_time": "2025-01-20T18:47:16.974563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing learning rate: 0.010000 -> 0.005000\n",
      "Reducing learning rate: 0.005000 -> 0.002500\n",
      "Reducing learning rate: 0.002500 -> 0.001250\n",
      "Reducing learning rate: 0.001250 -> 0.000625\n",
      "Reducing learning rate: 0.000625 -> 0.000313\n",
      "Reducing learning rate: 0.000313 -> 0.000156\n",
      "Reducing learning rate: 0.000156 -> 0.000078\n",
      "Reducing learning rate: 0.000078 -> 0.000039\n",
      "Reducing learning rate: 0.000039 -> 0.000020\n",
      "Reducing learning rate: 0.000020 -> 0.000010\n",
      "Reducing learning rate: 0.000010 -> 0.000005\n",
      "Reducing learning rate: 0.000005 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Reducing learning rate: 0.000001 -> 0.000001\n",
      "Optimiseur SGD_custom:\n",
      "W1=1.7965510145299812, b1=1.7965510145299812, W2=2.3565333855349313, b2=1.5327276992600714\n",
      "Reducing learning rate: 0.050000 -> 0.025000\n",
      "Reducing learning rate: 0.025000 -> 0.012500\n",
      "Reducing learning rate: 0.012500 -> 0.006250\n",
      "Reducing learning rate: 0.006250 -> 0.003125\n",
      "Reducing learning rate: 0.003125 -> 0.001563\n",
      "Reducing learning rate: 0.001563 -> 0.000781\n",
      "Reducing learning rate: 0.000781 -> 0.000391\n",
      "Reducing learning rate: 0.000391 -> 0.000195\n",
      "Reducing learning rate: 0.000195 -> 0.000098\n",
      "Reducing learning rate: 0.000098 -> 0.000049\n",
      "Reducing learning rate: 0.000049 -> 0.000024\n",
      "Reducing learning rate: 0.000024 -> 0.000012\n",
      "Reducing learning rate: 0.000012 -> 0.000006\n",
      "Reducing learning rate: 0.000006 -> 0.000003\n",
      "Reducing learning rate: 0.000003 -> 0.000002\n",
      "Optimiseur RMSProp_custom:\n",
      "W1=2.004388154342763, b1=2.004388154342763, W2=2.004388154342763, b2=1.964856315463248\n",
      "Reducing learning rate: 0.900000 -> 0.450000\n",
      "Reducing learning rate: 0.450000 -> 0.225000\n",
      "Reducing learning rate: 0.225000 -> 0.112500\n",
      "Reducing learning rate: 0.112500 -> 0.056250\n",
      "Reducing learning rate: 0.056250 -> 0.028125\n",
      "Reducing learning rate: 0.028125 -> 0.014063\n",
      "Reducing learning rate: 0.014063 -> 0.007031\n",
      "Reducing learning rate: 0.007031 -> 0.003516\n",
      "Reducing learning rate: 0.003516 -> 0.001758\n",
      "Reducing learning rate: 0.001758 -> 0.000879\n",
      "Reducing learning rate: 0.000879 -> 0.000439\n",
      "Reducing learning rate: 0.000439 -> 0.000220\n",
      "Reducing learning rate: 0.000220 -> 0.000110\n",
      "Reducing learning rate: 0.000110 -> 0.000055\n",
      "Optimiseur Adagrad_custom:\n",
      "W1=2.000409302209841, b1=2.000409302209841, W2=2.000409302209841, b2=1.9967252473852117\n",
      "Reducing learning rate: 0.250000 -> 0.125000\n",
      "Reducing learning rate: 0.125000 -> 0.062500\n",
      "Reducing learning rate: 0.062500 -> 0.031250\n",
      "Reducing learning rate: 0.031250 -> 0.015625\n",
      "Reducing learning rate: 0.015625 -> 0.007812\n",
      "Reducing learning rate: 0.007812 -> 0.003906\n",
      "Reducing learning rate: 0.003906 -> 0.001953\n",
      "Reducing learning rate: 0.001953 -> 0.000977\n",
      "Reducing learning rate: 0.000977 -> 0.000488\n",
      "Reducing learning rate: 0.000488 -> 0.000244\n",
      "Reducing learning rate: 0.000244 -> 0.000122\n",
      "Reducing learning rate: 0.000122 -> 0.000061\n",
      "Reducing learning rate: 0.000061 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000015\n",
      "Reducing learning rate: 0.000015 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur Adam_custom:\n",
      "W1=1.9199642223144788, b1=1.9199642223144788, W2=1.9199642227924367, b2=2.3495129287059933\n",
      "Reducing learning rate: 0.250000 -> 0.125000\n",
      "Reducing learning rate: 0.125000 -> 0.062500\n",
      "Reducing learning rate: 0.062500 -> 0.031250\n",
      "Reducing learning rate: 0.031250 -> 0.015625\n",
      "Reducing learning rate: 0.015625 -> 0.007812\n",
      "Reducing learning rate: 0.007812 -> 0.003906\n",
      "Reducing learning rate: 0.003906 -> 0.001953\n",
      "Reducing learning rate: 0.001953 -> 0.000977\n",
      "Reducing learning rate: 0.000977 -> 0.000488\n",
      "Reducing learning rate: 0.000488 -> 0.000244\n",
      "Reducing learning rate: 0.000244 -> 0.000122\n",
      "Reducing learning rate: 0.000122 -> 0.000061\n",
      "Reducing learning rate: 0.000061 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000015\n",
      "Reducing learning rate: 0.000015 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur AdamW_custom:\n",
      "W1=1.8980487189606503, b1=1.8980487189606503, W2=1.8980487194008169, b2=2.306712029160739\n"
     ]
    }
   ],
   "source": [
    "custom_scheduler_nn = eval_nn_optim(scheduler=True, custom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f75f0ec950d9d1b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:17.091741Z",
     "start_time": "2025-01-20T18:47:17.089061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements between\n",
      "[[1.796550989151001, 1.796550989151001, 2.3565328121185303, 1.5327274799346924], [2.0043885707855225, 2.0043885707855225, 2.0043885707855225, 1.9648537635803223], [2.0004093647003174, 2.0004093647003174, 2.0004093647003174, 1.99672532081604], [1.9199641942977905, 1.9199641942977905, 1.9199641942977905, 2.3495137691497803], [1.8980488777160645, 1.8980488777160645, 1.8980488777160645, 2.3067123889923096]]\n",
      "and\n",
      "[[1.7965510145299812, 1.7965510145299812, 2.3565333855349313, 1.5327276992600714], [2.004388154342763, 2.004388154342763, 2.004388154342763, 1.964856315463248], [2.000409302209841, 2.000409302209841, 2.000409302209841, 1.9967252473852117], [1.9199642223144788, 1.9199642223144788, 1.9199642227924367, 2.3495129287059933], [1.8980487189606503, 1.8980487189606503, 1.8980487194008169, 2.306712029160739]]\n",
      "are close within a tolerance of 0.0001\n"
     ]
    }
   ],
   "source": [
    "check_diffs(torch_scheduler_nn, custom_scheduler_nn, tol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2da0192eb5184a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:17.193188Z",
     "start_time": "2025-01-20T18:47:17.191338Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
