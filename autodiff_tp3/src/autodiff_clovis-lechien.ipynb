{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ***TP3 - Clovis Lechien***\n",
    "\n",
    "1. [Utils](#Utils)\n",
    "2. [SGD](#SGD)\n",
    "3. [RMSProp](#RMSProp)\n",
    "4. [Adagrad](#Adagrad)\n",
    "5. [Adam](#Adam)\n",
    "6. [AdamW](#AdamW)\n",
    "7. [Evaluation des Optimiseurs](#evaluation-des-optimiseurs) FIXME\n",
    "8. [Réseau de Neurones](#réseau-de-neurones) FIXME\n",
    "9. [Scheduler de Taux d'Apprentissage](#schedulers) FIXME"
   ],
   "id": "e418516f7da7ad36"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:25.458910Z",
     "start_time": "2025-01-15T17:18:25.456642Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "execution_count": 194
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:25.473496Z",
     "start_time": "2025-01-15T17:18:25.470602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Génération du jeu de données linéaire\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "x_linear = np.linspace(-10, 10, n_samples)\n",
    "y_linear = 3 * x_linear + 5 + np.random.normal(0, 2, n_samples)\n",
    "\n",
    " # Génération du jeu de données non linéaire\n",
    "y_nonlinear = 0.5 * x_linear **2 - 4 * x_linear + np.random.normal(0 ,5 ,n_samples)"
   ],
   "id": "5b054a38d6955342",
   "outputs": [],
   "execution_count": 195
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Utils<a name=\"Utils\"></a>***",
   "id": "6ec84e42849b4a17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:25.521820Z",
     "start_time": "2025-01-15T17:18:25.519137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimizer_testing_loop(parameters : dict[str,]):\n",
    "    model = parameters['model']\n",
    "\n",
    "    criterion = parameters['criterion']\n",
    "    optimizer = parameters['optimizer']\n",
    "\n",
    "    x_tensor = parameters['x_tensor']\n",
    "    y_tensor = parameters['y_tensor']\n",
    "\n",
    "    epochs = parameters['epochs']\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x_tensor)\n",
    "        loss = criterion(predictions, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.data}\")"
   ],
   "id": "14a6a420243220be",
   "outputs": [],
   "execution_count": 196
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***SGD<a name=\"SGD\"></a>***",
   "id": "8dfdde3bc692cd33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de SGD**",
   "id": "5eb8e9d42bd537a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:25.572462Z",
     "start_time": "2025-01-15T17:18:25.569050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        hyperparams = {'lr': learning_rate}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                theta_t -= lr * theta_t.grad"
   ],
   "id": "9b1604245ddcf6ee",
   "outputs": [],
   "execution_count": 197
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de SGD**",
   "id": "b9e407ea443cb5d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:25.637008Z",
     "start_time": "2025-01-15T17:18:25.615657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': SGD(model.parameters(), learning_rate=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "6f319e016a6d00cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 16.806262969970703\n",
      "Epoch 20, Loss: 12.560613632202148\n",
      "Epoch 30, Loss: 9.726184844970703\n",
      "Epoch 40, Loss: 7.833895206451416\n",
      "Epoch 50, Loss: 6.570591449737549\n",
      "Epoch 60, Loss: 5.727196216583252\n",
      "Epoch 70, Loss: 5.164138317108154\n",
      "Epoch 80, Loss: 4.788238525390625\n",
      "Epoch 90, Loss: 4.537283420562744\n",
      "Epoch 100, Loss: 4.369743824005127\n",
      "weight: tensor([[2.9703]])\n",
      "bias: tensor([4.5511])\n"
     ]
    }
   ],
   "execution_count": 198
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:25.688446Z",
     "start_time": "2025-01-15T17:18:25.665598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': SGD(model.parameters(), learning_rate=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "fe10005fb82a84f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 484.959228515625\n",
      "Epoch 20, Loss: 415.849609375\n",
      "Epoch 30, Loss: 369.71160888671875\n",
      "Epoch 40, Loss: 338.9094543457031\n",
      "Epoch 50, Loss: 318.345703125\n",
      "Epoch 60, Loss: 304.6171875\n",
      "Epoch 70, Loss: 295.45196533203125\n",
      "Epoch 80, Loss: 289.3331298828125\n",
      "Epoch 90, Loss: 285.2481689453125\n",
      "Epoch 100, Loss: 282.5210266113281\n",
      "weight: tensor([[-4.1445]])\n",
      "bias: tensor([15.1198])\n"
     ]
    }
   ],
   "execution_count": 199
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***RMSProp<a name=\"RMSProp\"></a>***",
   "id": "152e498551a6e055"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de RMSProp**",
   "id": "777cec9838bb901a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:25.720274Z",
     "start_time": "2025-01-15T17:18:25.716508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RMSProp(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01, decay=0.9):\n",
    "        hyperparams = {'lr': learning_rate, 'decay': decay}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            decay = group['decay']\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'square_avg' not in state:\n",
    "                    state['square_avg'] = torch.zeros_like(theta_t)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                square_avg = decay * square_avg + (1 - decay) * (theta_t.grad ** 2)\n",
    "                state['square_avg'] = square_avg\n",
    "\n",
    "                theta_t -= lr * theta_t.grad / square_avg.sqrt()"
   ],
   "id": "aa87be320c6bac1c",
   "outputs": [],
   "execution_count": 200
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de RMSProp**",
   "id": "1eeb4b122e967c04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:25.791639Z",
     "start_time": "2025-01-15T17:18:25.766268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': RMSProp(model.parameters(), learning_rate=0.01, decay=0.8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "94571c79790e8b25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 310.5052795410156\n",
      "Epoch 20, Loss: 289.93426513671875\n",
      "Epoch 30, Loss: 270.5417175292969\n",
      "Epoch 40, Loss: 251.8887481689453\n",
      "Epoch 50, Loss: 233.93060302734375\n",
      "Epoch 60, Loss: 216.66220092773438\n",
      "Epoch 70, Loss: 200.08251953125\n",
      "Epoch 80, Loss: 184.19105529785156\n",
      "Epoch 90, Loss: 168.98728942871094\n",
      "Epoch 100, Loss: 154.4706268310547\n",
      "weight: tensor([[0.9513]])\n",
      "bias: tensor([1.8938])\n"
     ]
    }
   ],
   "execution_count": 201
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:25.847784Z",
     "start_time": "2025-01-15T17:18:25.823686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': RMSProp(model.parameters(), learning_rate=0.01, decay=0.8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "91785a0139d74176",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1295.18603515625\n",
      "Epoch 20, Loss: 1259.952392578125\n",
      "Epoch 30, Loss: 1226.224853515625\n",
      "Epoch 40, Loss: 1193.274169921875\n",
      "Epoch 50, Loss: 1161.0260009765625\n",
      "Epoch 60, Loss: 1129.4725341796875\n",
      "Epoch 70, Loss: 1098.6126708984375\n",
      "Epoch 80, Loss: 1068.4461669921875\n",
      "Epoch 90, Loss: 1038.9730224609375\n",
      "Epoch 100, Loss: 1010.192626953125\n",
      "weight: tensor([[-0.4023]])\n",
      "bias: tensor([1.4741])\n"
     ]
    }
   ],
   "execution_count": 202
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Adagrad<a name=\"Adagrad\"></a>***",
   "id": "f010754e9cd887d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de Adagrad**",
   "id": "4242814cd90f5d60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:25.873006Z",
     "start_time": "2025-01-15T17:18:25.869864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adagrad(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        hyperparams = {'lr': learning_rate}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'sum_squared_grads' not in state:\n",
    "                    state['sum_squared_grads'] = torch.zeros_like(theta_t)\n",
    "\n",
    "                sum_squared_grads = state['sum_squared_grads']\n",
    "                sum_squared_grads += theta_t.grad ** 2\n",
    "                state['sum_squared_grads'] = sum_squared_grads\n",
    "\n",
    "                adjusted_lr = lr / sum_squared_grads.sqrt()\n",
    "\n",
    "                theta_t -= adjusted_lr * theta_t.grad"
   ],
   "id": "1368be27bac6048a",
   "outputs": [],
   "execution_count": 203
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de Adagrad**",
   "id": "8a5105ec47c4d5e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:25.945256Z",
     "start_time": "2025-01-15T17:18:25.921623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adagrad(model.parameters(), learning_rate=0.5),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "c43614bd41ea6d64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 54.12459945678711\n",
      "Epoch 20, Loss: 14.844115257263184\n",
      "Epoch 30, Loss: 6.648614406585693\n",
      "Epoch 40, Loss: 4.724640846252441\n",
      "Epoch 50, Loss: 4.235114574432373\n",
      "Epoch 60, Loss: 4.098513603210449\n",
      "Epoch 70, Loss: 4.056282043457031\n",
      "Epoch 80, Loss: 4.0419111251831055\n",
      "Epoch 90, Loss: 4.036640644073486\n",
      "Epoch 100, Loss: 4.034607410430908\n",
      "weight: tensor([[2.9692]])\n",
      "bias: tensor([5.0849])\n"
     ]
    }
   ],
   "execution_count": 204
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:25.992737Z",
     "start_time": "2025-01-15T17:18:25.969158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adagrad(model.parameters(), learning_rate=0.5),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "ba6af84cb07e158b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 693.0726318359375\n",
      "Epoch 20, Loss: 540.7066650390625\n",
      "Epoch 30, Loss: 472.582275390625\n",
      "Epoch 40, Loss: 435.4715881347656\n",
      "Epoch 50, Loss: 412.2949523925781\n",
      "Epoch 60, Loss: 396.10198974609375\n",
      "Epoch 70, Loss: 383.7505187988281\n",
      "Epoch 80, Loss: 373.7216796875\n",
      "Epoch 90, Loss: 365.2349548339844\n",
      "Epoch 100, Loss: 357.8613586425781\n",
      "weight: tensor([[-4.0844]])\n",
      "bias: tensor([8.4687])\n"
     ]
    }
   ],
   "execution_count": 205
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Adam<a name=\"Adam\"></a>***",
   "id": "b13a43547f37f00b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de Adam**",
   "id": "b6d5b0e1de91cdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:26.023017Z",
     "start_time": "2025-01-15T17:18:26.019237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        hyperparams = {'lr': learning_rate, 'beta1': beta1, 'beta2': beta2, 'epsilon': epsilon}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'm' not in state: # Moment d'ordre 1\n",
    "                    state['m'] = torch.zeros_like(theta_t)\n",
    "                if 'v' not in state: # Moment d'ordre 2\n",
    "                    state['v'] = torch.zeros_like(theta_t)\n",
    "                if 't' not in state: # Temps\n",
    "                    state['t'] = 0\n",
    "\n",
    "                # Premier Moment\n",
    "                m = state['m']\n",
    "                m_t = beta1 * m + (1 - beta1) * theta_t.grad\n",
    "                state['m'] = m_t\n",
    "\n",
    "                # Second Moment\n",
    "                v = state['v']\n",
    "                v_t = beta2 * v + (1 - beta2) * theta_t.grad ** 2\n",
    "                state['v'] = v_t\n",
    "\n",
    "                # Temps\n",
    "                t = state['t'] + 1\n",
    "                state['t'] = t\n",
    "\n",
    "                # Correction des biais\n",
    "                m_hat = m_t / (1 - beta1 ** t)\n",
    "                v_hat = v_t / (1 - beta2 ** t)\n",
    "\n",
    "                theta_t -= lr * m_hat / (v_hat.sqrt() + epsilon)"
   ],
   "id": "b6573846cf44e8b3",
   "outputs": [],
   "execution_count": 206
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de Adam**",
   "id": "c210eec6a98cf32e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:26.098442Z",
     "start_time": "2025-01-15T17:18:26.069108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adam(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "521702270449bd96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 272.73614501953125\n",
      "Epoch 20, Loss: 122.67634582519531\n",
      "Epoch 30, Loss: 42.47823715209961\n",
      "Epoch 40, Loss: 11.343353271484375\n",
      "Epoch 50, Loss: 4.416177272796631\n",
      "Epoch 60, Loss: 4.3578972816467285\n",
      "Epoch 70, Loss: 4.522104740142822\n",
      "Epoch 80, Loss: 4.262188911437988\n",
      "Epoch 90, Loss: 4.072848320007324\n",
      "Epoch 100, Loss: 4.0351057052612305\n",
      "weight: tensor([[2.9678]])\n",
      "bias: tensor([5.1600])\n"
     ]
    }
   ],
   "execution_count": 207
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:26.145958Z",
     "start_time": "2025-01-15T17:18:26.116050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adam(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "ff677ad6cf158b95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1007.5213012695312\n",
      "Epoch 20, Loss: 769.0822143554688\n",
      "Epoch 30, Loss: 606.478271484375\n",
      "Epoch 40, Loss: 506.4487609863281\n",
      "Epoch 50, Loss: 449.4682312011719\n",
      "Epoch 60, Loss: 416.75750732421875\n",
      "Epoch 70, Loss: 395.20257568359375\n",
      "Epoch 80, Loss: 378.142822265625\n",
      "Epoch 90, Loss: 363.25933837890625\n",
      "Epoch 100, Loss: 350.08673095703125\n",
      "weight: tensor([[-4.1920]])\n",
      "bias: tensor([8.9434])\n"
     ]
    }
   ],
   "execution_count": 208
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***AdamW<a name=\"AdamW\"></a>***",
   "id": "40fbceea56e83cc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:26.167128Z",
     "start_time": "2025-01-15T17:18:26.162987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):\n",
    "        hyperparams = {'lr': learning_rate, 'beta1': beta1, 'beta2': beta2, 'epsilon': epsilon, 'weight_decay': weight_decay}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'm' not in state: # Moment d'ordre 1\n",
    "                    state['m'] = torch.zeros_like(theta_t)\n",
    "                if 'v' not in state: # Moment d'ordre 2\n",
    "                    state['v'] = torch.zeros_like(theta_t)\n",
    "                if 't' not in state: # Temps\n",
    "                    state['t'] = 0\n",
    "\n",
    "                # Premier Moment\n",
    "                m = state['m']\n",
    "                m_t = beta1 * m + (1 - beta1) * theta_t.grad\n",
    "                state['m'] = m_t\n",
    "\n",
    "                # Second Moment\n",
    "                v = state['v']\n",
    "                v_t = beta2 * v + (1 - beta2) * theta_t.grad ** 2\n",
    "                state['v'] = v_t\n",
    "\n",
    "                # Temps\n",
    "                t = state['t'] + 1\n",
    "                state['t'] = t\n",
    "\n",
    "                # Correction des biais\n",
    "                m_hat = m_t / (1 - beta1 ** t)\n",
    "                v_hat = v_t / (1 - beta2 ** t)\n",
    "\n",
    "                theta_t -= lr * m_hat / (v_hat.sqrt() + epsilon) - lr * weight_decay * theta_t"
   ],
   "id": "94d7052ef2a65b78",
   "outputs": [],
   "execution_count": 209
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ***Test de AdamW***",
   "id": "505fef5fa0888d50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:26.249299Z",
     "start_time": "2025-01-15T17:18:26.216829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': AdamW(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "8b4093aa3bfe36d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 258.0908203125\n",
      "Epoch 20, Loss: 112.39228820800781\n",
      "Epoch 30, Loss: 36.251949310302734\n",
      "Epoch 40, Loss: 8.770923614501953\n",
      "Epoch 50, Loss: 4.215054512023926\n",
      "Epoch 60, Loss: 4.99793815612793\n",
      "Epoch 70, Loss: 5.110691070556641\n",
      "Epoch 80, Loss: 4.594119071960449\n",
      "Epoch 90, Loss: 4.2212371826171875\n",
      "Epoch 100, Loss: 4.091061592102051\n",
      "weight: tensor([[3.0012]])\n",
      "bias: tensor([5.2599])\n"
     ]
    }
   ],
   "execution_count": 210
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:26.306226Z",
     "start_time": "2025-01-15T17:18:26.270669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': AdamW(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "f9a3cc8059159a66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 169.35293579101562\n",
      "Epoch 20, Loss: 61.04060363769531\n",
      "Epoch 30, Loss: 16.49265480041504\n",
      "Epoch 40, Loss: 7.252600193023682\n",
      "Epoch 50, Loss: 6.957948684692383\n",
      "Epoch 60, Loss: 5.84036922454834\n",
      "Epoch 70, Loss: 4.5421953201293945\n",
      "Epoch 80, Loss: 4.0788893699646\n",
      "Epoch 90, Loss: 4.040548801422119\n",
      "Epoch 100, Loss: 4.054375648498535\n",
      "weight: tensor([[2.9785]])\n",
      "bias: tensor([5.2621])\n"
     ]
    }
   ],
   "execution_count": 211
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Evaluation des Optimiseurs<a name=\"evaluation-des-optimiseurs\"></a>***",
   "id": "195056380131d973"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:26.325912Z",
     "start_time": "2025-01-15T17:18:26.323587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def f(x : torch.Tensor):\n",
    "    return (x - 2) ** 2\n",
    "\n",
    "\n",
    "def f_nonconvexe(x : torch.Tensor):\n",
    "    return 3*x ** 2 - 2*x"
   ],
   "id": "ee1532fe2701e9b7",
   "outputs": [],
   "execution_count": 212
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:26.372361Z",
     "start_time": "2025-01-15T17:18:26.368988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_optim(x : torch.Tensor, convexe : bool = True, scheduler : bool = False):\n",
    "    if convexe:\n",
    "        print(f\"Optimisation de la fonction convexe f(x) = (x - 2)²\")\n",
    "        y = f(x)\n",
    "    else:\n",
    "        print(f\"Optimisation de la fonction non convexe f(x) = 3x² - 2x\")\n",
    "        y = f_nonconvexe(x)\n",
    "\n",
    "    y.backward()\n",
    "    print(f\"Gradient de f en x={x.item()}: x.grad={x.grad.item()}\")\n",
    "\n",
    "    optimizers = [\n",
    "        SGD,\n",
    "        RMSProp,\n",
    "        Adagrad,\n",
    "        Adam,\n",
    "        AdamW\n",
    "    ]\n",
    "\n",
    "    for optimizer in optimizers:\n",
    "        x = torch.tensor([100.], requires_grad=True)\n",
    "        optimizer = optimizer([x])\n",
    "        if scheduler:\n",
    "            scheduler = LRSchedulerOnPlateau(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "        for i in range(100):\n",
    "            optimizer.zero_grad()\n",
    "            if convexe:\n",
    "                y = f(x)\n",
    "            else:\n",
    "                y = f_nonconvexe(x)\n",
    "            y.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step(y)\n",
    "        print(f\"Optimiseur {optimizer.__class__.__name__}: x={x.item()}, f(x)={f(x).item()}\")"
   ],
   "id": "a4424025c79c9f87",
   "outputs": [],
   "execution_count": 213
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:26.577109Z",
     "start_time": "2025-01-15T17:18:26.415318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([69.], requires_grad=True)\n",
    "eval_optim(x, convexe=True)\n",
    "print()\n",
    "eval_optim(x, convexe=False)"
   ],
   "id": "85f4258055c8e2fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la fonction convexe f(x) = (x - 2)²\n",
      "Gradient de f en x=69.0: x.grad=134.0\n",
      "Optimiseur SGD: x=14.996723175048828, f(x)=168.91481018066406\n",
      "Optimiseur RMSProp: x=98.90937805175781, f(x)=9391.427734375\n",
      "Optimiseur Adagrad: x=99.81420135498047, f(x)=9567.6181640625\n",
      "Optimiseur Adam: x=99.90005493164062, f(x)=9584.4208984375\n",
      "Optimiseur AdamW: x=100.0, f(x)=9604.0\n",
      "\n",
      "Optimisation de la fonction non convexe f(x) = 3x² - 2x\n",
      "Gradient de f en x=69.0: x.grad=546.0\n",
      "Optimiseur SGD: x=0.5381360054016113, f(x)=2.1370463371276855\n",
      "Optimiseur RMSProp: x=98.90937805175781, f(x)=9391.427734375\n",
      "Optimiseur Adagrad: x=99.81420135498047, f(x)=9567.6181640625\n",
      "Optimiseur Adam: x=99.90005493164062, f(x)=9584.4208984375\n",
      "Optimiseur AdamW: x=100.0, f(x)=9604.0\n"
     ]
    }
   ],
   "execution_count": 214
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Réseau de Neurones<a name=\"réseau-de-neurones\"></a>***",
   "id": "1d95d3f325e5b75f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:26.590613Z",
     "start_time": "2025-01-15T17:18:26.588789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def func_nn(x, W1, b1, W2, b2):\n",
    "    h1 = W1 * x + b1\n",
    "    y = W2 * h1 + b2\n",
    "    return y\n",
    "\n",
    "\n",
    "def mse(y, y_hat):\n",
    "    return (y - y_hat) ** 2"
   ],
   "id": "f1369e9cd3e0638b",
   "outputs": [],
   "execution_count": 215
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:26.636045Z",
     "start_time": "2025-01-15T17:18:26.632456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_nn_optim(scheduler : bool = False):\n",
    "\n",
    "    optimizers = [\n",
    "        SGD,\n",
    "        RMSProp,\n",
    "        Adagrad,\n",
    "        Adam,\n",
    "        AdamW\n",
    "    ]\n",
    "\n",
    "    for optimizer in optimizers:\n",
    "        W1 = torch.tensor([1.], requires_grad=True)\n",
    "        b1 = torch.tensor([1.], requires_grad=True)\n",
    "        W2 = torch.tensor([1.], requires_grad=True)\n",
    "        b2 = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "        x = torch.tensor([1.], requires_grad=True)\n",
    "        y = torch.tensor([10.])\n",
    "\n",
    "        optimizer = optimizer([W1, b1, W2, b2])\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler = LRSchedulerOnPlateau(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "\n",
    "        for i in range(100):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_hat = func_nn(x, W1, b1, W2, b2)\n",
    "            loss = mse(y, y_hat)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler.step(loss)\n",
    "\n",
    "        print(f\"Optimiseur {optimizer.__class__.__name__}:\\nW1={W1.item()}, b1={b1.item()}, W2={W2.item()}, b2={b2.item()}\")\n",
    "        print(f\"Optimisation du réseau de neurones:\\nW1={W1.item()}, b1={b1.item()}, W2={W2.item()}, b2={b2.item()}\\n\")"
   ],
   "id": "da4600406c78abac",
   "outputs": [],
   "execution_count": 216
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:26.809109Z",
     "start_time": "2025-01-15T17:18:26.678704Z"
    }
   },
   "cell_type": "code",
   "source": "eval_nn_optim()",
   "id": "7e9a1e2f9cc247a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimiseur SGD:\n",
      "W1=1.7965515851974487, b1=1.7965515851974487, W2=2.356534481048584, b2=1.532727837562561\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.7965515851974487, b1=1.7965515851974487, W2=2.356534481048584, b2=1.532727837562561\n",
      "\n",
      "Optimiseur RMSProp:\n",
      "W1=1.956400752067566, b1=1.956400752067566, W2=1.956400752067566, b2=1.899566411972046\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.956400752067566, b1=1.956400752067566, W2=1.956400752067566, b2=1.899566411972046\n",
      "\n",
      "Optimiseur Adagrad:\n",
      "W1=1.186563491821289, b1=1.186563491821289, W2=1.186563491821289, b2=1.1806892156600952\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.186563491821289, b1=1.186563491821289, W2=1.186563491821289, b2=1.1806892156600952\n",
      "\n",
      "Optimiseur Adam:\n",
      "W1=1.1003371477127075, b1=1.1003371477127075, W2=1.1003371477127075, b2=1.0987093448638916\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.1003371477127075, b1=1.1003371477127075, W2=1.1003371477127075, b2=1.0987093448638916\n",
      "\n",
      "Optimiseur AdamW:\n",
      "W1=1.1013890504837036, b1=1.1013890504837036, W2=1.1013890504837036, b2=1.0997445583343506\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.1013890504837036, b1=1.1013890504837036, W2=1.1013890504837036, b2=1.0997445583343506\n",
      "\n"
     ]
    }
   ],
   "execution_count": 217
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Scheduler de Taux d'apprentissage<a name=\"schedulers\"></a>***",
   "id": "5b3df25542164dbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de LRScheduler**",
   "id": "533f3bef6468a3a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:26.822393Z",
     "start_time": "2025-01-15T17:18:26.820369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LRScheduler:\n",
    "    def __init__(self, optimizer, initial_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    def set_lr(self, lr):\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group['lr'] = lr"
   ],
   "id": "1a5628f99bfe8a7f",
   "outputs": [],
   "execution_count": 218
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de LRSchedulerOnPlateau**",
   "id": "4c7b175f2dd4c96e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:26.860169Z",
     "start_time": "2025-01-15T17:18:26.855914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LRSchedulerOnPlateau(LRScheduler):\n",
    "    def __init__(self, optimizer, initial_lr, patience=10, factor=0.1, min_lr=1e-6, mode='min', threshold=1e-4):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.best_value = None\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, current_value):\n",
    "        if self.best_value is None:\n",
    "            self.best_value = current_value\n",
    "            return\n",
    "\n",
    "        if self.mode == 'min':\n",
    "            improvement = self.best_value - current_value\n",
    "        elif self.mode == 'max':\n",
    "            improvement = current_value - self.best_value\n",
    "        else:\n",
    "            raise ValueError(\"Mode must be either 'min' (minimize) or 'max' (maximize).\")\n",
    "\n",
    "        if improvement > self.threshold:\n",
    "            self.best_value = current_value\n",
    "            self.num_bad_epochs = 0\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            self.reduce_lr()\n",
    "\n",
    "    def reduce_lr(self):\n",
    "        current_lr = self.get_lr()\n",
    "        new_lr = max(current_lr * self.factor, self.min_lr)\n",
    "        if new_lr < current_lr:\n",
    "            print(f\"Reducing learning rate: {current_lr:.6f} -> {new_lr:.6f}\")\n",
    "            self.set_lr(new_lr)\n",
    "        self.num_bad_epochs = 0"
   ],
   "id": "720c56dbb5332b80",
   "outputs": [],
   "execution_count": 219
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de LRSchedulerOnPlateau**",
   "id": "a0015f5a3eab0528"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:27.093659Z",
     "start_time": "2025-01-15T17:18:26.906804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([69.], requires_grad=True)\n",
    "eval_optim(x, convexe=True, scheduler=True)\n",
    "print()\n",
    "eval_optim(x, convexe=False, scheduler=True)"
   ],
   "id": "e38504ba155084ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la fonction convexe f(x) = (x - 2)²\n",
      "Gradient de f en x=69.0: x.grad=134.0\n",
      "Optimiseur SGD: x=14.996723175048828, f(x)=168.91481018066406\n",
      "Optimiseur RMSProp: x=98.90937805175781, f(x)=9391.427734375\n",
      "Optimiseur Adagrad: x=99.81420135498047, f(x)=9567.6181640625\n",
      "Optimiseur Adam: x=99.90005493164062, f(x)=9584.4208984375\n",
      "Reducing learning rate: 0.001000 -> 0.000500\n",
      "Reducing learning rate: 0.000500 -> 0.000250\n",
      "Reducing learning rate: 0.000250 -> 0.000125\n",
      "Reducing learning rate: 0.000125 -> 0.000063\n",
      "Reducing learning rate: 0.000063 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000016\n",
      "Reducing learning rate: 0.000016 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur AdamW: x=100.0, f(x)=9604.0\n",
      "\n",
      "Optimisation de la fonction non convexe f(x) = 3x² - 2x\n",
      "Gradient de f en x=69.0: x.grad=546.0\n",
      "Optimiseur SGD: x=0.5381360054016113, f(x)=2.1370463371276855\n",
      "Optimiseur RMSProp: x=98.90937805175781, f(x)=9391.427734375\n",
      "Optimiseur Adagrad: x=99.81420135498047, f(x)=9567.6181640625\n",
      "Optimiseur Adam: x=99.90005493164062, f(x)=9584.4208984375\n",
      "Reducing learning rate: 0.001000 -> 0.000500\n",
      "Reducing learning rate: 0.000500 -> 0.000250\n",
      "Reducing learning rate: 0.000250 -> 0.000125\n",
      "Reducing learning rate: 0.000125 -> 0.000063\n",
      "Reducing learning rate: 0.000063 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000016\n",
      "Reducing learning rate: 0.000016 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur AdamW: x=100.0, f(x)=9604.0\n"
     ]
    }
   ],
   "execution_count": 220
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:18:27.243565Z",
     "start_time": "2025-01-15T17:18:27.105033Z"
    }
   },
   "cell_type": "code",
   "source": "eval_nn_optim(scheduler=True)",
   "id": "454c7f191b933243",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing learning rate: 0.010000 -> 0.005000\n",
      "Reducing learning rate: 0.005000 -> 0.002500\n",
      "Reducing learning rate: 0.002500 -> 0.001250\n",
      "Reducing learning rate: 0.001250 -> 0.000625\n",
      "Reducing learning rate: 0.000625 -> 0.000313\n",
      "Reducing learning rate: 0.000313 -> 0.000156\n",
      "Reducing learning rate: 0.000156 -> 0.000078\n",
      "Reducing learning rate: 0.000078 -> 0.000039\n",
      "Reducing learning rate: 0.000039 -> 0.000020\n",
      "Reducing learning rate: 0.000020 -> 0.000010\n",
      "Reducing learning rate: 0.000010 -> 0.000005\n",
      "Reducing learning rate: 0.000005 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Reducing learning rate: 0.000001 -> 0.000001\n",
      "Optimiseur SGD:\n",
      "W1=1.796550989151001, b1=1.796550989151001, W2=2.3565328121185303, b2=1.5327274799346924\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.796550989151001, b1=1.796550989151001, W2=2.3565328121185303, b2=1.5327274799346924\n",
      "\n",
      "Optimiseur RMSProp:\n",
      "W1=1.956400752067566, b1=1.956400752067566, W2=1.956400752067566, b2=1.899566411972046\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.956400752067566, b1=1.956400752067566, W2=1.956400752067566, b2=1.899566411972046\n",
      "\n",
      "Optimiseur Adagrad:\n",
      "W1=1.186563491821289, b1=1.186563491821289, W2=1.186563491821289, b2=1.1806892156600952\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.186563491821289, b1=1.186563491821289, W2=1.186563491821289, b2=1.1806892156600952\n",
      "\n",
      "Optimiseur Adam:\n",
      "W1=1.1003371477127075, b1=1.1003371477127075, W2=1.1003371477127075, b2=1.0987093448638916\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.1003371477127075, b1=1.1003371477127075, W2=1.1003371477127075, b2=1.0987093448638916\n",
      "\n",
      "Optimiseur AdamW:\n",
      "W1=1.1013890504837036, b1=1.1013890504837036, W2=1.1013890504837036, b2=1.0997445583343506\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.1013890504837036, b1=1.1013890504837036, W2=1.1013890504837036, b2=1.0997445583343506\n",
      "\n"
     ]
    }
   ],
   "execution_count": 221
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
