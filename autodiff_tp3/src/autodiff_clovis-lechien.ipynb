{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***TP3 - Clovis Lechien***",
   "id": "e418516f7da7ad36"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:04.465035Z",
     "start_time": "2025-01-15T14:56:04.462963Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer"
   ],
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:04.516397Z",
     "start_time": "2025-01-15T14:56:04.513559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Génération du jeu de données linéaire\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "x_linear = np.linspace(-10, 10, n_samples)\n",
    "y_linear = 3 * x_linear + 5 + np.random.normal(0, 2, n_samples)\n",
    "\n",
    " # Génération du jeu de données non linéaire\n",
    "y_nonlinear = 0.5 * x_linear **2 - 4 * x_linear + np.random.normal(0 ,5 ,n_samples)"
   ],
   "id": "5b054a38d6955342",
   "outputs": [],
   "execution_count": 157
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Utils***",
   "id": "6ec84e42849b4a17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:04.576759Z",
     "start_time": "2025-01-15T14:56:04.574305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimizer_testing_loop(parameters : dict[str,]):\n",
    "    model = parameters['model']\n",
    "\n",
    "    criterion = parameters['criterion']\n",
    "    optimizer = parameters['optimizer']\n",
    "\n",
    "    x_tensor = parameters['x_tensor']\n",
    "    y_tensor = parameters['y_tensor']\n",
    "\n",
    "    epochs = parameters['epochs']\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x_tensor)\n",
    "        loss = criterion(predictions, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.data}\")"
   ],
   "id": "14a6a420243220be",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***SGD***",
   "id": "8dfdde3bc692cd33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de SGD**",
   "id": "5eb8e9d42bd537a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:04.622595Z",
     "start_time": "2025-01-15T14:56:04.620100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        hyperparams = {'lr': learning_rate}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                theta_t -= lr * theta_t.grad"
   ],
   "id": "9b1604245ddcf6ee",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de SGD**",
   "id": "b9e407ea443cb5d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:04.687493Z",
     "start_time": "2025-01-15T14:56:04.666732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': SGD(model.parameters(), learning_rate=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "6f319e016a6d00cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 25.781906127929688\n",
      "Epoch 20, Loss: 18.552831649780273\n",
      "Epoch 30, Loss: 13.726634979248047\n",
      "Epoch 40, Loss: 10.504631042480469\n",
      "Epoch 50, Loss: 8.353592872619629\n",
      "Epoch 60, Loss: 6.917543888092041\n",
      "Epoch 70, Loss: 5.958826065063477\n",
      "Epoch 80, Loss: 5.318777561187744\n",
      "Epoch 90, Loss: 4.891477584838867\n",
      "Epoch 100, Loss: 4.606206893920898\n",
      "weight: tensor([[2.9703]])\n",
      "bias: tensor([4.3778])\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:04.734755Z",
     "start_time": "2025-01-15T14:56:04.713512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': SGD(model.parameters(), learning_rate=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "fe10005fb82a84f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 499.5698547363281\n",
      "Epoch 20, Loss: 425.6038818359375\n",
      "Epoch 30, Loss: 376.2235412597656\n",
      "Epoch 40, Loss: 343.2569580078125\n",
      "Epoch 50, Loss: 321.2481384277344\n",
      "Epoch 60, Loss: 306.55487060546875\n",
      "Epoch 70, Loss: 296.7455139160156\n",
      "Epoch 80, Loss: 290.19677734375\n",
      "Epoch 90, Loss: 285.8247375488281\n",
      "Epoch 100, Loss: 282.90594482421875\n",
      "weight: tensor([[-4.1445]])\n",
      "bias: tensor([15.0406])\n"
     ]
    }
   ],
   "execution_count": 161
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***RMSProp***",
   "id": "152e498551a6e055"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de RMSProp**",
   "id": "777cec9838bb901a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:04.764001Z",
     "start_time": "2025-01-15T14:56:04.760860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RMSProp(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01, decay=0.9):\n",
    "        hyperparams = {'lr': learning_rate, 'decay': decay}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            decay = group['decay']\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'square_avg' not in state:\n",
    "                    state['square_avg'] = torch.zeros_like(theta_t)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                square_avg = decay * square_avg + (1 - decay) * (theta_t.grad ** 2)\n",
    "                state['square_avg'] = square_avg\n",
    "\n",
    "                theta_t -= lr * theta_t.grad / square_avg.sqrt()"
   ],
   "id": "aa87be320c6bac1c",
   "outputs": [],
   "execution_count": 162
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de RMSProp**",
   "id": "1eeb4b122e967c04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:04.838685Z",
     "start_time": "2025-01-15T14:56:04.810344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': RMSProp(model.parameters(), learning_rate=0.01, decay=0.8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "94571c79790e8b25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 303.2937316894531\n",
      "Epoch 20, Loss: 283.24786376953125\n",
      "Epoch 30, Loss: 264.368408203125\n",
      "Epoch 40, Loss: 246.2269287109375\n",
      "Epoch 50, Loss: 228.77969360351562\n",
      "Epoch 60, Loss: 212.02163696289062\n",
      "Epoch 70, Loss: 195.95184326171875\n",
      "Epoch 80, Loss: 180.56961059570312\n",
      "Epoch 90, Loss: 165.87445068359375\n",
      "Epoch 100, Loss: 151.8656768798828\n",
      "weight: tensor([[1.0768]])\n",
      "bias: tensor([0.1647])\n"
     ]
    }
   ],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:04.883061Z",
     "start_time": "2025-01-15T14:56:04.856939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': RMSProp(model.parameters(), learning_rate=0.01, decay=0.8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "91785a0139d74176",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1004.470458984375\n",
      "Epoch 20, Loss: 976.4335327148438\n",
      "Epoch 30, Loss: 949.7418212890625\n",
      "Epoch 40, Loss: 923.80908203125\n",
      "Epoch 50, Loss: 898.5750732421875\n",
      "Epoch 60, Loss: 874.0335693359375\n",
      "Epoch 70, Loss: 850.1834106445312\n",
      "Epoch 80, Loss: 827.0245361328125\n",
      "Epoch 90, Loss: 804.5563354492188\n",
      "Epoch 100, Loss: 782.7786865234375\n",
      "weight: tensor([[-1.4310]])\n",
      "bias: tensor([1.5010])\n"
     ]
    }
   ],
   "execution_count": 164
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Adagrad***",
   "id": "f010754e9cd887d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de Adagrad**",
   "id": "4242814cd90f5d60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:04.906604Z",
     "start_time": "2025-01-15T14:56:04.903201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adagrad(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        hyperparams = {'lr': learning_rate}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'sum_squared_grads' not in state:\n",
    "                    state['sum_squared_grads'] = torch.zeros_like(theta_t)\n",
    "\n",
    "                sum_squared_grads = state['sum_squared_grads']\n",
    "                sum_squared_grads += theta_t.grad ** 2\n",
    "                state['sum_squared_grads'] = sum_squared_grads\n",
    "\n",
    "                adjusted_lr = lr / sum_squared_grads.sqrt()\n",
    "\n",
    "                theta_t -= adjusted_lr * theta_t.grad"
   ],
   "id": "1368be27bac6048a",
   "outputs": [],
   "execution_count": 165
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de Adagrad**",
   "id": "8a5105ec47c4d5e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:04.982111Z",
     "start_time": "2025-01-15T14:56:04.953859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adagrad(model.parameters(), learning_rate=0.5),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "c43614bd41ea6d64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 19.23186492919922\n",
      "Epoch 20, Loss: 6.453787803649902\n",
      "Epoch 30, Loss: 4.7115607261657715\n",
      "Epoch 40, Loss: 4.281811714172363\n",
      "Epoch 50, Loss: 4.131573677062988\n",
      "Epoch 60, Loss: 4.072866916656494\n",
      "Epoch 70, Loss: 4.049282550811768\n",
      "Epoch 80, Loss: 4.0397443771362305\n",
      "Epoch 90, Loss: 4.035879135131836\n",
      "Epoch 100, Loss: 4.0343122482299805\n",
      "weight: tensor([[2.9703]])\n",
      "bias: tensor([5.0884])\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:05.030755Z",
     "start_time": "2025-01-15T14:56:05.003578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adagrad(model.parameters(), learning_rate=0.5),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "ba6af84cb07e158b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 662.9287719726562\n",
      "Epoch 20, Loss: 539.48095703125\n",
      "Epoch 30, Loss: 485.64019775390625\n",
      "Epoch 40, Loss: 455.394775390625\n",
      "Epoch 50, Loss: 435.1168212890625\n",
      "Epoch 60, Loss: 419.7544860839844\n",
      "Epoch 70, Loss: 407.2055969238281\n",
      "Epoch 80, Loss: 396.5013732910156\n",
      "Epoch 90, Loss: 387.1409912109375\n",
      "Epoch 100, Loss: 378.831787109375\n",
      "weight: tensor([[-4.1229]])\n",
      "bias: tensor([7.3640])\n"
     ]
    }
   ],
   "execution_count": 167
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Adam***",
   "id": "b13a43547f37f00b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de Adam**",
   "id": "b6d5b0e1de91cdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:05.054082Z",
     "start_time": "2025-01-15T14:56:05.050063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        hyperparams = {'lr': learning_rate, 'beta1': beta1, 'beta2': beta2, 'epsilon': epsilon}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'm' not in state: # Moment d'ordre 1\n",
    "                    state['m'] = torch.zeros_like(theta_t)\n",
    "                if 'v' not in state: # Moment d'ordre 2\n",
    "                    state['v'] = torch.zeros_like(theta_t)\n",
    "                if 't' not in state: # Temps\n",
    "                    state['t'] = 0\n",
    "\n",
    "                # Premier Moment\n",
    "                m = state['m']\n",
    "                m_t = beta1 * m + (1 - beta1) * theta_t.grad\n",
    "                state['m'] = m_t\n",
    "\n",
    "                # Second Moment\n",
    "                v = state['v']\n",
    "                v_t = beta2 * v + (1 - beta2) * theta_t.grad ** 2\n",
    "                state['v'] = v_t\n",
    "\n",
    "                # Temps\n",
    "                t = state['t'] + 1\n",
    "                state['t'] = t\n",
    "\n",
    "                # Correction des biais\n",
    "                m_hat = m_t / (1 - beta1 ** t)\n",
    "                v_hat = v_t / (1 - beta2 ** t)\n",
    "\n",
    "                theta_t -= lr * m_hat / (v_hat.sqrt() + epsilon)"
   ],
   "id": "b6573846cf44e8b3",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de Adam**",
   "id": "c210eec6a98cf32e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:05.128554Z",
     "start_time": "2025-01-15T14:56:05.100048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adam(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "521702270449bd96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 152.90664672851562\n",
      "Epoch 20, Loss: 51.90208435058594\n",
      "Epoch 30, Loss: 12.439159393310547\n",
      "Epoch 40, Loss: 5.2970733642578125\n",
      "Epoch 50, Loss: 5.51474142074585\n",
      "Epoch 60, Loss: 4.866863250732422\n",
      "Epoch 70, Loss: 4.160236358642578\n",
      "Epoch 80, Loss: 4.036962985992432\n",
      "Epoch 90, Loss: 4.063048839569092\n",
      "Epoch 100, Loss: 4.046626091003418\n",
      "weight: tensor([[2.9538]])\n",
      "bias: tensor([5.1655])\n"
     ]
    }
   ],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:05.178827Z",
     "start_time": "2025-01-15T14:56:05.146970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adam(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "ff677ad6cf158b95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1092.7698974609375\n",
      "Epoch 20, Loss: 835.7798461914062\n",
      "Epoch 30, Loss: 655.69189453125\n",
      "Epoch 40, Loss: 540.6663208007812\n",
      "Epoch 50, Loss: 472.496337890625\n",
      "Epoch 60, Loss: 432.7613525390625\n",
      "Epoch 70, Loss: 407.51483154296875\n",
      "Epoch 80, Loss: 388.6913146972656\n",
      "Epoch 90, Loss: 372.80157470703125\n",
      "Epoch 100, Loss: 358.7831726074219\n",
      "weight: tensor([[-4.1903]])\n",
      "bias: tensor([8.4491])\n"
     ]
    }
   ],
   "execution_count": 170
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***AdamW***",
   "id": "40fbceea56e83cc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:05.201114Z",
     "start_time": "2025-01-15T14:56:05.196994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):\n",
    "        hyperparams = {'lr': learning_rate, 'beta1': beta1, 'beta2': beta2, 'epsilon': epsilon, 'weight_decay': weight_decay}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'm' not in state: # Moment d'ordre 1\n",
    "                    state['m'] = torch.zeros_like(theta_t)\n",
    "                if 'v' not in state: # Moment d'ordre 2\n",
    "                    state['v'] = torch.zeros_like(theta_t)\n",
    "                if 't' not in state: # Temps\n",
    "                    state['t'] = 0\n",
    "\n",
    "                # Premier Moment\n",
    "                m = state['m']\n",
    "                m_t = beta1 * m + (1 - beta1) * theta_t.grad\n",
    "                state['m'] = m_t\n",
    "\n",
    "                # Second Moment\n",
    "                v = state['v']\n",
    "                v_t = beta2 * v + (1 - beta2) * theta_t.grad ** 2\n",
    "                state['v'] = v_t\n",
    "\n",
    "                # Temps\n",
    "                t = state['t'] + 1\n",
    "                state['t'] = t\n",
    "\n",
    "                # Correction des biais\n",
    "                m_hat = m_t / (1 - beta1 ** t)\n",
    "                v_hat = v_t / (1 - beta2 ** t)\n",
    "\n",
    "                theta_t -= lr * m_hat / (v_hat.sqrt() + epsilon) - lr * weight_decay * theta_t"
   ],
   "id": "94d7052ef2a65b78",
   "outputs": [],
   "execution_count": 171
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ***Test de AdamW***",
   "id": "505fef5fa0888d50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:05.279617Z",
     "start_time": "2025-01-15T14:56:05.246009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': AdamW(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "8b4093aa3bfe36d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 70.88858032226562\n",
      "Epoch 20, Loss: 17.531997680664062\n",
      "Epoch 30, Loss: 9.452136039733887\n",
      "Epoch 40, Loss: 8.697772979736328\n",
      "Epoch 50, Loss: 5.476622104644775\n",
      "Epoch 60, Loss: 4.144920825958252\n",
      "Epoch 70, Loss: 4.08082389831543\n",
      "Epoch 80, Loss: 4.044642448425293\n",
      "Epoch 90, Loss: 4.080211162567139\n",
      "Epoch 100, Loss: 4.091483116149902\n",
      "weight: tensor([[3.0000]])\n",
      "bias: tensor([5.2814])\n"
     ]
    }
   ],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:05.328952Z",
     "start_time": "2025-01-15T14:56:05.296685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': AdamW(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "f9a3cc8059159a66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 292.70428466796875\n",
      "Epoch 20, Loss: 135.1478271484375\n",
      "Epoch 30, Loss: 48.26994323730469\n",
      "Epoch 40, Loss: 13.050810813903809\n",
      "Epoch 50, Loss: 4.737183570861816\n",
      "Epoch 60, Loss: 4.707895278930664\n",
      "Epoch 70, Loss: 5.05279541015625\n",
      "Epoch 80, Loss: 4.710052967071533\n",
      "Epoch 90, Loss: 4.320013046264648\n",
      "Epoch 100, Loss: 4.137866973876953\n",
      "weight: tensor([[3.0154]])\n",
      "bias: tensor([5.2814])\n"
     ]
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Evaluation des Optimiseurs***",
   "id": "195056380131d973"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:05.349510Z",
     "start_time": "2025-01-15T14:56:05.347547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def f(x):\n",
    "    return (x - 2) ** 2\n",
    "\n",
    "\n",
    "def f_nonconvexe(x):\n",
    "    return 3*x ** 2 - 2*x"
   ],
   "id": "ee1532fe2701e9b7",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:05.396065Z",
     "start_time": "2025-01-15T14:56:05.393146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_optim(x : torch.Tensor, convexe : bool = True):\n",
    "    if convexe:\n",
    "        print(f\"Optimisation de la fonction convexe f(x) = (x - 2)²\")\n",
    "        y = f(x)\n",
    "    else:\n",
    "        print(f\"Optimisation de la fonction non convexe f(x) = 3x² - 2x\")\n",
    "        y = f_nonconvexe(x)\n",
    "\n",
    "    y.backward()\n",
    "    print(f\"Gradient de f en x={x.item()}: x.grad={x.grad.item()}\")\n",
    "\n",
    "    optimizers = [\n",
    "        SGD,\n",
    "        RMSProp,\n",
    "        Adagrad,\n",
    "        Adam,\n",
    "        AdamW\n",
    "    ]\n",
    "\n",
    "    for optimizer in optimizers:\n",
    "        x = torch.tensor([100.], requires_grad=True)\n",
    "        optimizer = optimizer([x])\n",
    "        for i in range(100):\n",
    "            optimizer.zero_grad()\n",
    "            if convexe:\n",
    "                y = f(x)\n",
    "            else:\n",
    "                y = f_nonconvexe(x)\n",
    "            y.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Optimiseur {optimizer.__class__.__name__}: x={x.item()}, f(x)={f(x).item()}\")"
   ],
   "id": "a4424025c79c9f87",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:05.612145Z",
     "start_time": "2025-01-15T14:56:05.442204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([69.], requires_grad=True)\n",
    "eval_optim(x, convexe=True)\n",
    "print()\n",
    "eval_optim(x, convexe=False)"
   ],
   "id": "85f4258055c8e2fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la fonction convexe f(x) = (x - 2)²\n",
      "Gradient de f en x=69.0: x.grad=134.0\n",
      "Optimiseur SGD: x=14.996723175048828, f(x)=168.91481018066406\n",
      "Optimiseur RMSProp: x=98.90937805175781, f(x)=9391.427734375\n",
      "Optimiseur Adagrad: x=99.81420135498047, f(x)=9567.6181640625\n",
      "Optimiseur Adam: x=99.90005493164062, f(x)=9584.4208984375\n",
      "Optimiseur AdamW: x=100.0, f(x)=9604.0\n",
      "\n",
      "Optimisation de la fonction non convexe f(x) = 3x² - 2x\n",
      "Gradient de f en x=69.0: x.grad=546.0\n",
      "Optimiseur SGD: x=0.5381360054016113, f(x)=2.1370463371276855\n",
      "Optimiseur RMSProp: x=98.90937805175781, f(x)=9391.427734375\n",
      "Optimiseur Adagrad: x=99.81420135498047, f(x)=9567.6181640625\n",
      "Optimiseur Adam: x=99.90005493164062, f(x)=9584.4208984375\n",
      "Optimiseur AdamW: x=100.0, f(x)=9604.0\n"
     ]
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Réseau de Neurones***",
   "id": "1d95d3f325e5b75f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:05.625154Z",
     "start_time": "2025-01-15T14:56:05.623198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def func_nn(x, W1, b1, W2, b2):\n",
    "    h1 = W1 * x + b1\n",
    "    y = W2 * h1 + b2\n",
    "    return y\n",
    "\n",
    "\n",
    "def mse(y, y_hat):\n",
    "    return (y - y_hat) ** 2"
   ],
   "id": "f1369e9cd3e0638b",
   "outputs": [],
   "execution_count": 177
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:05.670520Z",
     "start_time": "2025-01-15T14:56:05.667466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_nn_optim():\n",
    "\n",
    "    optimizers = [\n",
    "        SGD,\n",
    "        RMSProp,\n",
    "        Adagrad,\n",
    "        Adam,\n",
    "        AdamW\n",
    "    ]\n",
    "\n",
    "    for optimizer in optimizers:\n",
    "        W1 = torch.tensor([1.], requires_grad=True)\n",
    "        b1 = torch.tensor([1.], requires_grad=True)\n",
    "        W2 = torch.tensor([1.], requires_grad=True)\n",
    "        b2 = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "        x = torch.tensor([1.], requires_grad=True)\n",
    "        y = torch.tensor([10.])\n",
    "\n",
    "        optimizer = optimizer([W1, b1, W2, b2])\n",
    "\n",
    "        for i in range(100):\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = func_nn(x, W1, b1, W2, b2)\n",
    "            loss = mse(y, y_hat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Optimiseur {optimizer.__class__.__name__}:\\nW1={W1.item()}, b1={b1.item()}, W2={W2.item()}, b2={b2.item()}\")\n",
    "        print(f\"Optimisation du réseau de neurones:\\nW1={W1.item()}, b1={b1.item()}, W2={W2.item()}, b2={b2.item()}\\n\")"
   ],
   "id": "da4600406c78abac",
   "outputs": [],
   "execution_count": 178
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:56:05.857141Z",
     "start_time": "2025-01-15T14:56:05.718252Z"
    }
   },
   "cell_type": "code",
   "source": "eval_nn_optim()",
   "id": "7e9a1e2f9cc247a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimiseur SGD:\n",
      "W1=1.7965515851974487, b1=1.7965515851974487, W2=2.356534481048584, b2=1.532727837562561\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.7965515851974487, b1=1.7965515851974487, W2=2.356534481048584, b2=1.532727837562561\n",
      "\n",
      "Optimiseur RMSProp:\n",
      "W1=1.956400752067566, b1=1.956400752067566, W2=1.956400752067566, b2=1.899566411972046\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.956400752067566, b1=1.956400752067566, W2=1.956400752067566, b2=1.899566411972046\n",
      "\n",
      "Optimiseur Adagrad:\n",
      "W1=1.186563491821289, b1=1.186563491821289, W2=1.186563491821289, b2=1.1806892156600952\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.186563491821289, b1=1.186563491821289, W2=1.186563491821289, b2=1.1806892156600952\n",
      "\n",
      "Optimiseur Adam:\n",
      "W1=1.1003371477127075, b1=1.1003371477127075, W2=1.1003371477127075, b2=1.0987093448638916\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.1003371477127075, b1=1.1003371477127075, W2=1.1003371477127075, b2=1.0987093448638916\n",
      "\n",
      "Optimiseur AdamW:\n",
      "W1=1.1013890504837036, b1=1.1013890504837036, W2=1.1013890504837036, b2=1.0997445583343506\n",
      "Optimisation du réseau de neurones:\n",
      "W1=1.1013890504837036, b1=1.1013890504837036, W2=1.1013890504837036, b2=1.0997445583343506\n",
      "\n"
     ]
    }
   ],
   "execution_count": 179
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Scheduler de Taux d'apprentissage***",
   "id": "5b3df25542164dbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de LRScheduler**",
   "id": "533f3bef6468a3a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:00:10.369793Z",
     "start_time": "2025-01-15T15:00:10.367418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LRScheduler:\n",
    "    def __init__(self, optimizer, initial_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        # TODO: Implementer le reste."
   ],
   "id": "1a5628f99bfe8a7f",
   "outputs": [],
   "execution_count": 181
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de LRSchedulerOnPlateau**",
   "id": "4c7b175f2dd4c96e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:00:11.154102Z",
     "start_time": "2025-01-15T15:00:11.151157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LRSchedulerOnPlateau(LRScheduler):\n",
    "    def __init__(self, optimizer, initial_lr, patience=10, factor=0.1, min_lr=1e-6, mode='min', threshold=1e-4):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        # TODO: Implementer le reste."
   ],
   "id": "720c56dbb5332b80",
   "outputs": [],
   "execution_count": 182
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
