{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T10:38:12.765338Z",
     "start_time": "2025-01-15T10:38:12.762868Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:29:42.709919Z",
     "start_time": "2025-01-15T10:29:42.707488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Génération du jeu de données linéaire\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "x_linear = np.linspace(-10, 10, n_samples)\n",
    "y_linear = 3 * x_linear + 5 + np.random.normal(0, 2, n_samples)\n",
    "\n",
    " # Génération du jeu de données non linéaire\n",
    "y_nonlinear = 0.5 * x_linear **2 - 4 * x_linear + np.random.normal(0 ,5 ,n_samples)"
   ],
   "id": "5b054a38d6955342",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Utils***",
   "id": "6ec84e42849b4a17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:55:33.778823Z",
     "start_time": "2025-01-15T10:55:33.774699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimizer_testing_loop(parameters : dict[str,]):\n",
    "    model = parameters['model']\n",
    "\n",
    "    criterion = parameters['criterion']\n",
    "    optimizer = parameters['optimizer']\n",
    "\n",
    "    x_tensor = parameters['x_tensor']\n",
    "    y_tensor = parameters['y_tensor']\n",
    "\n",
    "    epochs = parameters['epochs']\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x_tensor)\n",
    "        loss = criterion(predictions, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.data}\")"
   ],
   "id": "14a6a420243220be",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***SGD***",
   "id": "8dfdde3bc692cd33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de SGD**",
   "id": "5eb8e9d42bd537a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:14:45.872303Z",
     "start_time": "2025-01-15T11:14:45.869517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        hyperparams = {'lr': learning_rate}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                theta_t -= lr * theta_t.grad"
   ],
   "id": "9b1604245ddcf6ee",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de SGD**",
   "id": "b9e407ea443cb5d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:14:46.706844Z",
     "start_time": "2025-01-15T11:14:46.681548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': SGD(model.parameters(), learning_rate=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "6f319e016a6d00cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 23.399639129638672\n",
      "Epoch 20, Loss: 16.962400436401367\n",
      "Epoch 30, Loss: 12.664852142333984\n",
      "Epoch 40, Loss: 9.795772552490234\n",
      "Epoch 50, Loss: 7.880355358123779\n",
      "Epoch 60, Loss: 6.601606369018555\n",
      "Epoch 70, Loss: 5.747902870178223\n",
      "Epoch 80, Loss: 5.177964687347412\n",
      "Epoch 90, Loss: 4.797468185424805\n",
      "Epoch 100, Loss: 4.5434441566467285\n",
      "weight: tensor([[2.9703]])\n",
      "bias: tensor([4.4196])\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:14:46.909606Z",
     "start_time": "2025-01-15T11:14:46.885677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': SGD(model.parameters(), learning_rate=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "fe10005fb82a84f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 483.17266845703125\n",
      "Epoch 20, Loss: 414.6569519042969\n",
      "Epoch 30, Loss: 368.91534423828125\n",
      "Epoch 40, Loss: 338.3778381347656\n",
      "Epoch 50, Loss: 317.9908142089844\n",
      "Epoch 60, Loss: 304.3802795410156\n",
      "Epoch 70, Loss: 295.29376220703125\n",
      "Epoch 80, Loss: 289.2275695800781\n",
      "Epoch 90, Loss: 285.1777038574219\n",
      "Epoch 100, Loss: 282.47393798828125\n",
      "weight: tensor([[-4.1445]])\n",
      "bias: tensor([15.1297])\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***RMSProp***",
   "id": "152e498551a6e055"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de RMSProp**",
   "id": "777cec9838bb901a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:16:53.417776Z",
     "start_time": "2025-01-15T11:16:53.414420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RMSProp(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01, decay=0.9):\n",
    "        hyperparams = {'lr': learning_rate, 'decay': decay}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            decay = group['decay']\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'square_avg' not in state:\n",
    "                    state['square_avg'] = torch.zeros_like(theta_t)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                square_avg = decay * square_avg + (1 - decay) * (theta_t.grad ** 2)\n",
    "                state['square_avg'] = square_avg\n",
    "\n",
    "                theta_t -= lr * theta_t.grad / square_avg.sqrt()"
   ],
   "id": "aa87be320c6bac1c",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de RMSProp**",
   "id": "1eeb4b122e967c04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:17:54.562054Z",
     "start_time": "2025-01-15T11:17:54.534262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': RMSProp(model.parameters(), learning_rate=0.01, decay=0.8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "94571c79790e8b25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 166.65988159179688\n",
      "Epoch 20, Loss: 152.1524658203125\n",
      "Epoch 30, Loss: 138.68487548828125\n",
      "Epoch 40, Loss: 125.93838500976562\n",
      "Epoch 50, Loss: 113.8799819946289\n",
      "Epoch 60, Loss: 102.50534057617188\n",
      "Epoch 70, Loss: 91.81302642822266\n",
      "Epoch 80, Loss: 81.8017807006836\n",
      "Epoch 90, Loss: 72.47024536132812\n",
      "Epoch 100, Loss: 63.81686019897461\n",
      "weight: tensor([[1.8320]])\n",
      "bias: tensor([1.2606])\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:17:56.131719Z",
     "start_time": "2025-01-15T11:17:56.102504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': RMSProp(model.parameters(), learning_rate=0.01, decay=0.8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "91785a0139d74176",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1153.8677978515625\n",
      "Epoch 20, Loss: 1122.767578125\n",
      "Epoch 30, Loss: 1093.081298828125\n",
      "Epoch 40, Loss: 1064.1612548828125\n",
      "Epoch 50, Loss: 1035.94189453125\n",
      "Epoch 60, Loss: 1008.4160766601562\n",
      "Epoch 70, Loss: 981.5826416015625\n",
      "Epoch 80, Loss: 955.44140625\n",
      "Epoch 90, Loss: 929.9918823242188\n",
      "Epoch 100, Loss: 905.2341918945312\n",
      "weight: tensor([[-1.0310]])\n",
      "bias: tensor([0.2064])\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Adagrad***",
   "id": "f010754e9cd887d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de Adagrad**",
   "id": "4242814cd90f5d60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:30:10.113178Z",
     "start_time": "2025-01-15T11:30:10.109657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adagrad(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        hyperparams = {'lr': learning_rate}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'sum_squared_grads' not in state:\n",
    "                    state['sum_squared_grads'] = torch.zeros_like(theta_t)\n",
    "\n",
    "                sum_squared_grads = state['sum_squared_grads']\n",
    "                sum_squared_grads += theta_t.grad ** 2\n",
    "                state['sum_squared_grads'] = sum_squared_grads\n",
    "\n",
    "                adjusted_lr = lr / sum_squared_grads.sqrt()\n",
    "\n",
    "                theta_t -= adjusted_lr * theta_t.grad"
   ],
   "id": "1368be27bac6048a",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de Adagrad**",
   "id": "8a5105ec47c4d5e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:30:11.857189Z",
     "start_time": "2025-01-15T11:30:11.821779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adagrad(model.parameters(), learning_rate=0.5),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "c43614bd41ea6d64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 53.2936897277832\n",
      "Epoch 20, Loss: 15.242056846618652\n",
      "Epoch 30, Loss: 7.162252902984619\n",
      "Epoch 40, Loss: 5.094719409942627\n",
      "Epoch 50, Loss: 4.459561347961426\n",
      "Epoch 60, Loss: 4.224538326263428\n",
      "Epoch 70, Loss: 4.124268531799316\n",
      "Epoch 80, Loss: 4.077759742736816\n",
      "Epoch 90, Loss: 4.055278778076172\n",
      "Epoch 100, Loss: 4.0442070960998535\n",
      "weight: tensor([[2.9694]])\n",
      "bias: tensor([5.0186])\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:30:12.540027Z",
     "start_time": "2025-01-15T11:30:12.508596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adagrad(model.parameters(), learning_rate=0.5),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "ba6af84cb07e158b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 703.102783203125\n",
      "Epoch 20, Loss: 552.7655639648438\n",
      "Epoch 30, Loss: 485.4203186035156\n",
      "Epoch 40, Loss: 448.4309387207031\n",
      "Epoch 50, Loss: 425.0162353515625\n",
      "Epoch 60, Loss: 408.40093994140625\n",
      "Epoch 70, Loss: 395.54473876953125\n",
      "Epoch 80, Loss: 384.9854431152344\n",
      "Epoch 90, Loss: 375.9720458984375\n",
      "Epoch 100, Loss: 368.0896911621094\n",
      "weight: tensor([[-4.0899]])\n",
      "bias: tensor([7.9156])\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Adam***",
   "id": "b13a43547f37f00b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de Adam**",
   "id": "b6d5b0e1de91cdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:11:26.346916Z",
     "start_time": "2025-01-15T14:11:26.342229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        hyperparams = {'lr': learning_rate, 'beta1': beta1, 'beta2': beta2, 'epsilon': epsilon}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'm' not in state: # Moment d'ordre 1\n",
    "                    state['m'] = torch.zeros_like(theta_t)\n",
    "                if 'v' not in state: # Moment d'ordre 2\n",
    "                    state['v'] = torch.zeros_like(theta_t)\n",
    "                if 't' not in state: # Temps\n",
    "                    state['t'] = 0\n",
    "\n",
    "                # Premier Moment\n",
    "                m = state['m']\n",
    "                m_t = beta1 * m + (1 - beta1) * theta_t.grad\n",
    "                state['m'] = m_t\n",
    "\n",
    "                # Second Moment\n",
    "                v = state['v']\n",
    "                v_t = beta2 * v + (1 - beta2) * theta_t.grad ** 2\n",
    "                state['v'] = v_t\n",
    "\n",
    "                # Temps\n",
    "                t = state['t'] + 1\n",
    "                state['t'] = t\n",
    "\n",
    "                # Correction des biais\n",
    "                m_hat = m_t / (1 - beta1 ** t)\n",
    "                v_hat = v_t / (1 - beta2 ** t)\n",
    "\n",
    "                theta_t -= lr * m_hat / (v_hat.sqrt() + epsilon)"
   ],
   "id": "b6573846cf44e8b3",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de Adam**",
   "id": "c210eec6a98cf32e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:11:26.939175Z",
     "start_time": "2025-01-15T14:11:26.908205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adam(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "521702270449bd96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 89.61981201171875\n",
      "Epoch 20, Loss: 22.54928207397461\n",
      "Epoch 30, Loss: 6.636232852935791\n",
      "Epoch 40, Loss: 6.613010883331299\n",
      "Epoch 50, Loss: 5.385785102844238\n",
      "Epoch 60, Loss: 4.148060321807861\n",
      "Epoch 70, Loss: 4.068060874938965\n",
      "Epoch 80, Loss: 4.093616485595703\n",
      "Epoch 90, Loss: 4.043006420135498\n",
      "Epoch 100, Loss: 4.036981105804443\n",
      "weight: tensor([[2.9789]])\n",
      "bias: tensor([5.1578])\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:11:27.496725Z",
     "start_time": "2025-01-15T14:11:27.467933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adam(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "ff677ad6cf158b95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1076.4212646484375\n",
      "Epoch 20, Loss: 826.5248413085938\n",
      "Epoch 30, Loss: 653.0672607421875\n",
      "Epoch 40, Loss: 543.5895385742188\n",
      "Epoch 50, Loss: 479.2992248535156\n",
      "Epoch 60, Loss: 441.5941467285156\n",
      "Epoch 70, Loss: 416.8678894042969\n",
      "Epoch 80, Loss: 397.67169189453125\n",
      "Epoch 90, Loss: 381.0584411621094\n",
      "Epoch 100, Loss: 366.2650451660156\n",
      "weight: tensor([[-4.1924]])\n",
      "bias: tensor([8.0455])\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***AdamW***",
   "id": "40fbceea56e83cc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:15:19.988141Z",
     "start_time": "2025-01-15T14:15:19.983455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):\n",
    "        hyperparams = {'lr': learning_rate, 'beta1': beta1, 'beta2': beta2, 'epsilon': epsilon, 'weight_decay': weight_decay}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'm' not in state: # Moment d'ordre 1\n",
    "                    state['m'] = torch.zeros_like(theta_t)\n",
    "                if 'v' not in state: # Moment d'ordre 2\n",
    "                    state['v'] = torch.zeros_like(theta_t)\n",
    "                if 't' not in state: # Temps\n",
    "                    state['t'] = 0\n",
    "\n",
    "                # Premier Moment\n",
    "                m = state['m']\n",
    "                m_t = beta1 * m + (1 - beta1) * theta_t.grad\n",
    "                state['m'] = m_t\n",
    "\n",
    "                # Second Moment\n",
    "                v = state['v']\n",
    "                v_t = beta2 * v + (1 - beta2) * theta_t.grad ** 2\n",
    "                state['v'] = v_t\n",
    "\n",
    "                # Temps\n",
    "                t = state['t'] + 1\n",
    "                state['t'] = t\n",
    "\n",
    "                # Correction des biais\n",
    "                m_hat = m_t / (1 - beta1 ** t)\n",
    "                v_hat = v_t / (1 - beta2 ** t)\n",
    "\n",
    "                theta_t -= lr * m_hat / (v_hat.sqrt() + epsilon) - lr * weight_decay * theta_t"
   ],
   "id": "94d7052ef2a65b78",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ***Test de AdamW***",
   "id": "505fef5fa0888d50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:15:21.014562Z",
     "start_time": "2025-01-15T14:15:20.981595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': AdamW(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "8b4093aa3bfe36d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 60.561180114746094\n",
      "Epoch 20, Loss: 15.564854621887207\n",
      "Epoch 30, Loss: 10.691732406616211\n",
      "Epoch 40, Loss: 8.701184272766113\n",
      "Epoch 50, Loss: 5.131641387939453\n",
      "Epoch 60, Loss: 4.197311878204346\n",
      "Epoch 70, Loss: 4.089041709899902\n",
      "Epoch 80, Loss: 4.040408134460449\n",
      "Epoch 90, Loss: 4.091141223907471\n",
      "Epoch 100, Loss: 4.080957889556885\n",
      "weight: tensor([[2.9939]])\n",
      "bias: tensor([5.2811])\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:15:23.907514Z",
     "start_time": "2025-01-15T14:15:23.872278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': AdamW(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "f9a3cc8059159a66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 178.56480407714844\n",
      "Epoch 20, Loss: 66.68789672851562\n",
      "Epoch 30, Loss: 19.171457290649414\n",
      "Epoch 40, Loss: 8.25043773651123\n",
      "Epoch 50, Loss: 7.443469047546387\n",
      "Epoch 60, Loss: 6.193104267120361\n",
      "Epoch 70, Loss: 4.727298259735107\n",
      "Epoch 80, Loss: 4.123226165771484\n",
      "Epoch 90, Loss: 4.0350494384765625\n",
      "Epoch 100, Loss: 4.044890880584717\n",
      "weight: tensor([[2.9782]])\n",
      "bias: tensor([5.2258])\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Evaluation des Optimiseurs***",
   "id": "195056380131d973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ee1532fe2701e9b7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
