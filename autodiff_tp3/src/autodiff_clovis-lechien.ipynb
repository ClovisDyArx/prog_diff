{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ***TP3 - Clovis Lechien***\n",
    "\n",
    "1. [Utils](#Utils)\n",
    "2. [SGD](#SGD)\n",
    "3. [RMSProp](#RMSProp)\n",
    "4. [Adagrad](#Adagrad)\n",
    "5. [Adam](#Adam)\n",
    "6. [AdamW](#AdamW)\n",
    "7. [Evaluation des Optimiseurs](#evaluation-des-optimiseurs) FIXME\n",
    "8. [Réseau de Neurones](#réseau-de-neurones) FIXME\n",
    "9. [Scheduler de Taux d'Apprentissage](#schedulers) FIXME"
   ],
   "id": "e418516f7da7ad36"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:14.700085Z",
     "start_time": "2025-01-20T18:47:14.698122Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer"
   ],
   "outputs": [],
   "execution_count": 286
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:14.747889Z",
     "start_time": "2025-01-20T18:47:14.745463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Génération du jeu de données linéaire\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "x_linear = np.linspace(-10, 10, n_samples)\n",
    "y_linear = 3 * x_linear + 5 + np.random.normal(0, 2, n_samples)\n",
    "\n",
    " # Génération du jeu de données non linéaire\n",
    "y_nonlinear = 0.5 * x_linear **2 - 4 * x_linear + np.random.normal(0 ,5 ,n_samples)"
   ],
   "id": "5b054a38d6955342",
   "outputs": [],
   "execution_count": 287
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Custom Tensor class***",
   "id": "add1065b5a810b45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:14.800318Z",
     "start_time": "2025-01-20T18:47:14.792842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tensor:\n",
    "\n",
    "    \"\"\" stores a single scalar Tensor and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "\n",
    "        out = Tensor(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        out._prev = set([self, other])\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "\n",
    "        out = Tensor(self.data * other.data, [self, other], '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "\n",
    "        out = Tensor(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        # FIXME: implement relu\n",
    "        pass\n",
    "\n",
    "    def build_topo(self, visited=None, topo=None):\n",
    "        if self not in visited:\n",
    "            visited.add(self)\n",
    "            for child in self._prev:\n",
    "                child.build_topo(visited=visited, topo=topo)\n",
    "            topo.append(self)\n",
    "        return topo\n",
    "\n",
    "    def backward(self):\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        topo = self.build_topo(topo=topo, visited=visited)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad})\""
   ],
   "id": "7002f36d0bcccbac",
   "outputs": [],
   "execution_count": 288
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ***Custom operations***",
   "id": "29d81e5677e344c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:14.851767Z",
     "start_time": "2025-01-20T18:47:14.845564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def log_d(dual_number: Tensor):\n",
    "    out = Tensor(np.log(dual_number.data), (dual_number,), 'log')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += (1 / dual_number.data) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def exp_d(dual_number: Tensor):\n",
    "    out = Tensor(np.exp(dual_number.data), (dual_number,), 'exp')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += np.exp(dual_number.data) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def sin_d(dual_number: Tensor):\n",
    "    out = Tensor(np.sin(dual_number.data), (dual_number,), 'sin')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += np.cos(dual_number.data) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def cos_d(dual_number: Tensor):\n",
    "    out = Tensor(np.cos(dual_number.data), (dual_number,), 'cos')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += -np.sin(dual_number.data) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def sigmoid_d(dual_number: Tensor):\n",
    "    sig = 1 / (1 + np.exp(-dual_number.data))\n",
    "    out = Tensor(sig, (dual_number,), 'sigmoid')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += sig * (1 - sig) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def tanh_d(dual_number: Tensor):\n",
    "    tanh = np.tanh(dual_number.data)\n",
    "    out = Tensor(tanh, (dual_number,), 'tanh')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += (1 - tanh**2) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def tan_d(dual_number: Tensor):\n",
    "    out = Tensor(np.tan(dual_number.data), (dual_number,), 'tan')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += (1 / np.cos(dual_number.data)**2) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def sqrt_d(dual_number: Tensor):\n",
    "    out = Tensor(np.sqrt(dual_number.data), (dual_number,), 'sqrt')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += (0.5 / np.sqrt(dual_number.data)) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "\n",
    "def pow_d(dual_number: Tensor, power: int):\n",
    "    out = Tensor(dual_number.data**power, (dual_number,), f'pow{power}')\n",
    "\n",
    "    def _backward():\n",
    "        dual_number.grad += (power * dual_number.data**(power-1)) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def softmax_d(dual_number: Tensor):\n",
    "    e = np.exp(dual_number.data - np.max(dual_number.data))\n",
    "    out = Tensor(e / np.sum(e), (dual_number,), 'softmax')\n",
    "\n",
    "    def _backward():\n",
    "        for i in range(len(dual_number.data)):\n",
    "            for j in range(len(dual_number.data)):\n",
    "                if i == j:\n",
    "                    dual_number.grad[i] += out.data[i] * (1 - out.data[i]) * out.grad[i]\n",
    "                else:\n",
    "                    dual_number.grad[i] += -out.data[i] * out.data[j] * out.grad[j]\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out"
   ],
   "id": "52846f4c7ab39c0",
   "outputs": [],
   "execution_count": 289
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Utils<a name=\"Utils\"></a>***",
   "id": "6ec84e42849b4a17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:14.902359Z",
     "start_time": "2025-01-20T18:47:14.899012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimizer_testing_loop(parameters : dict[str,]):\n",
    "    model = parameters['model']\n",
    "\n",
    "    criterion = parameters['criterion']\n",
    "    optimizer = parameters['optimizer']\n",
    "\n",
    "    x_tensor = parameters['x_tensor']\n",
    "    y_tensor = parameters['y_tensor']\n",
    "\n",
    "    epochs = parameters['epochs']\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x_tensor)\n",
    "        loss = criterion(predictions, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.data}\")\n",
    "\n",
    "\n",
    "def check_diffs(a : list[float], b : list[float], tol : float = 1e-4):\n",
    "    res = np.allclose(a, b, atol=tol)\n",
    "    if res:\n",
    "        print(f\"All elements between\\n{a}\\and\\n{b}\\nare close within a tolerance of {tol}\")\n",
    "    else:\n",
    "        print(\"Test failed\")"
   ],
   "id": "14a6a420243220be",
   "outputs": [],
   "execution_count": 290
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***SGD<a name=\"SGD\"></a>***",
   "id": "8dfdde3bc692cd33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de SGD**",
   "id": "5eb8e9d42bd537a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:14.948618Z",
     "start_time": "2025-01-20T18:47:14.945511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SGD_torch(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        hyperparams = {'lr': learning_rate}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                theta_t -= lr * theta_t.grad\n",
    "\n",
    "\n",
    "class SGD_custom:\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.data -= self.learning_rate * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ],
   "id": "9b1604245ddcf6ee",
   "outputs": [],
   "execution_count": 291
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de SGD**",
   "id": "b9e407ea443cb5d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.015074Z",
     "start_time": "2025-01-20T18:47:14.991940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': SGD_torch(model.parameters(), learning_rate=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "6f319e016a6d00cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 21.153648376464844\n",
      "Epoch 20, Loss: 15.46296501159668\n",
      "Epoch 30, Loss: 11.663813591003418\n",
      "Epoch 40, Loss: 9.127474784851074\n",
      "Epoch 50, Loss: 7.434192657470703\n",
      "Epoch 60, Loss: 6.303741931915283\n",
      "Epoch 70, Loss: 5.549045562744141\n",
      "Epoch 80, Loss: 5.045206069946289\n",
      "Epoch 90, Loss: 4.708837985992432\n",
      "Epoch 100, Loss: 4.4842753410339355\n",
      "weight: tensor([[2.9703]])\n",
      "bias: tensor([4.4615])\n"
     ]
    }
   ],
   "execution_count": 292
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.062573Z",
     "start_time": "2025-01-20T18:47:15.039147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': SGD_torch(model.parameters(), learning_rate=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "fe10005fb82a84f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 467.876708984375\n",
      "Epoch 20, Loss: 404.4452209472656\n",
      "Epoch 30, Loss: 362.09796142578125\n",
      "Epoch 40, Loss: 333.8265380859375\n",
      "Epoch 50, Loss: 314.95233154296875\n",
      "Epoch 60, Loss: 302.35174560546875\n",
      "Epoch 70, Loss: 293.9394836425781\n",
      "Epoch 80, Loss: 288.32342529296875\n",
      "Epoch 90, Loss: 284.5740661621094\n",
      "Epoch 100, Loss: 282.07098388671875\n",
      "weight: tensor([[-4.1445]])\n",
      "bias: tensor([15.2161])\n"
     ]
    }
   ],
   "execution_count": 293
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***RMSProp<a name=\"RMSProp\"></a>***",
   "id": "152e498551a6e055"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de RMSProp**",
   "id": "777cec9838bb901a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.090154Z",
     "start_time": "2025-01-20T18:47:15.086149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RMSProp_torch(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01, decay=0.9):\n",
    "        hyperparams = {'lr': learning_rate, 'decay': decay}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            decay = group['decay']\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'square_avg' not in state:\n",
    "                    state['square_avg'] = torch.zeros_like(theta_t)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                square_avg = decay * square_avg + (1 - decay) * (theta_t.grad ** 2)\n",
    "                state['square_avg'] = square_avg\n",
    "\n",
    "                theta_t -= lr * theta_t.grad / square_avg.sqrt()\n",
    "\n",
    "\n",
    "class RMSProp_custom:\n",
    "    def __init__(self, params, learning_rate=0.01, decay=0.9):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.state = {param: {'square_avg': Tensor(0.0)} for param in params}\n",
    "\n",
    "    def step(self):\n",
    "        for theta_t in self.params:\n",
    "            if theta_t.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[theta_t]\n",
    "\n",
    "            square_avg = state['square_avg']\n",
    "            square_avg.data = self.decay * square_avg.data + (1 - self.decay) * (theta_t.grad ** 2)\n",
    "            state['square_avg'] = square_avg\n",
    "\n",
    "            theta_t.data -= self.learning_rate * theta_t.grad / np.sqrt(square_avg.data)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ],
   "id": "aa87be320c6bac1c",
   "outputs": [],
   "execution_count": 294
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de RMSProp**",
   "id": "1eeb4b122e967c04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.158944Z",
     "start_time": "2025-01-20T18:47:15.132657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': RMSProp_torch(model.parameters(), learning_rate=0.01, decay=0.8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "94571c79790e8b25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 154.77980041503906\n",
      "Epoch 20, Loss: 140.78639221191406\n",
      "Epoch 30, Loss: 127.82095336914062\n",
      "Epoch 40, Loss: 115.57490539550781\n",
      "Epoch 50, Loss: 104.01622772216797\n",
      "Epoch 60, Loss: 93.14061737060547\n",
      "Epoch 70, Loss: 82.94654083251953\n",
      "Epoch 80, Loss: 73.43268585205078\n",
      "Epoch 90, Loss: 64.59754180908203\n",
      "Epoch 100, Loss: 56.43938446044922\n",
      "weight: tensor([[1.8942]])\n",
      "bias: tensor([1.6193])\n"
     ]
    }
   ],
   "execution_count": 295
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.206358Z",
     "start_time": "2025-01-20T18:47:15.179622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': RMSProp_torch(model.parameters(), learning_rate=0.01, decay=0.8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "91785a0139d74176",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1134.2470703125\n",
      "Epoch 20, Loss: 1102.624267578125\n",
      "Epoch 30, Loss: 1072.427001953125\n",
      "Epoch 40, Loss: 1042.997802734375\n",
      "Epoch 50, Loss: 1014.2693481445312\n",
      "Epoch 60, Loss: 986.234619140625\n",
      "Epoch 70, Loss: 958.8926391601562\n",
      "Epoch 80, Loss: 932.2430419921875\n",
      "Epoch 90, Loss: 906.2855224609375\n",
      "Epoch 100, Loss: 881.0201416015625\n",
      "weight: tensor([[-0.9116]])\n",
      "bias: tensor([1.7273])\n"
     ]
    }
   ],
   "execution_count": 296
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Adagrad<a name=\"Adagrad\"></a>***",
   "id": "f010754e9cd887d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de Adagrad**",
   "id": "4242814cd90f5d60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.229589Z",
     "start_time": "2025-01-20T18:47:15.225688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adagrad_torch(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        hyperparams = {'lr': learning_rate}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'sum_squared_grads' not in state:\n",
    "                    state['sum_squared_grads'] = torch.zeros_like(theta_t)\n",
    "\n",
    "                sum_squared_grads = state['sum_squared_grads']\n",
    "                sum_squared_grads += theta_t.grad ** 2\n",
    "                state['sum_squared_grads'] = sum_squared_grads\n",
    "\n",
    "                adjusted_lr = lr / sum_squared_grads.sqrt()\n",
    "\n",
    "                theta_t -= adjusted_lr * theta_t.grad\n",
    "\n",
    "\n",
    "class Adagrad_custom:\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.state = {param: {'sum_squared_grads': Tensor(0.0)} for param in params}\n",
    "\n",
    "    def step(self):\n",
    "        for theta_t in self.params:\n",
    "            if theta_t.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[theta_t]\n",
    "\n",
    "            sum_squared_grads = state['sum_squared_grads']\n",
    "            sum_squared_grads.data += theta_t.grad ** 2\n",
    "            state['sum_squared_grads'] = sum_squared_grads\n",
    "\n",
    "            adjusted_lr = self.learning_rate / np.sqrt(sum_squared_grads.data)\n",
    "\n",
    "            theta_t.data -= adjusted_lr * theta_t.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ],
   "id": "1368be27bac6048a",
   "outputs": [],
   "execution_count": 297
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de Adagrad**",
   "id": "8a5105ec47c4d5e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.297398Z",
     "start_time": "2025-01-20T18:47:15.272206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adagrad_torch(model.parameters(), learning_rate=0.5),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "c43614bd41ea6d64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 63.79230499267578\n",
      "Epoch 20, Loss: 18.731962203979492\n",
      "Epoch 30, Loss: 8.29696273803711\n",
      "Epoch 40, Loss: 5.470900058746338\n",
      "Epoch 50, Loss: 4.594921112060547\n",
      "Epoch 60, Loss: 4.27977991104126\n",
      "Epoch 70, Loss: 4.149801731109619\n",
      "Epoch 80, Loss: 4.090644359588623\n",
      "Epoch 90, Loss: 4.062098979949951\n",
      "Epoch 100, Loss: 4.047893524169922\n",
      "weight: tensor([[2.9686]])\n",
      "bias: tensor([5.0030])\n"
     ]
    }
   ],
   "execution_count": 298
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.345087Z",
     "start_time": "2025-01-20T18:47:15.319330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adagrad_torch(model.parameters(), learning_rate=0.5),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "ba6af84cb07e158b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 637.4957275390625\n",
      "Epoch 20, Loss: 519.2680053710938\n",
      "Epoch 30, Loss: 468.3641662597656\n",
      "Epoch 40, Loss: 439.9420166015625\n",
      "Epoch 50, Loss: 420.90362548828125\n",
      "Epoch 60, Loss: 406.4686584472656\n",
      "Epoch 70, Loss: 394.6741027832031\n",
      "Epoch 80, Loss: 384.6214599609375\n",
      "Epoch 90, Loss: 375.8455505371094\n",
      "Epoch 100, Loss: 368.0722351074219\n",
      "weight: tensor([[-4.1258]])\n",
      "bias: tensor([7.9115])\n"
     ]
    }
   ],
   "execution_count": 299
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Adam<a name=\"Adam\"></a>***",
   "id": "b13a43547f37f00b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de Adam**",
   "id": "b6d5b0e1de91cdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.371564Z",
     "start_time": "2025-01-20T18:47:15.365801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adam_torch(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        hyperparams = {'lr': learning_rate, 'beta1': beta1, 'beta2': beta2, 'epsilon': epsilon}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'm' not in state: # Moment d'ordre 1\n",
    "                    state['m'] = torch.zeros_like(theta_t)\n",
    "                if 'v' not in state: # Moment d'ordre 2\n",
    "                    state['v'] = torch.zeros_like(theta_t)\n",
    "                if 't' not in state: # Temps\n",
    "                    state['t'] = 0\n",
    "\n",
    "                # Premier Moment\n",
    "                m = state['m']\n",
    "                m_t = beta1 * m + (1 - beta1) * theta_t.grad\n",
    "                state['m'] = m_t\n",
    "\n",
    "                # Second Moment\n",
    "                v = state['v']\n",
    "                v_t = beta2 * v + (1 - beta2) * theta_t.grad ** 2\n",
    "                state['v'] = v_t\n",
    "\n",
    "                # Temps\n",
    "                t = state['t'] + 1\n",
    "                state['t'] = t\n",
    "\n",
    "                # Correction des biais\n",
    "                m_hat = m_t / (1 - beta1 ** t)\n",
    "                v_hat = v_t / (1 - beta2 ** t)\n",
    "\n",
    "                theta_t -= lr * m_hat / (v_hat.sqrt() + epsilon)\n",
    "\n",
    "\n",
    "class Adam_custom:\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.state = {param: {'m': Tensor(0.0), 'v': Tensor(0.0), 't': 0} for param in params}\n",
    "\n",
    "    def step(self):\n",
    "        for theta_t in self.params:\n",
    "            if theta_t.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[theta_t]\n",
    "\n",
    "            # Premier Moment\n",
    "            m = state['m']\n",
    "            m_t = self.beta1 * m.data + (1 - self.beta1) * theta_t.grad\n",
    "            state['m'].data = m_t\n",
    "\n",
    "            # Second Moment\n",
    "            v = state['v']\n",
    "            v_t = self.beta2 * v.data + (1 - self.beta2) * theta_t.grad ** 2\n",
    "            state['v'].data = v_t\n",
    "\n",
    "            # Temps\n",
    "            t = state['t'] + 1\n",
    "            state['t'] = t\n",
    "\n",
    "            # Correction des biais\n",
    "            m_hat = m_t / (1 - self.beta1 ** t)\n",
    "            v_hat = v_t / (1 - self.beta2 ** t)\n",
    "\n",
    "            theta_t.data -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ],
   "id": "b6573846cf44e8b3",
   "outputs": [],
   "execution_count": 300
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de Adam**",
   "id": "c210eec6a98cf32e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.448702Z",
     "start_time": "2025-01-20T18:47:15.419225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adam_torch(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "521702270449bd96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 89.26514434814453\n",
      "Epoch 20, Loss: 22.805910110473633\n",
      "Epoch 30, Loss: 7.145306587219238\n",
      "Epoch 40, Loss: 6.927384853363037\n",
      "Epoch 50, Loss: 5.4639458656311035\n",
      "Epoch 60, Loss: 4.161281108856201\n",
      "Epoch 70, Loss: 4.072627067565918\n",
      "Epoch 80, Loss: 4.0902204513549805\n",
      "Epoch 90, Loss: 4.041722297668457\n",
      "Epoch 100, Loss: 4.037848949432373\n",
      "weight: tensor([[2.9794]])\n",
      "bias: tensor([5.1643])\n"
     ]
    }
   ],
   "execution_count": 301
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.496371Z",
     "start_time": "2025-01-20T18:47:15.466061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': Adam_torch(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_nonlinear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "ff677ad6cf158b95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 776.5965576171875\n",
      "Epoch 20, Loss: 606.6329956054688\n",
      "Epoch 30, Loss: 506.25115966796875\n",
      "Epoch 40, Loss: 454.3979797363281\n",
      "Epoch 50, Loss: 426.4893493652344\n",
      "Epoch 60, Loss: 406.1849365234375\n",
      "Epoch 70, Loss: 387.8637390136719\n",
      "Epoch 80, Loss: 371.2755432128906\n",
      "Epoch 90, Loss: 356.7945251464844\n",
      "Epoch 100, Loss: 344.265625\n",
      "weight: tensor([[-4.1374]])\n",
      "bias: tensor([9.2855])\n"
     ]
    }
   ],
   "execution_count": 302
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***AdamW<a name=\"AdamW\"></a>***",
   "id": "40fbceea56e83cc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.518452Z",
     "start_time": "2025-01-20T18:47:15.512479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AdamW_torch(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):\n",
    "        hyperparams = {'lr': learning_rate, 'beta1': beta1, 'beta2': beta2, 'epsilon': epsilon, 'weight_decay': weight_decay}\n",
    "        super().__init__(params=params, defaults=hyperparams)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            for theta_t in group['params']:\n",
    "                if theta_t.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[theta_t]\n",
    "                if 'm' not in state: # Moment d'ordre 1\n",
    "                    state['m'] = torch.zeros_like(theta_t)\n",
    "                if 'v' not in state: # Moment d'ordre 2\n",
    "                    state['v'] = torch.zeros_like(theta_t)\n",
    "                if 't' not in state: # Temps\n",
    "                    state['t'] = 0\n",
    "\n",
    "                # Premier Moment\n",
    "                m = state['m']\n",
    "                m_t = beta1 * m + (1 - beta1) * theta_t.grad\n",
    "                state['m'] = m_t\n",
    "\n",
    "                # Second Moment\n",
    "                v = state['v']\n",
    "                v_t = beta2 * v + (1 - beta2) * theta_t.grad ** 2\n",
    "                state['v'] = v_t\n",
    "\n",
    "                # Temps\n",
    "                t = state['t'] + 1\n",
    "                state['t'] = t\n",
    "\n",
    "                # Correction des biais\n",
    "                m_hat = m_t / (1 - beta1 ** t)\n",
    "                v_hat = v_t / (1 - beta2 ** t)\n",
    "\n",
    "                theta_t -= lr * m_hat / (np.sqrt(v_hat) + epsilon) - lr * weight_decay * theta_t\n",
    "\n",
    "\n",
    "class AdamW_custom:\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        self.state = {param: {'m': Tensor(0.0), 'v': Tensor(0.0), 't': 0} for param in params}\n",
    "\n",
    "    def step(self):\n",
    "        for theta_t in self.params:\n",
    "            if theta_t.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[theta_t]\n",
    "\n",
    "            # Premier Moment\n",
    "            m = state['m']\n",
    "            m_t = self.beta1 * m.data + (1 - self.beta1) * theta_t.grad\n",
    "            state['m'].data = m_t\n",
    "\n",
    "            # Second Moment\n",
    "            v = state['v']\n",
    "            v_t = self.beta2 * v.data + (1 - self.beta2) * theta_t.grad ** 2\n",
    "            state['v'].data = v_t\n",
    "\n",
    "            # Temps\n",
    "            t = state['t'] + 1\n",
    "            state['t'] = t\n",
    "\n",
    "            # Correction des biais\n",
    "            m_hat = m_t / (1 - self.beta1 ** t)\n",
    "            v_hat = v_t / (1 - self.beta2 ** t)\n",
    "\n",
    "            theta_t.data -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon) - self.learning_rate * self.weight_decay * theta_t.data\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ],
   "id": "94d7052ef2a65b78",
   "outputs": [],
   "execution_count": 303
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ***Test de AdamW***",
   "id": "505fef5fa0888d50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.600881Z",
     "start_time": "2025-01-20T18:47:15.565784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': AdamW_torch(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "8b4093aa3bfe36d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 323.1841125488281\n",
      "Epoch 20, Loss: 155.3002471923828\n",
      "Epoch 30, Loss: 59.000946044921875\n",
      "Epoch 40, Loss: 16.85196304321289\n",
      "Epoch 50, Loss: 5.0475873947143555\n",
      "Epoch 60, Loss: 4.246348857879639\n",
      "Epoch 70, Loss: 4.893298149108887\n",
      "Epoch 80, Loss: 4.814894199371338\n",
      "Epoch 90, Loss: 4.4424591064453125\n",
      "Epoch 100, Loss: 4.1949262619018555\n",
      "weight: tensor([[3.0312]])\n",
      "bias: tensor([5.2628])\n"
     ]
    }
   ],
   "execution_count": 304
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.648158Z",
     "start_time": "2025-01-20T18:47:15.612995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(1, 1)\n",
    "linear_parameters = {\n",
    "    'model': model,\n",
    "    'criterion': nn.MSELoss(),\n",
    "    'optimizer': AdamW_torch(model.parameters(), learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01),\n",
    "    'x_tensor': torch.from_numpy(x_linear).float().view(-1, 1),\n",
    "    'y_tensor': torch.from_numpy(y_linear).float().view(-1, 1),\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "optimizer_testing_loop(linear_parameters)"
   ],
   "id": "f9a3cc8059159a66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 260.1888427734375\n",
      "Epoch 20, Loss: 114.68164825439453\n",
      "Epoch 30, Loss: 38.556549072265625\n",
      "Epoch 40, Loss: 10.718267440795898\n",
      "Epoch 50, Loss: 5.453664779663086\n",
      "Epoch 60, Loss: 5.509186267852783\n",
      "Epoch 70, Loss: 5.184496879577637\n",
      "Epoch 80, Loss: 4.536145210266113\n",
      "Epoch 90, Loss: 4.17765998840332\n",
      "Epoch 100, Loss: 4.0824127197265625\n",
      "weight: tensor([[2.9987]])\n",
      "bias: tensor([5.2565])\n"
     ]
    }
   ],
   "execution_count": 305
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Evaluation des Optimiseurs<a name=\"evaluation-des-optimiseurs\"></a>***",
   "id": "195056380131d973"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.665509Z",
     "start_time": "2025-01-20T18:47:15.663234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def f(x : torch.Tensor | Tensor):\n",
    "    return (x - 2) ** 2\n",
    "\n",
    "\n",
    "def f_nonconvexe(x : torch.Tensor | Tensor):\n",
    "    return 3*x ** 2 - 2*x"
   ],
   "id": "ee1532fe2701e9b7",
   "outputs": [],
   "execution_count": 306
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.714455Z",
     "start_time": "2025-01-20T18:47:15.709126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_optim(x : torch.Tensor | Tensor, convexe : bool = True, scheduler : bool = False):\n",
    "    if convexe:\n",
    "        print(f\"Optimisation de la fonction convexe f(x) = (x - 2)²\")\n",
    "        y = f(x)\n",
    "    else:\n",
    "        print(f\"Optimisation de la fonction non convexe f(x) = 3x² - 2x\")\n",
    "        y = f_nonconvexe(x)\n",
    "\n",
    "    y.backward()\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        print(f\"Gradient de f en x={x.item()}: x.grad={x.grad.item()}\")\n",
    "    else:\n",
    "        print(f\"Gradient de f en x={x.data}: x.grad={x.grad}\")\n",
    "\n",
    "    resulting_x = []\n",
    "    resulting_fx = []\n",
    "\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        optimizers_torch = [\n",
    "            SGD_torch,\n",
    "            RMSProp_torch,\n",
    "            Adagrad_torch,\n",
    "            Adam_torch,\n",
    "            AdamW_torch\n",
    "        ]\n",
    "\n",
    "        for optimizer in optimizers_torch:\n",
    "            optimizer = optimizer([x])\n",
    "            if scheduler:\n",
    "                scheduler = LRSchedulerOnPlateauTorch(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "            for i in range(100):\n",
    "                optimizer.zero_grad()\n",
    "                if convexe:\n",
    "                    y = f(x)\n",
    "                else:\n",
    "                    y = f_nonconvexe(x)\n",
    "                y.backward()\n",
    "                optimizer.step()\n",
    "                if scheduler:\n",
    "                    scheduler.step(y)\n",
    "            print(f\"Optimiseur {optimizer.__class__.__name__}: x={x.item()}, f(x)={f(x).item()}\")\n",
    "            resulting_x.append(x.item())\n",
    "            resulting_fx.append(f(x).item())\n",
    "\n",
    "    else:\n",
    "        optimizers_custom = [\n",
    "            SGD_custom,\n",
    "            RMSProp_custom,\n",
    "            Adagrad_custom,\n",
    "            Adam_custom,\n",
    "            AdamW_custom\n",
    "        ]\n",
    "\n",
    "        for optimizer in optimizers_custom:\n",
    "            optimizer = optimizer([x])\n",
    "            if scheduler:\n",
    "                scheduler = LRSchedulerOnPlateauCustom(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "            for i in range(100):\n",
    "                optimizer.zero_grad()\n",
    "                if convexe:\n",
    "                    y = f(x)\n",
    "                else:\n",
    "                    y = f_nonconvexe(x)\n",
    "                y.backward()\n",
    "                optimizer.step()\n",
    "                if scheduler:\n",
    "                    scheduler.step(y)\n",
    "            print(f\"Optimiseur {optimizer.__class__.__name__}: x={x.data}, f(x)={f(x).data}\")\n",
    "            resulting_x.append(x.data)\n",
    "            resulting_fx.append(f(x).data)\n",
    "\n",
    "    return resulting_x, resulting_fx"
   ],
   "id": "a4424025c79c9f87",
   "outputs": [],
   "execution_count": 307
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.947978Z",
     "start_time": "2025-01-20T18:47:15.759840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([69.], requires_grad=True)\n",
    "conv_torch_x, conv_torch_fx = eval_optim(x, convexe=True)\n",
    "print()\n",
    "nonconv_torch_x, nonconv_torch_fx = eval_optim(x, convexe=False)"
   ],
   "id": "85f4258055c8e2fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la fonction convexe f(x) = (x - 2)²\n",
      "Gradient de f en x=69.0: x.grad=134.0\n",
      "Optimiseur SGD_torch: x=10.885512351989746, f(x)=78.95233154296875\n",
      "Optimiseur RMSProp_torch: x=9.80399227142334, f(x)=60.90229415893555\n",
      "Optimiseur Adagrad_torch: x=9.618916511535645, f(x)=58.047889709472656\n",
      "Optimiseur Adam_torch: x=9.519140243530273, f(x)=56.537471771240234\n",
      "Optimiseur AdamW_torch: x=9.428813934326172, f(x)=55.18727493286133\n",
      "\n",
      "Optimisation de la fonction non convexe f(x) = 3x² - 2x\n",
      "Gradient de f en x=9.428813934326172: x.grad=69.43231201171875\n",
      "Optimiseur SGD_torch: x=0.3520234227180481, f(x)=2.715826988220215\n",
      "Optimiseur RMSProp_torch: x=0.33838367462158203, f(x)=2.7609689235687256\n",
      "Optimiseur Adagrad_torch: x=0.3333333432674408, f(x)=2.777777671813965\n",
      "Optimiseur Adam_torch: x=0.3333333432674408, f(x)=2.777777671813965\n",
      "Optimiseur AdamW_torch: x=0.3333345949649811, f(x)=2.77777361869812\n"
     ]
    }
   ],
   "execution_count": 308
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:15.972083Z",
     "start_time": "2025-01-20T18:47:15.957634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = Tensor(69.)\n",
    "conv_custom_x, conv_custom_fx = eval_optim(x, convexe=True)\n",
    "print()\n",
    "nonconv_custom_x, nonconv_custom_fx = eval_optim(x, convexe=False)"
   ],
   "id": "f74e4aa2f7e56bd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la fonction convexe f(x) = (x - 2)²\n",
      "Gradient de f en x=69.0: x.grad=134.0\n",
      "Optimiseur SGD_custom: x=10.885510244948467, f(x)=78.95229231308416\n",
      "Optimiseur RMSProp_custom: x=9.803987775902403, f(x)=60.90222520643413\n",
      "Optimiseur Adagrad_custom: x=9.618913070757662, f(x)=58.04783637976195\n",
      "Optimiseur Adam_custom: x=9.519135827365043, f(x)=56.53740359036458\n",
      "Optimiseur AdamW_custom: x=9.428814427498079, f(x)=55.18728379820361\n",
      "\n",
      "Optimisation de la fonction non convexe f(x) = 3x² - 2x\n",
      "Gradient de f en x=9.428814427498079: x.grad=69.43231722986997\n",
      "Optimiseur SGD_custom: x=0.3520234079595069, f(x)=2.715826847913398\n",
      "Optimiseur RMSProp_custom: x=0.3383837368874569, f(x)=2.760968605840092\n",
      "Optimiseur Adagrad_custom: x=0.3333333333333333, f(x)=2.777777777777778\n",
      "Optimiseur Adam_custom: x=0.3333333333333333, f(x)=2.777777777777778\n",
      "Optimiseur AdamW_custom: x=0.33333456306680137, f(x)=2.77777367866773\n"
     ]
    }
   ],
   "execution_count": 309
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.004847Z",
     "start_time": "2025-01-20T18:47:16.002183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "check_diffs(conv_torch_x, conv_custom_x, tol=1e-4)\n",
    "print()\n",
    "check_diffs(conv_torch_fx, conv_custom_fx, tol=1e-4)"
   ],
   "id": "15c9a8928c275420",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements between\n",
      "[10.885512351989746, 9.80399227142334, 9.618916511535645, 9.519140243530273, 9.428813934326172]\u0007nd\n",
      "[10.885510244948467, 9.803987775902403, 9.618913070757662, 9.519135827365043, 9.428814427498079]\n",
      "are close within a tolerance of 0.0001\n",
      "\n",
      "All elements between\n",
      "[78.95233154296875, 60.90229415893555, 58.047889709472656, 56.537471771240234, 55.18727493286133]\u0007nd\n",
      "[78.95229231308416, 60.90222520643413, 58.04783637976195, 56.53740359036458, 55.18728379820361]\n",
      "are close within a tolerance of 0.0001\n"
     ]
    }
   ],
   "execution_count": 310
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Réseau de Neurones<a name=\"réseau-de-neurones\"></a>***",
   "id": "1d95d3f325e5b75f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.052384Z",
     "start_time": "2025-01-20T18:47:16.049858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def func_nn(x, W1, b1, W2, b2):\n",
    "    h1 = W1 * x + b1\n",
    "    y = W2 * h1 + b2\n",
    "    return y\n",
    "\n",
    "\n",
    "def mse(y, y_hat):\n",
    "    return (y - y_hat) ** 2"
   ],
   "id": "f1369e9cd3e0638b",
   "outputs": [],
   "execution_count": 311
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.100596Z",
     "start_time": "2025-01-20T18:47:16.095674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_nn_optim(scheduler : bool = False, custom : bool = True):\n",
    "    results = []\n",
    "\n",
    "    if not custom:\n",
    "        optimizers = [\n",
    "            SGD_torch,\n",
    "            RMSProp_torch,\n",
    "            Adagrad_torch,\n",
    "            Adam_torch,\n",
    "            AdamW_torch\n",
    "        ]\n",
    "\n",
    "        for optimizer in optimizers:\n",
    "            W1 = torch.tensor([1.], requires_grad=True)\n",
    "            b1 = torch.tensor([1.], requires_grad=True)\n",
    "            W2 = torch.tensor([1.], requires_grad=True)\n",
    "            b2 = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "            x = torch.tensor([1.], requires_grad=True)\n",
    "            y = torch.tensor([10.])\n",
    "\n",
    "            optimizer = optimizer([W1, b1, W2, b2])\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler = LRSchedulerOnPlateauTorch(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "\n",
    "            for i in range(100):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_hat = func_nn(x, W1, b1, W2, b2)\n",
    "                loss = mse(y, y_hat)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step(loss)\n",
    "\n",
    "            print(f\"Optimiseur {optimizer.__class__.__name__}:\\nW1={W1.item()}, b1={b1.item()}, W2={W2.item()}, b2={b2.item()}\")\n",
    "            results.append([W1.item(), b1.item(), W2.item(), b2.item()])\n",
    "\n",
    "    else:\n",
    "        optimizers = [\n",
    "            SGD_custom,\n",
    "            RMSProp_custom,\n",
    "            Adagrad_custom,\n",
    "            Adam_custom,\n",
    "            AdamW_custom\n",
    "        ]\n",
    "\n",
    "        for optimizer in optimizers:\n",
    "            W1 = Tensor(1.)\n",
    "            b1 = Tensor(1.)\n",
    "            W2 = Tensor(1.)\n",
    "            b2 = Tensor(1.)\n",
    "\n",
    "            x = Tensor(1.)\n",
    "            y = Tensor(10.)\n",
    "\n",
    "            optimizer = optimizer([W1, b1, W2, b2])\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler = LRSchedulerOnPlateauCustom(optimizer, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6, mode='min', threshold=1e-4)\n",
    "\n",
    "            for i in range(100):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_hat = func_nn(x, W1, b1, W2, b2)\n",
    "                loss = mse(y, y_hat)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step(loss)\n",
    "\n",
    "            print(f\"Optimiseur {optimizer.__class__.__name__}:\\nW1={W1.data}, b1={b1.data}, W2={W2.data}, b2={b2.data}\")\n",
    "            results.append([W1.data, b1.data, W2.data, b2.data])\n",
    "\n",
    "    return results"
   ],
   "id": "da4600406c78abac",
   "outputs": [],
   "execution_count": 312
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.286654Z",
     "start_time": "2025-01-20T18:47:16.147485Z"
    }
   },
   "cell_type": "code",
   "source": "torch_nn = eval_nn_optim(scheduler=False, custom=False)",
   "id": "7e9a1e2f9cc247a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimiseur SGD_torch:\n",
      "W1=1.7965515851974487, b1=1.7965515851974487, W2=2.356534481048584, b2=1.532727837562561\n",
      "Optimiseur RMSProp_torch:\n",
      "W1=1.956400752067566, b1=1.956400752067566, W2=1.956400752067566, b2=1.899566411972046\n",
      "Optimiseur Adagrad_torch:\n",
      "W1=1.186563491821289, b1=1.186563491821289, W2=1.186563491821289, b2=1.1806892156600952\n",
      "Optimiseur Adam_torch:\n",
      "W1=1.1003371477127075, b1=1.1003371477127075, W2=1.1003371477127075, b2=1.0987093448638916\n",
      "Optimiseur AdamW_torch:\n",
      "W1=1.1013890504837036, b1=1.1013890504837036, W2=1.1013890504837036, b2=1.0997445583343506\n"
     ]
    }
   ],
   "execution_count": 313
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.320711Z",
     "start_time": "2025-01-20T18:47:16.306700Z"
    }
   },
   "cell_type": "code",
   "source": "custom_nn = eval_nn_optim(scheduler=False, custom=True)",
   "id": "408b64606de25ff0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimiseur SGD_custom:\n",
      "W1=1.7965517126874235, b1=1.7965517126874235, W2=2.3565344500454675, b2=1.532727995527799\n",
      "Optimiseur RMSProp_custom:\n",
      "W1=1.9564006987933535, b1=1.9564006987933535, W2=1.9564006987933535, b2=1.8995662439967032\n",
      "Optimiseur Adagrad_custom:\n",
      "W1=1.1865635594156694, b1=1.1865635594156694, W2=1.1865635594156694, b2=1.1806890649673805\n",
      "Optimiseur Adam_custom:\n",
      "W1=1.1003370383873736, b1=1.1003370383873736, W2=1.1003370384224467, b2=1.0987096217296624\n",
      "Optimiseur AdamW_custom:\n",
      "W1=1.1013890816161964, b1=1.1013890816161964, W2=1.1013890816512846, b2=1.0997449288859633\n"
     ]
    }
   ],
   "execution_count": 314
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.370901Z",
     "start_time": "2025-01-20T18:47:16.368342Z"
    }
   },
   "cell_type": "code",
   "source": "check_diffs(torch_nn, custom_nn, tol=1e-4)",
   "id": "777fc773f3bc260a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements between\n",
      "[[1.7965515851974487, 1.7965515851974487, 2.356534481048584, 1.532727837562561], [1.956400752067566, 1.956400752067566, 1.956400752067566, 1.899566411972046], [1.186563491821289, 1.186563491821289, 1.186563491821289, 1.1806892156600952], [1.1003371477127075, 1.1003371477127075, 1.1003371477127075, 1.0987093448638916], [1.1013890504837036, 1.1013890504837036, 1.1013890504837036, 1.0997445583343506]]\u0007nd\n",
      "[[1.7965517126874235, 1.7965517126874235, 2.3565344500454675, 1.532727995527799], [1.9564006987933535, 1.9564006987933535, 1.9564006987933535, 1.8995662439967032], [1.1865635594156694, 1.1865635594156694, 1.1865635594156694, 1.1806890649673805], [1.1003370383873736, 1.1003370383873736, 1.1003370384224467, 1.0987096217296624], [1.1013890816161964, 1.1013890816161964, 1.1013890816512846, 1.0997449288859633]]\n",
      "are close within a tolerance of 0.0001\n"
     ]
    }
   ],
   "execution_count": 315
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***Scheduler de Taux d'apprentissage<a name=\"schedulers\"></a>***",
   "id": "5b3df25542164dbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de LRScheduler**",
   "id": "533f3bef6468a3a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.420102Z",
     "start_time": "2025-01-20T18:47:16.417276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LRSchedulerTorch:\n",
    "    def __init__(self, optimizer, initial_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    def set_lr(self, lr):\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group['lr'] = lr\n",
    "\n",
    "\n",
    "class LRSchedulerCustom:\n",
    "    def __init__(self, optimizer, initial_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.optimizer.learning_rate\n",
    "\n",
    "    def set_lr(self, lr):\n",
    "        self.optimizer.learning_rate = lr"
   ],
   "id": "1a5628f99bfe8a7f",
   "outputs": [],
   "execution_count": 316
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Implementation de LRSchedulerOnPlateau**",
   "id": "4c7b175f2dd4c96e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.467993Z",
     "start_time": "2025-01-20T18:47:16.462678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LRSchedulerOnPlateauTorch(LRSchedulerTorch):\n",
    "    def __init__(self, optimizer, initial_lr, patience=10, factor=0.1, min_lr=1e-6, mode='min', threshold=1e-4):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.best_value = None\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, current_value):\n",
    "        if self.best_value is None:\n",
    "            self.best_value = current_value\n",
    "            return\n",
    "\n",
    "        if self.mode == 'min':\n",
    "            improvement = self.best_value - current_value\n",
    "        elif self.mode == 'max':\n",
    "            improvement = current_value - self.best_value\n",
    "        else:\n",
    "            raise ValueError(\"Mode must be either 'min' (minimize) or 'max' (maximize).\")\n",
    "\n",
    "        if isinstance(improvement, Tensor):\n",
    "            if improvement.data > self.threshold:\n",
    "                self.best_value = current_value\n",
    "                self.num_bad_epochs = 0\n",
    "            else:\n",
    "                self.num_bad_epochs += 1\n",
    "        else:\n",
    "            if improvement > self.threshold:\n",
    "                self.best_value = current_value\n",
    "                self.num_bad_epochs = 0\n",
    "            else:\n",
    "                self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            self.reduce_lr()\n",
    "\n",
    "    def reduce_lr(self):\n",
    "        current_lr = self.get_lr()\n",
    "        new_lr = max(current_lr * self.factor, self.min_lr)\n",
    "        if new_lr < current_lr:\n",
    "            print(f\"Reducing learning rate: {current_lr:.6f} -> {new_lr:.6f}\")\n",
    "            self.set_lr(new_lr)\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "\n",
    "class LRSchedulerOnPlateauCustom(LRSchedulerCustom):\n",
    "    def __init__(self, optimizer, initial_lr, patience=10, factor=0.1, min_lr=1e-6, mode='min', threshold=1e-4):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.best_value = None\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, current_value):\n",
    "        if self.best_value is None:\n",
    "            self.best_value = current_value\n",
    "            return\n",
    "\n",
    "        if self.mode == 'min':\n",
    "            improvement = self.best_value - current_value\n",
    "        elif self.mode == 'max':\n",
    "            improvement = current_value - self.best_value\n",
    "        else:\n",
    "            raise ValueError(\"Mode must be either 'min' (minimize) or 'max' (maximize).\")\n",
    "\n",
    "        if improvement.data > self.threshold:\n",
    "            self.best_value = current_value\n",
    "            self.num_bad_epochs = 0\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            self.reduce_lr()\n",
    "\n",
    "    def reduce_lr(self):\n",
    "        current_lr = self.get_lr()\n",
    "        new_lr = max(current_lr * self.factor, self.min_lr)\n",
    "        if new_lr < current_lr:\n",
    "            print(f\"Reducing learning rate: {current_lr:.6f} -> {new_lr:.6f}\")\n",
    "            self.set_lr(new_lr)\n",
    "        self.num_bad_epochs = 0"
   ],
   "id": "720c56dbb5332b80",
   "outputs": [],
   "execution_count": 317
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Test de LRSchedulerOnPlateau**",
   "id": "a0015f5a3eab0528"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.700275Z",
     "start_time": "2025-01-20T18:47:16.514220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([69.], requires_grad=True)\n",
    "conv_torch_scheduler_x, conv_torch_scheduler_fx = eval_optim(x, convexe=True, scheduler=True)\n",
    "print()\n",
    "nonconv_torch_scheduler_x, nonconv_torch_scheduler_fx = eval_optim(x, convexe=False, scheduler=True)"
   ],
   "id": "e38504ba155084ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la fonction convexe f(x) = (x - 2)²\n",
      "Gradient de f en x=69.0: x.grad=134.0\n",
      "Optimiseur SGD_torch: x=10.885512351989746, f(x)=78.95233154296875\n",
      "Optimiseur RMSProp_torch: x=9.80399227142334, f(x)=60.90229415893555\n",
      "Optimiseur Adagrad_torch: x=9.618916511535645, f(x)=58.047889709472656\n",
      "Optimiseur Adam_torch: x=9.519140243530273, f(x)=56.537471771240234\n",
      "Optimiseur AdamW_torch: x=9.428813934326172, f(x)=55.18727493286133\n",
      "\n",
      "Optimisation de la fonction non convexe f(x) = 3x² - 2x\n",
      "Gradient de f en x=9.428813934326172: x.grad=69.43231201171875\n",
      "Optimiseur SGD_torch: x=0.3520234227180481, f(x)=2.715826988220215\n",
      "Reducing learning rate: 0.010000 -> 0.005000\n",
      "Reducing learning rate: 0.005000 -> 0.002500\n",
      "Reducing learning rate: 0.002500 -> 0.001250\n",
      "Reducing learning rate: 0.001250 -> 0.000625\n",
      "Reducing learning rate: 0.000625 -> 0.000313\n",
      "Reducing learning rate: 0.000313 -> 0.000156\n",
      "Reducing learning rate: 0.000156 -> 0.000078\n",
      "Reducing learning rate: 0.000078 -> 0.000039\n",
      "Reducing learning rate: 0.000039 -> 0.000020\n",
      "Reducing learning rate: 0.000020 -> 0.000010\n",
      "Reducing learning rate: 0.000010 -> 0.000005\n",
      "Reducing learning rate: 0.000005 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Reducing learning rate: 0.000001 -> 0.000001\n",
      "Optimiseur RMSProp_torch: x=0.3333333432674408, f(x)=2.777777671813965\n",
      "Reducing learning rate: 0.010000 -> 0.005000\n",
      "Reducing learning rate: 0.005000 -> 0.002500\n",
      "Reducing learning rate: 0.002500 -> 0.001250\n",
      "Reducing learning rate: 0.001250 -> 0.000625\n",
      "Reducing learning rate: 0.000625 -> 0.000313\n",
      "Reducing learning rate: 0.000313 -> 0.000156\n",
      "Reducing learning rate: 0.000156 -> 0.000078\n",
      "Reducing learning rate: 0.000078 -> 0.000039\n",
      "Reducing learning rate: 0.000039 -> 0.000020\n",
      "Reducing learning rate: 0.000020 -> 0.000010\n",
      "Reducing learning rate: 0.000010 -> 0.000005\n",
      "Reducing learning rate: 0.000005 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Reducing learning rate: 0.000001 -> 0.000001\n",
      "Optimiseur Adagrad_torch: x=nan, f(x)=nan\n",
      "Reducing learning rate: 0.001000 -> 0.000500\n",
      "Reducing learning rate: 0.000500 -> 0.000250\n",
      "Reducing learning rate: 0.000250 -> 0.000125\n",
      "Reducing learning rate: 0.000125 -> 0.000063\n",
      "Reducing learning rate: 0.000063 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000016\n",
      "Reducing learning rate: 0.000016 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur Adam_torch: x=nan, f(x)=nan\n",
      "Reducing learning rate: 0.001000 -> 0.000500\n",
      "Reducing learning rate: 0.000500 -> 0.000250\n",
      "Reducing learning rate: 0.000250 -> 0.000125\n",
      "Reducing learning rate: 0.000125 -> 0.000063\n",
      "Reducing learning rate: 0.000063 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000016\n",
      "Reducing learning rate: 0.000016 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur AdamW_torch: x=nan, f(x)=nan\n"
     ]
    }
   ],
   "execution_count": 318
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.727075Z",
     "start_time": "2025-01-20T18:47:16.709881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = Tensor(69.)\n",
    "conv_custom_scheduler_x, conv_custom_scheduler_fx = eval_optim(x, convexe=True, scheduler=True)\n",
    "print()\n",
    "nonconv_custom_scheduler_x, nonconv_custom_scheduler_fx = eval_optim(x, convexe=False, scheduler=True)"
   ],
   "id": "e938b0f6a751f60b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la fonction convexe f(x) = (x - 2)²\n",
      "Gradient de f en x=69.0: x.grad=134.0\n",
      "Optimiseur SGD_custom: x=10.885510244948467, f(x)=78.95229231308416\n",
      "Optimiseur RMSProp_custom: x=9.803987775902403, f(x)=60.90222520643413\n",
      "Optimiseur Adagrad_custom: x=9.618913070757662, f(x)=58.04783637976195\n",
      "Optimiseur Adam_custom: x=9.519135827365043, f(x)=56.53740359036458\n",
      "Optimiseur AdamW_custom: x=9.428814427498079, f(x)=55.18728379820361\n",
      "\n",
      "Optimisation de la fonction non convexe f(x) = 3x² - 2x\n",
      "Gradient de f en x=9.428814427498079: x.grad=69.43231722986997\n",
      "Optimiseur SGD_custom: x=0.3520234079595069, f(x)=2.715826847913398\n",
      "Reducing learning rate: 0.010000 -> 0.005000\n",
      "Reducing learning rate: 0.005000 -> 0.002500\n",
      "Reducing learning rate: 0.002500 -> 0.001250\n",
      "Reducing learning rate: 0.001250 -> 0.000625\n",
      "Reducing learning rate: 0.000625 -> 0.000313\n",
      "Reducing learning rate: 0.000313 -> 0.000156\n",
      "Reducing learning rate: 0.000156 -> 0.000078\n",
      "Reducing learning rate: 0.000078 -> 0.000039\n",
      "Reducing learning rate: 0.000039 -> 0.000020\n",
      "Reducing learning rate: 0.000020 -> 0.000010\n",
      "Reducing learning rate: 0.000010 -> 0.000005\n",
      "Reducing learning rate: 0.000005 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Reducing learning rate: 0.000001 -> 0.000001\n",
      "Optimiseur RMSProp_custom: x=0.3333333333333338, f(x)=2.777777777777776\n",
      "Reducing learning rate: 0.010000 -> 0.005000\n",
      "Reducing learning rate: 0.005000 -> 0.002500\n",
      "Reducing learning rate: 0.002500 -> 0.001250\n",
      "Reducing learning rate: 0.001250 -> 0.000625\n",
      "Reducing learning rate: 0.000625 -> 0.000313\n",
      "Reducing learning rate: 0.000313 -> 0.000156\n",
      "Reducing learning rate: 0.000156 -> 0.000078\n",
      "Reducing learning rate: 0.000078 -> 0.000039\n",
      "Reducing learning rate: 0.000039 -> 0.000020\n",
      "Reducing learning rate: 0.000020 -> 0.000010\n",
      "Reducing learning rate: 0.000010 -> 0.000005\n",
      "Reducing learning rate: 0.000005 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Reducing learning rate: 0.000001 -> 0.000001\n",
      "Optimiseur Adagrad_custom: x=0.33333333333333337, f(x)=2.7777777777777772\n",
      "Reducing learning rate: 0.001000 -> 0.000500\n",
      "Reducing learning rate: 0.000500 -> 0.000250\n",
      "Reducing learning rate: 0.000250 -> 0.000125\n",
      "Reducing learning rate: 0.000125 -> 0.000063\n",
      "Reducing learning rate: 0.000063 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000016\n",
      "Reducing learning rate: 0.000016 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur Adam_custom: x=0.33333333333333337, f(x)=2.7777777777777772\n",
      "Reducing learning rate: 0.001000 -> 0.000500\n",
      "Reducing learning rate: 0.000500 -> 0.000250\n",
      "Reducing learning rate: 0.000250 -> 0.000125\n",
      "Reducing learning rate: 0.000125 -> 0.000063\n",
      "Reducing learning rate: 0.000063 -> 0.000031\n",
      "Reducing learning rate: 0.000031 -> 0.000016\n",
      "Reducing learning rate: 0.000016 -> 0.000008\n",
      "Reducing learning rate: 0.000008 -> 0.000004\n",
      "Reducing learning rate: 0.000004 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Optimiseur AdamW_custom: x=0.33336777081415986, f(x)=2.777662987360963\n"
     ]
    }
   ],
   "execution_count": 319
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.761052Z",
     "start_time": "2025-01-20T18:47:16.758656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "check_diffs(conv_torch_scheduler_x, conv_custom_scheduler_x, tol=1e-4)\n",
    "print()\n",
    "check_diffs(conv_torch_scheduler_fx, conv_custom_scheduler_fx, tol=1e-4)"
   ],
   "id": "186f1b7002fb1028",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements between\n",
      "[10.885512351989746, 9.80399227142334, 9.618916511535645, 9.519140243530273, 9.428813934326172]\u0007nd\n",
      "[10.885510244948467, 9.803987775902403, 9.618913070757662, 9.519135827365043, 9.428814427498079]\n",
      "are close within a tolerance of 0.0001\n",
      "\n",
      "All elements between\n",
      "[78.95233154296875, 60.90229415893555, 58.047889709472656, 56.537471771240234, 55.18727493286133]\u0007nd\n",
      "[78.95229231308416, 60.90222520643413, 58.04783637976195, 56.53740359036458, 55.18728379820361]\n",
      "are close within a tolerance of 0.0001\n"
     ]
    }
   ],
   "execution_count": 320
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.953805Z",
     "start_time": "2025-01-20T18:47:16.807772Z"
    }
   },
   "cell_type": "code",
   "source": "torch_scheduler_nn = eval_nn_optim(scheduler=True, custom=False)",
   "id": "454c7f191b933243",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing learning rate: 0.010000 -> 0.005000\n",
      "Reducing learning rate: 0.005000 -> 0.002500\n",
      "Reducing learning rate: 0.002500 -> 0.001250\n",
      "Reducing learning rate: 0.001250 -> 0.000625\n",
      "Reducing learning rate: 0.000625 -> 0.000313\n",
      "Reducing learning rate: 0.000313 -> 0.000156\n",
      "Reducing learning rate: 0.000156 -> 0.000078\n",
      "Reducing learning rate: 0.000078 -> 0.000039\n",
      "Reducing learning rate: 0.000039 -> 0.000020\n",
      "Reducing learning rate: 0.000020 -> 0.000010\n",
      "Reducing learning rate: 0.000010 -> 0.000005\n",
      "Reducing learning rate: 0.000005 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Reducing learning rate: 0.000001 -> 0.000001\n",
      "Optimiseur SGD_torch:\n",
      "W1=1.796550989151001, b1=1.796550989151001, W2=2.3565328121185303, b2=1.5327274799346924\n",
      "Optimiseur RMSProp_torch:\n",
      "W1=1.956400752067566, b1=1.956400752067566, W2=1.956400752067566, b2=1.899566411972046\n",
      "Optimiseur Adagrad_torch:\n",
      "W1=1.186563491821289, b1=1.186563491821289, W2=1.186563491821289, b2=1.1806892156600952\n",
      "Optimiseur Adam_torch:\n",
      "W1=1.1003371477127075, b1=1.1003371477127075, W2=1.1003371477127075, b2=1.0987093448638916\n",
      "Optimiseur AdamW_torch:\n",
      "W1=1.1013890504837036, b1=1.1013890504837036, W2=1.1013890504837036, b2=1.0997445583343506\n"
     ]
    }
   ],
   "execution_count": 321
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:16.990165Z",
     "start_time": "2025-01-20T18:47:16.974563Z"
    }
   },
   "cell_type": "code",
   "source": "custom_scheduler_nn = eval_nn_optim(scheduler=True, custom=True)",
   "id": "9d418cf21f19dc93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing learning rate: 0.010000 -> 0.005000\n",
      "Reducing learning rate: 0.005000 -> 0.002500\n",
      "Reducing learning rate: 0.002500 -> 0.001250\n",
      "Reducing learning rate: 0.001250 -> 0.000625\n",
      "Reducing learning rate: 0.000625 -> 0.000313\n",
      "Reducing learning rate: 0.000313 -> 0.000156\n",
      "Reducing learning rate: 0.000156 -> 0.000078\n",
      "Reducing learning rate: 0.000078 -> 0.000039\n",
      "Reducing learning rate: 0.000039 -> 0.000020\n",
      "Reducing learning rate: 0.000020 -> 0.000010\n",
      "Reducing learning rate: 0.000010 -> 0.000005\n",
      "Reducing learning rate: 0.000005 -> 0.000002\n",
      "Reducing learning rate: 0.000002 -> 0.000001\n",
      "Reducing learning rate: 0.000001 -> 0.000001\n",
      "Optimiseur SGD_custom:\n",
      "W1=1.7965510145299812, b1=1.7965510145299812, W2=2.3565333855349313, b2=1.5327276992600714\n",
      "Optimiseur RMSProp_custom:\n",
      "W1=1.9564006987933535, b1=1.9564006987933535, W2=1.9564006987933535, b2=1.8995662439967032\n",
      "Optimiseur Adagrad_custom:\n",
      "W1=1.1865635594156694, b1=1.1865635594156694, W2=1.1865635594156694, b2=1.1806890649673805\n",
      "Optimiseur Adam_custom:\n",
      "W1=1.1003370383873736, b1=1.1003370383873736, W2=1.1003370384224467, b2=1.0987096217296624\n",
      "Optimiseur AdamW_custom:\n",
      "W1=1.1013890816161964, b1=1.1013890816161964, W2=1.1013890816512846, b2=1.0997449288859633\n"
     ]
    }
   ],
   "execution_count": 322
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:17.091741Z",
     "start_time": "2025-01-20T18:47:17.089061Z"
    }
   },
   "cell_type": "code",
   "source": "check_diffs(torch_scheduler_nn, custom_scheduler_nn, tol=1e-4)",
   "id": "f75f0ec950d9d1b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements between\n",
      "[[1.796550989151001, 1.796550989151001, 2.3565328121185303, 1.5327274799346924], [1.956400752067566, 1.956400752067566, 1.956400752067566, 1.899566411972046], [1.186563491821289, 1.186563491821289, 1.186563491821289, 1.1806892156600952], [1.1003371477127075, 1.1003371477127075, 1.1003371477127075, 1.0987093448638916], [1.1013890504837036, 1.1013890504837036, 1.1013890504837036, 1.0997445583343506]]\u0007nd\n",
      "[[1.7965510145299812, 1.7965510145299812, 2.3565333855349313, 1.5327276992600714], [1.9564006987933535, 1.9564006987933535, 1.9564006987933535, 1.8995662439967032], [1.1865635594156694, 1.1865635594156694, 1.1865635594156694, 1.1806890649673805], [1.1003370383873736, 1.1003370383873736, 1.1003370384224467, 1.0987096217296624], [1.1013890816161964, 1.1013890816161964, 1.1013890816512846, 1.0997449288859633]]\n",
      "are close within a tolerance of 0.0001\n"
     ]
    }
   ],
   "execution_count": 323
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:47:17.193188Z",
     "start_time": "2025-01-20T18:47:17.191338Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7a2da0192eb5184a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
